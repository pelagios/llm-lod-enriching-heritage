{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pelagios/llm-lod-enriching-heritage/blob/main/notebooks/data_preparation/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hLkfqo0gRKP"
   },
   "source": [
    "# Prepare Cultural Heritage Data for Named Entity Recognition\n",
    "\n",
    "1. Install required software libraries\n",
    "2. Download text from museum website\n",
    "3. Preprocess text for named entity recognition\n",
    "4. Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g_sWNZ7HLN2"
   },
   "source": [
    "## 1. Install required software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "def safe_import(package_name):\n",
    "    try:\n",
    "        return importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        print(f\"ðŸ“¦ {package_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        return importlib.import_module(package_name)\n",
    "\n",
    "spacy = safe_import(\"spacy\")\n",
    "langid = safe_import(\"langid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_4ibh6-tsb-n"
   },
   "outputs": [],
   "source": [
    "import re, unicodedata, json\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import hashlib\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKvJmCRQHQUN"
   },
   "source": [
    "## 2. Download text from museum website\n",
    "\n",
    "We use the description of a painting by Claude Monet from the artwork Cleveland Museum website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bOSpuE_xsvgX"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://openaccess-api.clevelandart.org/api/artworks\"\n",
    "data_source = \"CMA\"\n",
    "\n",
    "def fetch_cma(query_string: str) -> List[Dict[str, Any]]:\n",
    "    response = requests.get(base_url, params={\"q\": query_string, \"skip\": 0, \"limit\": 100}, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    artworks = response.json().get(\"data\", [])\n",
    "    if artworks == []:\n",
    "        return []\n",
    "    else:\n",
    "        return [{\"id\": artworks[0].get(\"id\"), \n",
    "                 \"text\": (artworks[0].get(\"description\") or \"\").strip()}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1rjBgsgwkM4",
    "outputId": "a139382e-f994-4ce0-9fc4-627bd1f27bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 135382, 'text': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\"}\n"
     ]
    }
   ],
   "source": [
    "texts = fetch_cma(\"monet\")\n",
    "if not texts:\n",
    "    print(\"No texts were found. Are you connected to the internet?\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkcitK7zHcuK"
   },
   "source": [
    "## 3. Preprocess text for named entity recognition\n",
    "\n",
    "1. Remove non-text characters, urls and email addresses\n",
    "2. Detect the language of the text\n",
    "3. Split the text in sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_Yzx1zaEv9Dp"
   },
   "outputs": [],
   "source": [
    "def cleanup_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r\"[\\u0000-\\u0008\\u000B\\u000C\\u000E-\\u001F\\u007F]\", \"\", text)\n",
    "    text = re.sub(r\"[ \\t\\u00A0]+\", \" \", text)\n",
    "    text = re.sub(r\"https?://\\S+\", \"<URL>\", text)\n",
    "    text = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"<EMAIL>\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CU7wQCewxyWc"
   },
   "outputs": [],
   "source": [
    "def detect_text_language(text: str) -> Dict[str, Any]:\n",
    "    return langid.classify(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e6xknyGKwQJz"
   },
   "outputs": [],
   "source": [
    "def preprocess_texts(texts: List[Tuple[str, Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    nlp = spacy.blank(\"xx\")\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for (text, meta), doc in zip(texts, nlp.pipe([text for text, _ in texts])):\n",
    "        text_clean = cleanup_text(text)\n",
    "        lang = detect_text_language(text)\n",
    "        sents = [{\"id\": sid, \n",
    "                  \"start\": s.start_char, \n",
    "                  \"end\": s.end_char, \n",
    "                  \"text\": s.text} for sid, s in enumerate(doc.sents)]\n",
    "        tok2sent = {tok.i: sid for sid, s in enumerate(doc.sents) for tok in s}\n",
    "        tokens = [{\"id\": tok.i,\n",
    "                   \"text\": tok.text,\n",
    "                   \"start\": tok.idx,\n",
    "                   \"end\": tok.idx + len(tok.text),\n",
    "                   \"ws\": tok.whitespace_ != \"\",\n",
    "                   \"is_punct\": tok.is_punct,\n",
    "                   \"sent_id\": tok2sent.get(tok.i)} for tok in doc]\n",
    "        results.append({\"text_original\": text,\n",
    "                        \"text_clean\": text_clean,\n",
    "                        \"language_id\": lang,\n",
    "                        \"sentences\": sents,\n",
    "                        \"tokens\": tokens,\n",
    "                        \"meta\": {**meta,\n",
    "                                 \"char_count\": len(text),\n",
    "                                 \"token_count\": len(tokens),\n",
    "                                 \"sentence_count\": len(sents)}})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sw9jFDPTxflU",
    "outputId": "68fd96cd-31e4-4e76-ce96-075cf1646a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 135382, 'text': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\"}\n"
     ]
    }
   ],
   "source": [
    "cleaned_texts = [{\"id\": text[\"id\"], \n",
    "                  \"text\": cleanup_text(text[\"text\"])} for text in texts]\n",
    "print(cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U0q26bQxzKQ",
    "outputId": "f378c7d6-b146-49f2-9015-d31bc5c52d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 135382, 'text': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\", 'language_id': 'en'}\n"
     ]
    }
   ],
   "source": [
    "identified_texts = [{\"id\": text[\"id\"], \n",
    "                     \"text\": text[\"text\"],\n",
    "                     \"language_id\": detect_text_language(text[\"text\"])} for text in cleaned_texts]\n",
    "print(identified_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sklE05mCyemA",
    "outputId": "28d1072c-23b8-408c-9fd0-aa5aaa3846e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_original': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\",\n",
       " 'text_clean': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\",\n",
       " 'language_id': 'en',\n",
       " 'sentences': [{'id': 0,\n",
       "   'start': 0,\n",
       "   'end': 130,\n",
       "   'text': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil.\"},\n",
       "  {'id': 1,\n",
       "   'start': 131,\n",
       "   'end': 324,\n",
       "   'text': 'Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.'}],\n",
       " 'tokens': [{'id': 0,\n",
       "   'text': 'This',\n",
       "   'start': 0,\n",
       "   'end': 4,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 1,\n",
       "   'text': 'painting',\n",
       "   'start': 5,\n",
       "   'end': 13,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 2,\n",
       "   'text': 'depicts',\n",
       "   'start': 14,\n",
       "   'end': 21,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 3,\n",
       "   'text': 'Monet',\n",
       "   'start': 22,\n",
       "   'end': 27,\n",
       "   'ws': False,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 4,\n",
       "   'text': \"'s\",\n",
       "   'start': 27,\n",
       "   'end': 29,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 5,\n",
       "   'text': 'first',\n",
       "   'start': 30,\n",
       "   'end': 35,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 6,\n",
       "   'text': 'wife',\n",
       "   'start': 36,\n",
       "   'end': 40,\n",
       "   'ws': False,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 7,\n",
       "   'text': ',',\n",
       "   'start': 40,\n",
       "   'end': 41,\n",
       "   'ws': True,\n",
       "   'is_punct': True,\n",
       "   'sent_id': 0},\n",
       "  {'id': 8,\n",
       "   'text': 'Camille',\n",
       "   'start': 42,\n",
       "   'end': 49,\n",
       "   'ws': False,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 9,\n",
       "   'text': ',',\n",
       "   'start': 49,\n",
       "   'end': 50,\n",
       "   'ws': True,\n",
       "   'is_punct': True,\n",
       "   'sent_id': 0},\n",
       "  {'id': 10,\n",
       "   'text': 'outside',\n",
       "   'start': 51,\n",
       "   'end': 58,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 11,\n",
       "   'text': 'on',\n",
       "   'start': 59,\n",
       "   'end': 61,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 12,\n",
       "   'text': 'a',\n",
       "   'start': 62,\n",
       "   'end': 63,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 13,\n",
       "   'text': 'snowy',\n",
       "   'start': 64,\n",
       "   'end': 69,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 14,\n",
       "   'text': 'day',\n",
       "   'start': 70,\n",
       "   'end': 73,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 15,\n",
       "   'text': 'passing',\n",
       "   'start': 74,\n",
       "   'end': 81,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 16,\n",
       "   'text': 'by',\n",
       "   'start': 82,\n",
       "   'end': 84,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 17,\n",
       "   'text': 'the',\n",
       "   'start': 85,\n",
       "   'end': 88,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 18,\n",
       "   'text': 'French',\n",
       "   'start': 89,\n",
       "   'end': 95,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 19,\n",
       "   'text': 'doors',\n",
       "   'start': 96,\n",
       "   'end': 101,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 20,\n",
       "   'text': 'of',\n",
       "   'start': 102,\n",
       "   'end': 104,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 21,\n",
       "   'text': 'their',\n",
       "   'start': 105,\n",
       "   'end': 110,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 22,\n",
       "   'text': 'home',\n",
       "   'start': 111,\n",
       "   'end': 115,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 23,\n",
       "   'text': 'at',\n",
       "   'start': 116,\n",
       "   'end': 118,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 24,\n",
       "   'text': 'Argenteuil',\n",
       "   'start': 119,\n",
       "   'end': 129,\n",
       "   'ws': False,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 0},\n",
       "  {'id': 25,\n",
       "   'text': '.',\n",
       "   'start': 129,\n",
       "   'end': 130,\n",
       "   'ws': True,\n",
       "   'is_punct': True,\n",
       "   'sent_id': 0},\n",
       "  {'id': 26,\n",
       "   'text': 'Her',\n",
       "   'start': 131,\n",
       "   'end': 134,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 27,\n",
       "   'text': 'face',\n",
       "   'start': 135,\n",
       "   'end': 139,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 28,\n",
       "   'text': 'is',\n",
       "   'start': 140,\n",
       "   'end': 142,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 29,\n",
       "   'text': 'rendered',\n",
       "   'start': 143,\n",
       "   'end': 151,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 30,\n",
       "   'text': 'in',\n",
       "   'start': 152,\n",
       "   'end': 154,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 31,\n",
       "   'text': 'a',\n",
       "   'start': 155,\n",
       "   'end': 156,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 32,\n",
       "   'text': 'radically',\n",
       "   'start': 157,\n",
       "   'end': 166,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 33,\n",
       "   'text': 'bold',\n",
       "   'start': 167,\n",
       "   'end': 171,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 34,\n",
       "   'text': 'Impressionist',\n",
       "   'start': 172,\n",
       "   'end': 185,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 35,\n",
       "   'text': 'technique',\n",
       "   'start': 186,\n",
       "   'end': 195,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 36,\n",
       "   'text': 'of',\n",
       "   'start': 196,\n",
       "   'end': 198,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 37,\n",
       "   'text': 'mere',\n",
       "   'start': 199,\n",
       "   'end': 203,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 38,\n",
       "   'text': 'daubs',\n",
       "   'start': 204,\n",
       "   'end': 209,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 39,\n",
       "   'text': 'of',\n",
       "   'start': 210,\n",
       "   'end': 212,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 40,\n",
       "   'text': 'paint',\n",
       "   'start': 213,\n",
       "   'end': 218,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 41,\n",
       "   'text': 'quickly',\n",
       "   'start': 219,\n",
       "   'end': 226,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 42,\n",
       "   'text': 'applied',\n",
       "   'start': 227,\n",
       "   'end': 234,\n",
       "   'ws': False,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 43,\n",
       "   'text': ',',\n",
       "   'start': 234,\n",
       "   'end': 235,\n",
       "   'ws': True,\n",
       "   'is_punct': True,\n",
       "   'sent_id': 1},\n",
       "  {'id': 44,\n",
       "   'text': 'just',\n",
       "   'start': 236,\n",
       "   'end': 240,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 45,\n",
       "   'text': 'as',\n",
       "   'start': 241,\n",
       "   'end': 243,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 46,\n",
       "   'text': 'the',\n",
       "   'start': 244,\n",
       "   'end': 247,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 47,\n",
       "   'text': 'snow',\n",
       "   'start': 248,\n",
       "   'end': 252,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 48,\n",
       "   'text': 'and',\n",
       "   'start': 253,\n",
       "   'end': 256,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 49,\n",
       "   'text': 'trees',\n",
       "   'start': 257,\n",
       "   'end': 262,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 50,\n",
       "   'text': 'are',\n",
       "   'start': 263,\n",
       "   'end': 266,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 51,\n",
       "   'text': 'defined',\n",
       "   'start': 267,\n",
       "   'end': 274,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 52,\n",
       "   'text': 'by',\n",
       "   'start': 275,\n",
       "   'end': 277,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 53,\n",
       "   'text': 'broad',\n",
       "   'start': 278,\n",
       "   'end': 283,\n",
       "   'ws': False,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 54,\n",
       "   'text': ',',\n",
       "   'start': 283,\n",
       "   'end': 284,\n",
       "   'ws': True,\n",
       "   'is_punct': True,\n",
       "   'sent_id': 1},\n",
       "  {'id': 55,\n",
       "   'text': 'broken',\n",
       "   'start': 285,\n",
       "   'end': 291,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 56,\n",
       "   'text': 'strokes',\n",
       "   'start': 292,\n",
       "   'end': 299,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 57,\n",
       "   'text': 'of',\n",
       "   'start': 300,\n",
       "   'end': 302,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 58,\n",
       "   'text': 'pure',\n",
       "   'start': 303,\n",
       "   'end': 307,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 59,\n",
       "   'text': 'white',\n",
       "   'start': 308,\n",
       "   'end': 313,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 60,\n",
       "   'text': 'and',\n",
       "   'start': 314,\n",
       "   'end': 317,\n",
       "   'ws': True,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 61,\n",
       "   'text': 'green',\n",
       "   'start': 318,\n",
       "   'end': 323,\n",
       "   'ws': False,\n",
       "   'is_punct': False,\n",
       "   'sent_id': 1},\n",
       "  {'id': 62,\n",
       "   'text': '.',\n",
       "   'start': 323,\n",
       "   'end': 324,\n",
       "   'ws': False,\n",
       "   'is_punct': True,\n",
       "   'sent_id': 1}],\n",
       " 'meta': {'data source': 'CMA',\n",
       "  'id': 135382,\n",
       "  'char_count': 324,\n",
       "  'token_count': 63,\n",
       "  'sentence_count': 2}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [(text[\"text\"], {\"data source\": data_source, \"id\": text[\"id\"]}) for text in texts]\n",
    "preprocessed_texts = preprocess_texts(pairs)\n",
    "preprocessed_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(texts):\n",
    "    json_string = json.dumps(texts, ensure_ascii=False, indent=2)\n",
    "    hash = hashlib.sha1(json_string.encode(\"utf-8\")).hexdigest()\n",
    "    output_file_name = f\"output_{hash}.json\"\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(out_api_id, output_file)\n",
    "        print(f\"Saved preprocessed files to {output_file_name}\")\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed files to output_81ebf3de92885303b69457a8089aee73b1fa3a8d.json\n"
     ]
    }
   ],
   "source": [
    "save_results(preprocessed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqC3iOM-y0BG"
   },
   "source": [
    "## Step 5: Put it all together\n",
    "\n",
    "Call the functions. The code below provides three different options, depending on the types of calls, and it also includes an option that processes local examples.\n",
    "\n",
    "Each option can be run independently and saves the output in a file within the Colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lrpGK5qHwVY-",
    "outputId": "a2c1c22f-9df0-47ba-991e-ac4100557a7c"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4) MAIN (robust demos)\n",
    "# ---------------------------\n",
    "def save_records(records: List[Dict[str, str]], source: str = \"CMA\") -> None:\n",
    "  if not records:\n",
    "    raise ValueError(\"No records found (likely 404 if using the CMA API).\")\n",
    "\n",
    "  pairs = [(r[\"text\"], {\"source\": source, \"id\": r[\"id\"]}) for r in records]\n",
    "  processed_texts = preprocess_texts(pairs)\n",
    "  json_string = json.dumps(processed_texts, ensure_ascii=False, indent=2)\n",
    "  # Use hash to prevent overwriting previous work\n",
    "  hash = hashlib.sha1(json_string.encode(\"utf-8\")).hexdigest()\n",
    "  print(f\"CMA by id: {json_string}\")\n",
    "\n",
    "  output_file = f\"output_{hash}.json\"\n",
    "  with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out_api_id, f)\n",
    "    print(f\"Saved preprocessed files to {output_file}\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    files.download(output_file)\n",
    "\n",
    "# Save to\n",
    "save_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39ml7sGrzdS_",
    "outputId": "19982457-373b-4ac4-d40b-e6f2e59c66f6"
   },
   "outputs": [],
   "source": [
    "    # OPTION B: CMA search (take a couple of hits, skip empty descriptions)\n",
    "if __name__ == \"__main__\":\n",
    "  try:\n",
    "        records = fetch_cma(\"monet\", mode=\"search\", limit=5)\n",
    "        pairs = [(r[\"text\"], {\"source\": \"CMA\", \"id\": r[\"id\"]}) for r in records if r[\"text\"]]\n",
    "        pairs = pairs[:2]  # first two with non-empty text\n",
    "        if pairs:\n",
    "            out_api_search = preprocess_texts(pairs)\n",
    "            print(\"\\nCMA search:\")\n",
    "            print(json.dumps(out_api_search, ensure_ascii=False, indent=2))\n",
    "            # save the results in a local file\n",
    "            output_file = \"output_api_search.json\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(out_api_search, f)\n",
    "                print(f\"Saved preprocessed files to {output_file}\")\n",
    "        else:\n",
    "            print(\"\\nCMA search: no descriptions returned for this query.\")\n",
    "  except Exception as e:\n",
    "        print(\"API search failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ies8iopAzhfE",
    "outputId": "8dc3a29f-b101-4bf0-df0d-b18dae48df91"
   },
   "outputs": [],
   "source": [
    "    # OPTION C: Local examples\n",
    "if __name__ == \"__main__\":\n",
    "  examples = [\n",
    "        (\"Rome is the capital of Italy.\", {\"source\": \"local\"}),\n",
    "        (\"Mark Rutte bezocht gisteren Groningen.\", {\"source\": \"local\"})\n",
    "    ]\n",
    "  out_local = preprocess_texts(examples)\n",
    "  print(\"\\nLocal examples:\")\n",
    "  print(json.dumps(out_local, ensure_ascii=False, indent=2))\n",
    "\n",
    "# save the results in a local file\n",
    "output_file = \"output_local.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(out_local, f)\n",
    "  print(f\"Results saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ku65AzpTfgMn",
    "outputId": "35f8d4e4-2594-48b2-a719-8dd0b09f4072"
   },
   "outputs": [],
   "source": [
    "# OPTION D: Local examples from a txt file (make sure to upload the txt file in the content folder of the Colab Notebook)\n",
    "if __name__ == \"__main__\":\n",
    "  # open a local .txt file as input. remember to change the path and filename to your file.\n",
    "  with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "    # then process the text of the input file\n",
    "    examples = [(text, {\"source\": \"local\"})]\n",
    "    out_local = preprocess_texts(examples)\n",
    "    print(\"\\nLocal examples:\")\n",
    "    print(json.dumps(out_local, ensure_ascii=False, indent=2))\n",
    "\n",
    "# save the results in a local file\n",
    "output_file = \"output_file.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(out_local, f)\n",
    "  print(f\"Results saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Cerm_cBIOo6"
   },
   "source": [
    "## Save the results in a local .zip file\n",
    "\n",
    "The output files are only saved locally in the Google Colab notebook, and will be deleted after the notebook is closed.\n",
    "\n",
    "Two options are available:\n",
    "1. If you are only interested in one of the files you generated, you can simply download the individual output file.\n",
    "2. If you ran all options and want to save all output files, you can download all of them as a zip folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "C8KaCUVd0Cxc",
    "outputId": "1513653f-5db5-440b-a663-80523ce2fcd6"
   },
   "outputs": [],
   "source": [
    "# take the output_file generated by your chosen option and download it on your machine.\n",
    "from google.colab import files\n",
    "files.download(\"output_file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "r3PNWW1wIR4P",
    "outputId": "2a1ae5c1-cfc3-4cd7-9a1a-603bc4656e82"
   },
   "outputs": [],
   "source": [
    "# take the output_file saved under content and download it locally as a .zip file\n",
    "from google.colab import files\n",
    "!zip -r output_files.zip output_api_id.json output_api_search.json output_local.json output_file.json\n",
    "files.download(\"output_files.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YDuesoBf067g"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
