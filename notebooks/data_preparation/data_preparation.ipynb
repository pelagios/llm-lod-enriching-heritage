{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pelagios/llm-lod-enriching-heritage/blob/main/notebooks/data_preparation/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hLkfqo0gRKP"
   },
   "source": [
    "# Prepare Cultural Heritage Data for Named Entity Analysis\n",
    "\n",
    "This notebook prepares data for named entity analysis. It performs four tasks which are represented by the next four chapters:\n",
    "\n",
    "1. Install required software libraries\n",
    "2. Download text from museum website\n",
    "3. Preprocess text for named entity analysis\n",
    "4. Save results\n",
    "\n",
    "The fifth chapter provides alternatives for handling texts from other sources than websites:\n",
    "\n",
    "5. Alternatives for reading text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g_sWNZ7HLN2"
   },
   "source": [
    "## 1. Install required software libraries\n",
    "\n",
    "Preprocessing data requires importing some standard software libraries. This step may take some time when run for the first time but in successive runs it will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import standard libraries which should always be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_4ibh6-tsb-n"
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "import hashlib\n",
    "import importlib\n",
    "import json\n",
    "import regex\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import packages which may require installation on this device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_package = \"üì¶\"\n",
    "char_success = \"‚úÖ\"\n",
    "char_failiure = \"‚ùå\"\n",
    "\n",
    "\n",
    "def safe_import(package_name):\n",
    "    \"\"\"Import a package;. If it missing, download it first\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        print(f\"{char_package} {package_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"Finished installing {package_name}\")\n",
    "        return importlib.import_module(package_name)\n",
    "\n",
    "\n",
    "spacy = safe_import(\"spacy\")\n",
    "langid = safe_import(\"langid\")\n",
    "regex = safe_import(\"regex\")\n",
    "pl = safe_import(\"polars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we set setting required for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def set_css():\n",
    "    \"\"\"Fix line wrapping of output texts for Google Colab\"\"\"\n",
    "    display(HTML(\"<style> pre { white-space: pre-wrap; </style>\"))\n",
    "\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading texts from a csv file\n",
    "\n",
    "We use the artefact descriptions from the Egyptian Museum in Turin as example texts. We use two columns of the file, one with the identifier of the artefact and one with the description text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function for reading the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"em.csv\"\n",
    "data_source = \"EMT\"\n",
    "id_column_name = \"Inventory Number\"\n",
    "text_column_name = \"Description\"\n",
    "\n",
    "\n",
    "def read_emt_data(file_name):\n",
    "    \"\"\"Read texts from the Egyptian Museum in Turin from a csv file\"\"\"\n",
    "    try:\n",
    "        table_pl = pl.read_csv(file_name)[id_column_name, text_column_name]\n",
    "        table_pl.write_csv(\"tmp.csv\")\n",
    "        return [{\"id\": row[0], \"data_source\": data_source, \"text_original\": row[1]}\n",
    "                for row in table_pl.iter_rows()]\n",
    "    except:\n",
    "        print(f\"{char_failiure} Cannot read data from file {file_name}!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the function and store the variable in the variable `texts`. We show the first text to check if the process was successful. Note that the text includes (`id`) the text identifier as metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 'C. 0115', 'data_source': 'EMT', 'text_original': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts = read_emt_data(file_name)\n",
    "if len(texts) <= 0:\n",
    "    print(f\"{char_failiure} No texts found in file {file_name}!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkcitK7zHcuK"
   },
   "source": [
    "## 3. Preprocess text for named entity analysis\n",
    "\n",
    "Three steps are performed while preprocessing the texts:\n",
    "\n",
    "1. Text cleanup: remove non-text characters, urls and email addresses\n",
    "2. Detect the language of the text\n",
    "3. Split the text in sentences and tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining five functions for performing the preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_Yzx1zaEv9Dp"
   },
   "outputs": [],
   "source": [
    "def cleanup_text(text: str) -> str:\n",
    "    \"\"\"Cleanup text: remove non-text characters, urls and email addresses\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = regex.sub(r\"[\\u0000-\\u0008\\u000B\\u000C\\u000E-\\u001F\\u007F]\", \"\", text)\n",
    "    text = regex.sub(r\"[ \\t\\u00A0]+\", \" \", text)\n",
    "    text = regex.sub(r\"\\s+\", \" \", text)\n",
    "    text = regex.sub(r\"https?://\\S+\", \"<URL>\", text)\n",
    "    text = regex.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"<EMAIL>\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CU7wQCewxyWc"
   },
   "outputs": [],
   "source": [
    "def detect_text_language(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Detect language text is written in and return id of the language\"\"\"\n",
    "    return langid.classify(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language_id):\n",
    "    \"\"\"Preprocess a single text: divide it in sentences and tokens\"\"\"\n",
    "    try:\n",
    "        spacy_model = spacy.blank(language_id)\n",
    "    except:\n",
    "        print(f\"{char_failiure} Cannot load model for language {language_id}\")\n",
    "        spacy_model = spacy.blank(\"xx\")\n",
    "    spacy_model.add_pipe(\"sentencizer\")\n",
    "    preprocessed_text = spacy_model(text)\n",
    "    sentences = [{\"id\": sentence_id, \n",
    "                  \"start\": sentence.start_char, \n",
    "                  \"end\": sentence.end_char, \n",
    "                  \"text\": sentence.text} for sentence_id, sentence in enumerate(preprocessed_text.sents)]\n",
    "    tok2sent = {token.i: sentence_id for sentence_id, sentence in enumerate(preprocessed_text.sents) \n",
    "                                     for token in sentence}\n",
    "    tokens = [{\"id\": token.i,\n",
    "               \"text\": token.text,\n",
    "               \"start\": token.idx,\n",
    "               \"end\": token.idx + len(token.text),\n",
    "               \"ws\": token.whitespace_ != \"\",\n",
    "               \"is_punct\": token.is_punct,\n",
    "               \"sent_id\": tok2sent.get(token.i)} for token in preprocessed_text]\n",
    "    return sentences, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e6xknyGKwQJz"
   },
   "outputs": [],
   "source": [
    "def preprocess_texts(texts) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Preprocess a list of texts and return the results as a list of dictionaries\"\"\"\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for text in texts:\n",
    "        sentences, tokens = preprocess_text(text[\"text_cleaned\"], text[\"language_id\"])\n",
    "        results.append({\"meta\": {**{key: text[key] for key in text if not regex.search(\"text\", key)},\n",
    "                                 \"char_count\": len(text),\n",
    "                                 \"token_count\": len(tokens),\n",
    "                                 \"sentence_count\": len(sentences)},\n",
    "                        \"text_original\": text[\"text_original\"],\n",
    "                        \"text_cleaned\": text[\"text_cleaned\"],\n",
    "                        \"sentences\": sentences,\n",
    "                        \"tokens\": tokens})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_text(text, skipped_fields=[]):\n",
    "    \"\"\"Show example text\"\"\"\n",
    "    text_shown = {key: text[key] for key in text if key not in skipped_fields}\n",
    "    if \"tokens\" in skipped_fields:\n",
    "        text_shown = text_shown | {\"tokens\": text[\"tokens\"][:3] + \n",
    "                                             [\"...\"] if len(text[\"tokens\"]) >= 3 else []}\n",
    "    print(text_shown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the cleanup function to the texts and store the results in the variable `text_cleaned`. We show the first text to check if the process was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sw9jFDPTxflU",
    "outputId": "68fd96cd-31e4-4e76-ce96-075cf1646a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'C. 0115', 'data_source': 'EMT', 'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts_cleaned = [text | {\"text_cleaned\": cleanup_text(text[\"text_original\"])} for text in texts]\n",
    "show_example_text(texts_cleaned[0], skipped_fields=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we apply the language derivation function to the texts and store the results in the variable `text_with_language_ids`. Again, we show the first text to check if the process was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U0q26bQxzKQ",
    "outputId": "f378c7d6-b146-49f2-9015-d31bc5c52d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language_id': 'en', 'id': 'C. 0115', 'data_source': 'EMT', 'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts_with_language_ids = [{\"language_id\": detect_text_language(text[\"text_cleaned\"])} | text\n",
    "                            for text in texts_cleaned]\n",
    "show_example_text(texts_with_language_ids[0], skipped_fields=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply the preprocess function to the texts and store the results in the variable `text_preprocessed`. We show the first text to check if the process was successful. The texts have been divided in sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sklE05mCyemA",
    "outputId": "28d1072c-23b8-408c-9fd0-aa5aaa3846e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'language_id': 'en', 'id': 'C. 0115', 'data_source': 'EMT', 'char_count': 5, 'token_count': 23, 'sentence_count': 4}, 'sentences': [{'id': 0, 'start': 0, 'end': 28, 'text': 'Statuette of the god Anubis.'}, {'id': 1, 'start': 29, 'end': 36, 'text': 'Bronze.'}, {'id': 2, 'start': 37, 'end': 85, 'text': 'Late Period (722-332 BC).. Acquired before 1882.'}, {'id': 3, 'start': 86, 'end': 92, 'text': 'C. 115'}], 'tokens': [{'id': 0, 'text': 'Statuette', 'start': 0, 'end': 9, 'ws': True, 'is_punct': False, 'sent_id': 0}, {'id': 1, 'text': 'of', 'start': 10, 'end': 12, 'ws': True, 'is_punct': False, 'sent_id': 0}, {'id': 2, 'text': 'the', 'start': 13, 'end': 16, 'ws': True, 'is_punct': False, 'sent_id': 0}, '...']}\n"
     ]
    }
   ],
   "source": [
    "texts_preprocessed = preprocess_texts(texts_with_language_ids)\n",
    "show_example_text(texts_preprocessed[0], skipped_fields=[\"text_original\", \"text_cleaned\", \"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results\n",
    "\n",
    "When running the notebook locally, the preprocessed texts can be saved locally. When running the notebook on Google Colab, the results need to be downloaded because saved files on Google Colab will be removed automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function for saving the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(texts):\n",
    "    \"\"\"Save preprocessed texts in a json file\"\"\"\n",
    "    json_string = json.dumps(texts, ensure_ascii=False, indent=2)\n",
    "    hash = hashlib.sha1(json_string.encode(\"utf-8\")).hexdigest()\n",
    "    output_file_name = f\"output_{hash}.json\"\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        print(json_string, end=\"\", file=output_file)\n",
    "        output_file.close()\n",
    "        if IN_COLAB:\n",
    "            try:\n",
    "                files.download(output_file_name)\n",
    "                print(f\"Ô∏è{char_success} Downloaded preprocessed texts to file {output_file_name}\")\n",
    "            except:\n",
    "                print(f\"Ô∏è{char_failiure} Downloading preprocessed texts failed!\")\n",
    "        else:\n",
    "            print(f\"Ô∏è{char_success} Saved preprocessed texts to file {output_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function to save the texts and their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ô∏è‚úÖ Saved preprocessed texts to file output_11f98441067263d80ee1a6bac27babf0f2c6734b.json\n"
     ]
    }
   ],
   "source": [
    "save_results(texts_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alternatives for reading text data\n",
    "\n",
    "Here are some methods for reading texts as alternatives for reading them from a museum website as presented in chapter 2 of this notebook. Run these code blocks instead of the code blocks of chapter 2 and then proceed with chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Text examples defined in the code\n",
    "\n",
    "We use descriptions from three famous artworks from Wikipedia, written in other languages than English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the texts. We include identifiers in their descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [{\"id\": 1, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"De Nachtwacht is een schuttersstuk van de \n",
    "           Hollandse schilder Rembrandt van Rijn (1606-1669) dat in 1642 gereed kwam. De huidige offici√´le \n",
    "           titel luidt: Officieren en andere schutters van wijk II in Amsterdam, onder leiding van kapitein \n",
    "           Frans Banninck Cocq en luitenant Willem van Ruytenburch, bekend als ‚ÄòDe Nachtwacht‚Äô.\"\"\"},\n",
    "         {\"id\": 2, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"La Gioconda, nota anche come Monna Lisa, \n",
    "           √® un dipinto a olio su tavola di pioppo realizzato da Leonardo da Vinci (77 √ó 53 cm e 13 mm di \n",
    "           spessore), databile al 1503-1506 circa e conservato nel Museo del Louvre di Parigi col numero 779 \n",
    "           di catalogo.\"\"\"},\n",
    "         {\"id\": 3, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"Le Penseur (initialement intitul√© Le Po√®te) \n",
    "           est un des chefs-d'≈ìuvre embl√©matiques d'Auguste Rodin.\"\"\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check if the definition worked and display the first text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 1, 'data_source': 'wikipedia', 'text_original': 'De Nachtwacht is een schuttersstuk van de \\n           Hollandse schilder Rembrandt van Rijn (1606-1669) dat in 1642 gereed kwam. De huidige offici√´le \\n           titel luidt: Officieren en andere schutters van wijk II in Amsterdam, onder leiding van kapitein \\n           Frans Banninck Cocq en luitenant Willem van Ruytenburch, bekend als ‚ÄòDe Nachtwacht‚Äô.'}\n"
     ]
    }
   ],
   "source": [
    "if len(texts) <= 0:\n",
    "    print(f\"{char_failiure} No texts found!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Read a single text from a text file\n",
    "\n",
    "We use a description of a monument from Wikipedia, written in a different language than English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining a function for reading the text from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"wikipedia.txt\"\n",
    "data_source = \"wikipedia\"\n",
    "\n",
    "\n",
    "def read_wikipedia_file(file_name):\n",
    "    \"\"\"Read a text from Wikipedia from a text file\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\") as infile:\n",
    "            text = infile.read().strip()\n",
    "            infile.close()\n",
    "        return [{\"id\": 1, \"data_source\": data_source, \"text_original\": text}]\n",
    "    except:\n",
    "        print(f\"{char_failiure} Cannot read data from file {file_name}!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function. This requires that a file with the specified file name in present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 1, 'data_source': 'wikipedia', 'text_original': 'Der K√∂lner Dom (offiziell Hohe Domkirche zu K√∂ln) ist eine r√∂misch-katholische Kirche in K√∂ln unter dem Patrozinium des Apostels Petrus. Er ist die Kathedrale des Erzbistums K√∂ln sowie Metropolitan kirche der Kirchenprovinz K√∂ln. Hausherr ist der Dompropst. Der K√∂lner Dom ist eine der gr√∂√üten Kathedralen im gotischen Baustil. Sein Bau wurde 1248 im Auftrag von Konrad I. nach Entwurf von Meister Gerhard begonnen und 1880 im Auftrag von Friedrich Wilhelm IV. nach Entwurf von Ernst Friedrich Zwirner vollendet. Einige Kunsthistoriker haben den Dom wegen seiner einheitlichen und ausgewogenen Bauform als ‚Äûvollkommene Kathedrale‚Äú bezeichnet. Mit 157,22 Metern ist er nach dem Ulmer M√ºnster der zweith√∂chste Sakralbau Deutschlands und hinter der Basilika Notre-Dame-de-la-Paix de Yamoussoukro die dritth√∂chste Kirche der Welt.'}\n"
     ]
    }
   ],
   "source": [
    "texts = read_wikipedia_file(file_name)\n",
    "if len(texts) <= 0:\n",
    "    print(f\"{char_failiure} No texts found in file {file_name}!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKvJmCRQHQUN"
   },
   "source": [
    "### 5.3. Download text from museum website\n",
    "\n",
    "We use the description of a painting by Claude Monet from the artwork Cleveland Museum website as example text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function for reading the texts from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bOSpuE_xsvgX"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://openaccess-api.clevelandart.org/api/artworks\"\n",
    "data_source = \"CMA\"\n",
    "\n",
    "\n",
    "def fetch_cma(query_string: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch metadata from Cleveland Museum of Art website\"\"\"\n",
    "    try:\n",
    "        response = requests.get(base_url, params={\"q\": query_string, \"skip\": 0, \"limit\": 100}, timeout=10)\n",
    "    except:\n",
    "        print(f\"{char_failiure} Cannot download data from {base_url}\")\n",
    "        return []\n",
    "    response.raise_for_status()\n",
    "    artworks = response.json().get(\"data\", [])\n",
    "    if artworks == []:\n",
    "        return []\n",
    "    else:\n",
    "        return [{\"id\": artworks[0].get(\"id\"), \n",
    "                 \"data_source\": data_source,\n",
    "                 \"text_original\": (artworks[0].get(\"description\") or \"\").strip()}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1rjBgsgwkM4",
    "outputId": "a139382e-f994-4ce0-9fc4-627bd1f27bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 136510, 'data_source': 'CMA', 'text_original': 'A skilled horticulturalist as well as an artist, Claude Monet spent the last 30 years of his life painting the private garden he designed and helped cultivate at his home in Giverny in northern France. The resultant canvases are notable for their varied motifs, formats, and sizes. Monumental in scale, this rendering of his water lily pond focuses on the momentary effects of sunlight as it both penetrates and reflects off its shimmering surface. By zeroing in on the water and omitting its horizon and surrounding banks, Monet infers a limitless expanse‚Äîa perception amplified by the painting‚Äôs vast horizontal format that fills the viewer‚Äôs field of vision.'}\n"
     ]
    }
   ],
   "source": [
    "texts = fetch_cma(\"monet\")\n",
    "if not texts:\n",
    "    print(\"{char_failiure} No texts were found. Are you connected to the internet?\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YDuesoBf067g"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
