{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pelagios/llm-lod-enriching-heritage/blob/main/notebooks/data_preparation/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hLkfqo0gRKP"
   },
   "source": [
    "# Prepare Cultural Heritage Data for Named Entity Analysis\n",
    "\n",
    "This notebook fetches and prepares data for named entity analysis. \n",
    "\n",
    "### Rationale\n",
    "\n",
    "This recipe takes some source content and preprocesses it for use across the rest of the cookbook.\n",
    "\n",
    "You can either:\n",
    "\n",
    "* upload your own sample text (e.g. transcribed text from a digitised item), or\n",
    "* run queries to get sample content from the Cleveland Museum of Art API from a keyword (e.g. \"Manet\") or record ID search\n",
    "\n",
    "The notebook shows the necessary steps to process the results and format them into a JSON file that can then be fed into a NER process.\n",
    "\n",
    "### Overview of the process\n",
    "\n",
    "Most parts of the recipe simply need to be run in a Notebook environment like Colab, Binder, Jupyter Notebooks.\n",
    "\n",
    "1. Initialise the notebook: run the first 'cells' to import the required libraries\n",
    "2. Fetch input: this is the part where you can provide some specific input, in csv format\n",
    "3. Preprocess text: clean text, detect language and split the text in tokens and sentences\n",
    "4. Save the results: combine results into a structured json record: text (original and cleaned), language, sentences, tokens and meta information (counts and ids)\n",
    "\n",
    "The fifth chapter provides alternatives for handling texts from other sources than websites:\n",
    "\n",
    "5. Alternatives for reading text data: text defined in code, text from file, text from pdf file and text from a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g_sWNZ7HLN2"
   },
   "source": [
    "## 1. Install required software libraries\n",
    "\n",
    "Preprocessing data requires importing some standard software libraries. This step may take some time when run for the first time but in successive runs it will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import standard libraries which should always be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_4ibh6-tsb-n"
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "import hashlib\n",
    "import importlib\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import regex\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import packages which may require installation on this device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_package = \"üì¶\"\n",
    "char_success = \"‚úÖ\"\n",
    "char_failure = \"‚ùå\"\n",
    "\n",
    "\n",
    "def safe_import(package_name):\n",
    "    \"\"\"Import a package;. If it missing, download it first\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        print(f\"{char_package} {package_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"Finished installing {package_name}\")\n",
    "        return importlib.import_module(package_name)\n",
    "\n",
    "\n",
    "spacy = safe_import(\"spacy\")\n",
    "langid = safe_import(\"langid\")\n",
    "regex = safe_import(\"regex\")\n",
    "pl = safe_import(\"polars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for monitoring the progress of the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    \"\"\"Clear the output buffer of the current cell and print the given text\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: \n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we set setting required for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def set_css():\n",
    "    \"\"\"Fix line wrapping of output texts for Google Colab\"\"\"\n",
    "    display(HTML(\"<style> pre { white-space: pre-wrap; </style>\"))\n",
    "\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading texts from a csv file\n",
    "\n",
    "We use the artefact descriptions from the Egyptian Museum in Turin as example texts. We use two columns of the file, one with the identifier of the artefact and one with the description text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function for reading the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"em.csv\"\n",
    "data_source = \"EMT\"\n",
    "id_column_name = \"Inventory Number\"\n",
    "text_column_name = \"Description\"\n",
    "\n",
    "\n",
    "def read_emt_data(file_name):\n",
    "    \"\"\"Read texts from the Egyptian Museum in Turin from a csv file\"\"\"\n",
    "    try:\n",
    "        table_pl = pl.read_csv(file_name)[id_column_name, text_column_name]\n",
    "        table_pl.write_csv(\"tmp.csv\")\n",
    "        return [{\"id\": row[0], \"data_source\": data_source, \"text_original\": row[1]}\n",
    "                for row in table_pl.iter_rows()]\n",
    "    except:\n",
    "        print(f\"{char_failure} Cannot read data from file {file_name}!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the function and store the variable in the variable `texts`. We show the first text to check if the process was successful. Note that the text includes (`id`) the text identifier as metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 'C. 0115', 'data_source': 'EMT', 'text_original': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts = read_emt_data(file_name)\n",
    "if len(texts) <= 0:\n",
    "    print(f\"{char_failure} No texts found in file {file_name}!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkcitK7zHcuK"
   },
   "source": [
    "## 3. Preprocess text for named entity analysis\n",
    "\n",
    "Three steps are performed while preprocessing the texts:\n",
    "\n",
    "1. Text cleanup: remove non-text characters, urls and email addresses\n",
    "2. Detect the language of the text\n",
    "3. Split the text in sentences and tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining five functions for performing the preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_Yzx1zaEv9Dp"
   },
   "outputs": [],
   "source": [
    "def cleanup_text(text: str) -> str:\n",
    "    \"\"\"Cleanup text: remove non-text characters, urls and email addresses\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = regex.sub(r\"[\\u0000-\\u0008\\u000B\\u000C\\u000E-\\u001F\\u007F]\", \"\", text)\n",
    "    text = regex.sub(r\"[ \\t\\u00A0]+\", \" \", text)\n",
    "    text = regex.sub(r\"\\s+\", \" \", text)\n",
    "    text = regex.sub(r\"https?://\\S+\", \"<URL>\", text)\n",
    "    text = regex.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"<EMAIL>\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CU7wQCewxyWc"
   },
   "outputs": [],
   "source": [
    "def detect_text_language(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Detect language text is written in and return id of the language\"\"\"\n",
    "    return langid.classify(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language_id):\n",
    "    \"\"\"Preprocess a single text: divide it in sentences and tokens\"\"\"\n",
    "    try:\n",
    "        spacy_model = spacy.blank(language_id)\n",
    "    except:\n",
    "        print(f\"{char_failure} Cannot load model for language {language_id}\")\n",
    "        spacy_model = spacy.blank(\"xx\")\n",
    "    spacy_model.add_pipe(\"sentencizer\")\n",
    "    preprocessed_text = spacy_model(text)\n",
    "    sentences = [{\"id\": sentence_id, \n",
    "                  \"start\": sentence.start_char, \n",
    "                  \"end\": sentence.end_char, \n",
    "                  \"text\": sentence.text} for sentence_id, sentence in enumerate(preprocessed_text.sents)]\n",
    "    tok2sent = {token.i: sentence_id for sentence_id, sentence in enumerate(preprocessed_text.sents) \n",
    "                                     for token in sentence}\n",
    "    tokens = [{\"id\": token.i,\n",
    "               \"text\": token.text,\n",
    "               \"start\": token.idx,\n",
    "               \"end\": token.idx + len(token.text),\n",
    "               \"ws\": token.whitespace_ != \"\",\n",
    "               \"is_punct\": token.is_punct,\n",
    "               \"sent_id\": tok2sent.get(token.i)} for token in preprocessed_text]\n",
    "    return sentences, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e6xknyGKwQJz"
   },
   "outputs": [],
   "source": [
    "def preprocess_texts(texts) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Preprocess a list of texts and return the results as a list of dictionaries\"\"\"\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for index, text in enumerate(texts):\n",
    "        squeal(f\"Processing text {index + 1}\")\n",
    "        sentences, tokens = preprocess_text(text[\"text_cleaned\"], text[\"language_id\"])\n",
    "        results.append({\"meta\": {**{key: text[key] for key in text if not regex.search(\"text\", key)},\n",
    "                                 \"char_count\": len(text),\n",
    "                                 \"token_count\": len(tokens),\n",
    "                                 \"sentence_count\": len(sentences)},\n",
    "                        \"text_original\": text[\"text_original\"],\n",
    "                        \"text_cleaned\": text[\"text_cleaned\"],\n",
    "                        \"sentences\": sentences,\n",
    "                        \"tokens\": tokens})\n",
    "    print(\"Finished processing\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_text(text, skipped_fields=[]):\n",
    "    \"\"\"Show example text\"\"\"\n",
    "    text_shown = {key: text[key] for key in text if key not in skipped_fields}\n",
    "    if \"tokens\" in skipped_fields:\n",
    "        text_shown = text_shown | {\"tokens\": text[\"tokens\"][:3] + \n",
    "                                             [\"...\"] if len(text[\"tokens\"]) >= 3 else []}\n",
    "    print(text_shown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the cleanup function to the texts and store the results in the variable `text_cleaned`. We show the first text to check if the process was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sw9jFDPTxflU",
    "outputId": "68fd96cd-31e4-4e76-ce96-075cf1646a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'C. 0115', 'data_source': 'EMT', 'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts_cleaned = [text | {\"text_cleaned\": cleanup_text(text[\"text_original\"])} for text in texts]\n",
    "show_example_text(texts_cleaned[0], skipped_fields=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we apply the language derivation function to the texts and store the results in the variable `text_with_language_ids`. Again, we show the first text to check if the process was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U0q26bQxzKQ",
    "outputId": "f378c7d6-b146-49f2-9015-d31bc5c52d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language_id': 'en', 'id': 'C. 0115', 'data_source': 'EMT', 'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts_with_language_ids = [{\"language_id\": detect_text_language(text[\"text_cleaned\"])} | text\n",
    "                            for text in texts_cleaned]\n",
    "show_example_text(texts_with_language_ids[0], skipped_fields=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply the preprocess function to the texts and store the results in the variable `text_preprocessed`. We show the first text to check if the process was successful. The texts have been divided in sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sklE05mCyemA",
    "outputId": "28d1072c-23b8-408c-9fd0-aa5aaa3846e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 100\n",
      "Finished processing\n",
      "{'meta': {'language_id': 'en', 'id': 'C. 0115', 'data_source': 'EMT', 'char_count': 5, 'token_count': 23, 'sentence_count': 4}, 'sentences': [{'id': 0, 'start': 0, 'end': 28, 'text': 'Statuette of the god Anubis.'}, {'id': 1, 'start': 29, 'end': 36, 'text': 'Bronze.'}, {'id': 2, 'start': 37, 'end': 85, 'text': 'Late Period (722-332 BC).. Acquired before 1882.'}, {'id': 3, 'start': 86, 'end': 92, 'text': 'C. 115'}], 'tokens': [{'id': 0, 'text': 'Statuette', 'start': 0, 'end': 9, 'ws': True, 'is_punct': False, 'sent_id': 0}, {'id': 1, 'text': 'of', 'start': 10, 'end': 12, 'ws': True, 'is_punct': False, 'sent_id': 0}, {'id': 2, 'text': 'the', 'start': 13, 'end': 16, 'ws': True, 'is_punct': False, 'sent_id': 0}, '...']}\n"
     ]
    }
   ],
   "source": [
    "texts_preprocessed = preprocess_texts(texts_with_language_ids)\n",
    "show_example_text(texts_preprocessed[0], skipped_fields=[\"text_original\", \"text_cleaned\", \"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results\n",
    "\n",
    "When running the notebook locally, the preprocessed texts can be saved locally. When running the notebook on Google Colab, the results need to be downloaded because saved files on Google Colab will be removed automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function for saving the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(texts):\n",
    "    \"\"\"Save preprocessed texts in a json file\"\"\"\n",
    "    json_string = json.dumps(texts, ensure_ascii=False, indent=2)\n",
    "    hash = hashlib.sha1(json_string.encode(\"utf-8\")).hexdigest()\n",
    "    output_file_name = f\"output_{hash}.json\"\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        print(json_string, end=\"\", file=output_file)\n",
    "        output_file.close()\n",
    "        if IN_COLAB:\n",
    "            try:\n",
    "                files.download(output_file_name)\n",
    "                print(f\"Ô∏è{char_success} Downloaded preprocessed texts to file {output_file_name}\")\n",
    "            except:\n",
    "                print(f\"Ô∏è{char_failure} Downloading preprocessed texts failed!\")\n",
    "        else:\n",
    "            print(f\"Ô∏è{char_success} Saved preprocessed texts to file {output_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function to save the texts and their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ô∏è‚úÖ Saved preprocessed texts to file output_11f98441067263d80ee1a6bac27babf0f2c6734b.json\n"
     ]
    }
   ],
   "source": [
    "save_results(texts_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alternatives for reading text data\n",
    "\n",
    "Here are some methods for reading texts as alternatives for reading them from a museum website as presented in chapter 2 of this notebook. Run these code blocks instead of the code blocks of chapter 2 and then proceed with chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Text examples defined in the code\n",
    "\n",
    "We use descriptions from three famous artworks from Wikipedia, written in other languages than English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the texts. We include identifiers in their descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [{\"id\": 1, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"De Nachtwacht is een schuttersstuk van de \n",
    "           Hollandse schilder Rembrandt van Rijn (1606-1669) dat in 1642 gereed kwam. De huidige offici√´le \n",
    "           titel luidt: Officieren en andere schutters van wijk II in Amsterdam, onder leiding van kapitein \n",
    "           Frans Banninck Cocq en luitenant Willem van Ruytenburch, bekend als ‚ÄòDe Nachtwacht‚Äô.\"\"\"},\n",
    "         {\"id\": 2, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"La Gioconda, nota anche come Monna Lisa, \n",
    "           √® un dipinto a olio su tavola di pioppo realizzato da Leonardo da Vinci (77 √ó 53 cm e 13 mm di \n",
    "           spessore), databile al 1503-1506 circa e conservato nel Museo del Louvre di Parigi col numero 779 \n",
    "           di catalogo.\"\"\"},\n",
    "         {\"id\": 3, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"Le Penseur (initialement intitul√© Le Po√®te) \n",
    "           est un des chefs-d'≈ìuvre embl√©matiques d'Auguste Rodin.\"\"\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check if the definition worked and display the first text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 1, 'data_source': 'wikipedia', 'text_original': 'De Nachtwacht is een schuttersstuk van de \\n           Hollandse schilder Rembrandt van Rijn (1606-1669) dat in 1642 gereed kwam. De huidige offici√´le \\n           titel luidt: Officieren en andere schutters van wijk II in Amsterdam, onder leiding van kapitein \\n           Frans Banninck Cocq en luitenant Willem van Ruytenburch, bekend als ‚ÄòDe Nachtwacht‚Äô.'}\n"
     ]
    }
   ],
   "source": [
    "if len(texts) <= 0:\n",
    "    print(f\"{char_failure} No texts found!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Read a single text from a text file\n",
    "\n",
    "We use a description of a monument from Wikipedia, written in a different language than English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining a function for reading the text from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"wikipedia.txt\"\n",
    "data_source = \"wikipedia\"\n",
    "\n",
    "\n",
    "def read_wikipedia_file(file_name):\n",
    "    \"\"\"Read a text from Wikipedia from a text file\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\") as infile:\n",
    "            text = infile.read().strip()\n",
    "            infile.close()\n",
    "        return [{\"id\": 1, \"data_source\": data_source, \"text_original\": text}]\n",
    "    except:\n",
    "        print(f\"{char_failure} Cannot read data from file {file_name}!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function. This requires that a file with the specified file name in present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 1, 'data_source': 'wikipedia', 'text_original': 'Der K√∂lner Dom (offiziell Hohe Domkirche zu K√∂ln) ist eine r√∂misch-katholische Kirche in K√∂ln unter dem Patrozinium des Apostels Petrus. Er ist die Kathedrale des Erzbistums K√∂ln sowie Metropolitan kirche der Kirchenprovinz K√∂ln. Hausherr ist der Dompropst. Der K√∂lner Dom ist eine der gr√∂√üten Kathedralen im gotischen Baustil. Sein Bau wurde 1248 im Auftrag von Konrad I. nach Entwurf von Meister Gerhard begonnen und 1880 im Auftrag von Friedrich Wilhelm IV. nach Entwurf von Ernst Friedrich Zwirner vollendet. Einige Kunsthistoriker haben den Dom wegen seiner einheitlichen und ausgewogenen Bauform als ‚Äûvollkommene Kathedrale‚Äú bezeichnet. Mit 157,22 Metern ist er nach dem Ulmer M√ºnster der zweith√∂chste Sakralbau Deutschlands und hinter der Basilika Notre-Dame-de-la-Paix de Yamoussoukro die dritth√∂chste Kirche der Welt.'}\n"
     ]
    }
   ],
   "source": [
    "texts = read_wikipedia_file(file_name)\n",
    "if len(texts) <= 0:\n",
    "    print(f\"{char_failure} No texts found in file {file_name}!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Read text from a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypdf = safe_import(\"pypdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function for reading text from a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"wanderer.pdf\"\n",
    "\n",
    "\n",
    "def get_pdf_text(file_name):\n",
    "    \"\"\"Read a single text from a pdf file\"\"\"\n",
    "    try:\n",
    "        reader = pypdf.PdfReader(file_name)\n",
    "    except:\n",
    "        print(f\"{char_failure} Cannot read pdf file {file_name}!\")\n",
    "        return []\n",
    "    if len(reader.pages) == 0:\n",
    "        print(f\"{char_failure} No text found in pdf file {file_name}!\")\n",
    "        return []\n",
    "    return [reader.pages[0].extract_text()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the function and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: Wanderer above the Sea of Fog is a painting by German Romanticist artist\n",
      "Caspar David Friedrich made in 1818. It depicts a man standing upon a rocky\n",
      "precipice with his back to the viewer; he is gazing out on a landscape covered\n",
      "in a thick sea of fog through which other ridges, trees, and mountains pierce,\n",
      "which stretches out into the distance indefinitely.\n"
     ]
    }
   ],
   "source": [
    "texts = get_pdf_text(\"wanderer.pdf\")\n",
    "if not texts:\n",
    "    print(f\"{char_failure} No texts were found. Are you connected to the internet?\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKvJmCRQHQUN"
   },
   "source": [
    "### 5.4. Download text from museum website\n",
    "\n",
    "We use the description of a painting by Claude Monet from the artwork Cleveland Museum website as example text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function for reading the texts from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bOSpuE_xsvgX"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://openaccess-api.clevelandart.org/api/artworks\"\n",
    "data_source = \"CMA\"\n",
    "\n",
    "\n",
    "def fetch_cma(query_string: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch metadata from Cleveland Museum of Art website\"\"\"\n",
    "    try:\n",
    "        response = requests.get(base_url, params={\"q\": query_string, \"skip\": 0, \"limit\": 100}, timeout=10)\n",
    "    except:\n",
    "        print(f\"{char_failure} Cannot download data from {base_url}\")\n",
    "        return []\n",
    "    response.raise_for_status()\n",
    "    artworks = response.json().get(\"data\", [])\n",
    "    if artworks == []:\n",
    "        return []\n",
    "    else:\n",
    "        return [{\"id\": artworks[0].get(\"id\"), \n",
    "                 \"data_source\": data_source,\n",
    "                 \"text_original\": (artworks[0].get(\"description\") or \"\").strip()}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1rjBgsgwkM4",
    "outputId": "a139382e-f994-4ce0-9fc4-627bd1f27bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 135382, 'data_source': 'CMA', 'text_original': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\"}\n"
     ]
    }
   ],
   "source": [
    "texts = fetch_cma(\"monet\")\n",
    "if not texts:\n",
    "    print(f\"{char_failure} No texts were found. Are you connected to the internet?\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YDuesoBf067g"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
