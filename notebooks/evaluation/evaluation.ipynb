{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BozTO4J6YDV"
   },
   "source": [
    "# Evaluation of three entity tasks\n",
    "\n",
    "This recipe describes how to perform the evaluation of **named entity recognition (NER)**, **entity disambiguation (ED)** and **entity linking (EL)** output in different scenarios:\n",
    "\n",
    "1. The user does have groundtruth data, where groundtruth data is the manually verified entity tags of entities found in a given text. In this case **quantitative evaluation** is possible.  \n",
    "2. The user does not have groundtruth data, but they are willing to manually inspect the NER output in order to spot and flag errors, inconsistencies, hallucinations, etc. In th|is case, **qualitative evaluation** is necessary. As this process is time-consuming, it can be supported by in-notebook visualizations for quick data inspection.  \n",
    "\n",
    "Note that the recipe only showcases a subset of the possible approaches, cf. [Variations and alternatives](#scrollTo=-riY-m5r7xv_&line=3&uniqifier=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5lOGUoI62D6"
   },
   "source": [
    "### Rationale\n",
    "\n",
    "These methods help the user assess the quality of **named entity recognition (NER)**, **entity disambiguation (ED)** and **entity linking (EL)** outputs. This is essential for any application, but especially when communicating with lay people, who often have reservations about new technologies.\n",
    "\n",
    "The cookbook also allows for evaluation both in a situation in which data comes with ground truth labels (quantitative evaluation) and in a situation where data is not labeled (qualitative evaluation, a.k.a. eye-balling).\n",
    "\n",
    "To run the quantitative evaluation with use the [`HIPE-scorer`](https://github.com/hipe-eval/HIPE-scorer), a set of Python scripts developed as part of the [HIPE shared task](https://hipe-eval.github.io/), focused on named entity processing of historical documents. As such, these scripts have certain requirements, for example when it comes to file naming or data format.\n",
    "\n",
    "Output data format can be fed to application recipes for visualizing and analyzing errors, making the estimation of the performance easier also for lay people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu11_x1n656e"
   },
   "source": [
    "### Process overview\n",
    "\n",
    "The evaluation module takes as input a tsv file where the first column is the token and the others are used to classify the token.\n",
    "\n",
    "If the file includes gold labels, the user can perform the quantitative evaluation of the annotated test data. The process uses the following steps:\n",
    "\n",
    "1. Installing the HIPE scorer\n",
    "2. Downloading the evaluation data and ground truths\n",
    "3. Reshape data to the format required by the scorer\n",
    "4. Running the scorer and saving the results\n",
    "\n",
    "If the file does not include gold labels, the cookbook returns a visualization of the annotation and gives the possibility to the user to give a free-text feedback about the annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5Apqc50tRhu"
   },
   "source": [
    "## 1. Evaluation preparation\n",
    "\n",
    "The notebook cells in this section contain the defintion of functions that are used further down in the notebook. These cells **must be run** but you don't need to inspect them closely unless you want to modify the behaviour of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we install some standard Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from docopt import docopt\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import regex\n",
    "import sys\n",
    "from termcolor import colored\n",
    "sys.path.insert(0, '../tasks')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fPMK5cuxbnn"
   },
   "source": [
    "Next, we test if the HIPE-scorer is available. If it is not, we install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 917,
     "status": "ok",
     "timestamp": 1757600746016,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "Qotqjj0iwqv3",
    "outputId": "b33f895d-9ea6-4ae7-b589-90ad5f25d73a"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "if not os.path.exists(\"HIPE-scorer/clef_evaluation.py\"):\n",
    "    ! git clone https://github.com/enriching-digital-heritage/HIPE-scorer.git\n",
    "    os.chdir(os.path.join(BASE_DIR, \"HIPE-scorer\"))\n",
    "    ! pip install -r requirements.txt\n",
    "    ! pip install .\n",
    "    os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the HIPE-scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(BASE_DIR, \"HIPE-scorer\"))\n",
    "import clef_evaluation\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation of named entity recognition\n",
    "\n",
    "Evaluation of named entity recognition performed by a large language model (LLM) is not trivial because current LLMs are not able to give back the exact position of the recognized entities. Therefore we first need to find the recognized entities back in the texts (function `get_char_offsets_of_entities`) before we can compare them with the correct gold standard data. For the comparison we use externaly developed software: the [HIPE-scorer](https://github.com/hipe-eval/HIPE-scorer). We need 12 helper functions for performing the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_offsets_of_entity(text_cleaned, entity_text):\n",
    "    \"\"\"lookup entity in original text and return list of character offsets\"\"\"\n",
    "    matches = regex.finditer(entity_text, text_cleaned)\n",
    "    char_offsets = []\n",
    "    for match in matches:\n",
    "        char_offsets.append({\"start_char\": match.start(), \"end_char\": match.end()})\n",
    "    return char_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_offsets_of_entities(text_id, text, raw_entities):\n",
    "    \"\"\"lookup list of entities in original text and return list with character offsets\"\"\"\n",
    "    cleaned_entities = []\n",
    "    for raw_entity in raw_entities:\n",
    "        char_offsets = get_char_offsets_of_entity(text, raw_entity[\"text\"])\n",
    "        if not char_offsets:\n",
    "            print(f\"raw entity not found in text! text_id: {text_id}; entity: {raw_entity['text']}\")\n",
    "            continue\n",
    "        cleaned_entities.extend([char_offset | {\"text\": raw_entity[\"text\"], \"label\": raw_entity[\"label\"]} \n",
    "                               for char_offset in char_offsets])\n",
    "    return cleaned_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overlapping_entities(entities):\n",
    "    \"\"\"remove overlapping entities from list of entities sorted by start offset\"\"\"\n",
    "    indices_to_be_deleted = []\n",
    "    for entity_index, entity in enumerate(entities):\n",
    "        if entity_index > 0:\n",
    "            if entities[entity_index - 1][\"end_char\"] > entities[entity_index][\"start_char\"]:\n",
    "                indices_to_be_deleted.append(entity_index)\n",
    "    for entity_index in reversed(indices_to_be_deleted):\n",
    "        entities.pop(entity_index)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines_from_file(file_name):\n",
    "    \"\"\"Read lines from the file file_name, return as list of strings\"\"\"\n",
    "    if type(file_name) != str:\n",
    "        return read_lines_from_stdin()\n",
    "    else:\n",
    "        with open(file_name, \"r\") as file_handle:\n",
    "            lines = [line.strip() for line in file_handle]\n",
    "        file_handle.close()\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cleaned_entities_to_tokens(text_tokens, cleaned_entities, text_id):\n",
    "    \"\"\"Add entity information to token information and return the combination\"\"\"\n",
    "    token_id = 0\n",
    "    tokens_with_machine_tags = []\n",
    "    for entity in cleaned_entities:\n",
    "        if entity[\"label\"] == \"PLACE\":\n",
    "            entity[\"label\"] = \"LOCATION\"\n",
    "        if entity[\"label\"] not in [\"LOCATION\", \"PERSON\"]:\n",
    "            continue\n",
    "        while token_id < len(text_tokens) and text_tokens[token_id][\"start\"] < entity[\"start_char\"]:\n",
    "            tokens_with_machine_tags.append(text_tokens[token_id] | {\"machine_tag\": \"O\"})\n",
    "            token_id += 1\n",
    "        if token_id < len(text_tokens):\n",
    "            if text_tokens[token_id][\"start\"] != entity[\"start_char\"]:\n",
    "                print(f\"clean entity not found in text! text_id: {text_id}; entity: {entity}; token: {text_tokens[token_id]}\")\n",
    "            else:\n",
    "                iob = \"B\"\n",
    "                while token_id < len(text_tokens) and text_tokens[token_id][\"end\"] <= entity[\"end_char\"]:\n",
    "                    tokens_with_machine_tags.append(text_tokens[token_id] | {\"machine_tag\": f\"{iob}-{entity['label']}\"})\n",
    "                    token_id += 1\n",
    "                    iob = \"I\"\n",
    "                if abs(text_tokens[token_id - 1][\"end\"] - entity[\"end_char\"]) > 1:\n",
    "                    print(f\"warning: machine token boundary error: text_id = {text_id}; entity: {entity}; token: {text_tokens[token_id - 1]}\")\n",
    "    while token_id < len(text_tokens):\n",
    "        tokens_with_machine_tags.append(text_tokens[token_id] | {\"machine_tag\": \"O\"})\n",
    "        token_id += 1\n",
    "    return tokens_with_machine_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_machine_data(file_name):\n",
    "    \"\"\"Read the output file of the NER task and return the contents as a list of text results\"\"\"\n",
    "    ner_json = utils.read_json_file(file_name)\n",
    "    text_tokens = []\n",
    "    for text_id, text in enumerate(ner_json):\n",
    "        cleaned_entities = get_char_offsets_of_entities(text_id, text[\"text_cleaned\"], text[\"entities\"])\n",
    "        tokens_with_machine_tags = add_cleaned_entities_to_tokens(text[\"tokens\"],\n",
    "                                                                  remove_overlapping_entities(sorted(cleaned_entities, \n",
    "                                                                                                     key=lambda entity: (entity[\"start_char\"], \n",
    "                                                                                                                         -entity[\"end_char\"]))),\n",
    "                                                                  text_id)\n",
    "        text_tokens.append(tokens_with_machine_tags)\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gold_entities_to_tokens(lines, text_tokens):\n",
    "    \"\"\"Add the golden standard entities to the text tokens\"\"\"\n",
    "    text_tokens_text_index = 0\n",
    "    text_tokens_token_index = 0\n",
    "    last_label = \"O\"\n",
    "    for line in lines:\n",
    "        if line == \"\" or line == f\"{DOC_SEPARATOR} {DOC_SEPARATOR}\":\n",
    "            continue\n",
    "        label, token = line.split()\n",
    "        if token != text_tokens[text_tokens_text_index][text_tokens_token_index][\"text\"]:\n",
    "            print(f\"token mismatch!: annotation: {token}; machine: {text_tokens[text_tokens_text_index][text_tokens_token_index]['text']}\")\n",
    "            break\n",
    "        elif label not in [\"p\", \"l\"]:\n",
    "            text_tokens[text_tokens_text_index][text_tokens_token_index][\"gold_tag\"] = \"O\"\n",
    "            last_label = label\n",
    "        else:\n",
    "            iob = \"I\" if last_label == label else \"B\"\n",
    "            text_tokens[text_tokens_text_index][text_tokens_token_index][\"gold_tag\"] = f\"{iob}-{label_names[label]}\"\n",
    "            last_label = label\n",
    "        text_tokens_token_index += 1\n",
    "        if text_tokens_token_index >= len(text_tokens[text_tokens_text_index]):\n",
    "            text_tokens_text_index += 1\n",
    "            text_tokens_token_index = 0\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIPE_MACHINE_FILE = \"HIPE_machine.txt\"\n",
    "HIPE_GOLD_FILE = \"HIPE_gold.txt\"\n",
    "\n",
    "\n",
    "def save_data_for_evaluation(text_tokens):\n",
    "    \"\"\"Save the data for evaluation to two files: on for the machine analysis and one for the golden standard\"\"\"\n",
    "    save_data_to_file(text_tokens, \"machine_tag\", HIPE_MACHINE_FILE)\n",
    "    save_data_to_file(text_tokens, \"gold_tag\", HIPE_GOLD_FILE)\n",
    "\n",
    "\n",
    "def save_data_to_file(text_tokens, data_column_name, file_name):\n",
    "    \"\"\"Save the data for evaluation to a single specified file\"\"\"\n",
    "    with open(file_name, \"w\") as hipe_file:\n",
    "        print(\"TOKEN\\tNE-COARSE-LIT\\tNE-COARSE-METO\\tMISC\", file=hipe_file)\n",
    "        for text in text_tokens:\n",
    "            sent_id = 0\n",
    "            for token in text:\n",
    "                if token[\"sent_id\"] != sent_id:\n",
    "                    print(\"-\\tO\\tO\\tO\", file=hipe_file)\n",
    "                    sent_id = token[\"sent_id\"]\n",
    "                print(token[\"text\"], token[data_column_name], token[data_column_name], \"O\", sep=\"\\t\", file=hipe_file)\n",
    "            print(\"-\\tO\\tO\\tO\", file=hipe_file)\n",
    "        hipe_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS_VALUES = {'--glue': None, '--help': False, '--hipe_edition': 'hipe-2020',\n",
    " '--log': 'log.txt', '--n_best': '1', '--noise-level': None,\n",
    " '--original_nel': False, '--skip-check': False, '--suffix': None,\n",
    " '--tagset': None, '--time-period': None}\n",
    "\n",
    "\n",
    "def run_scorer(args):\n",
    "    \"\"\"Run the HIPE-scorer to perform the evaluation\"\"\"\n",
    "    tasks = (\"nerc_coarse\", \"nel\")\n",
    "    if args[\"--task\"] not in tasks:\n",
    "        msg = \"Please restrict to one of the available evaluation tasks: \" + \", \".join(tasks)\n",
    "        logging.error(msg)\n",
    "        sys.exit(1)\n",
    "    logging.debug(f\"ARGUMENTS {args}\")\n",
    "    clef_evaluation.main(args=args | ARGS_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORER_DIR = \"HIPE-scorer\"\n",
    "\n",
    "def evaluate(text_tokens):\n",
    "    \"\"\"Evaluate the data present in text tokens: save them, run the scorer and show the results\"\"\"\n",
    "    save_data_for_evaluation(text_tokens)\n",
    "    os.chdir(os.path.join(BASE_DIR, SCORER_DIR))\n",
    "    run_scorer(args={\"--ref\": os.path.join(\"..\", HIPE_GOLD_FILE),\n",
    "                     \"--pred\": os.path.join(\"..\", HIPE_MACHINE_FILE),\n",
    "                     \"--task\": \"nerc_coarse\",\n",
    "                     \"--outdir\": \".\"})\n",
    "    os.chdir(BASE_DIR)\n",
    "    show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"PERSON\", \"LOCATION\"]\n",
    "\n",
    "def show_scores(labels=LABELS):\n",
    "    \"\"\"Extract relevant evaluation scores from the output file of the scorer and show them\"\"\"\n",
    "    hipe_scores = utils.read_json_file(os.path.join(SCORER_DIR, HIPE_MACHINE_FILE))\n",
    "    print(colored(\"HIPE Analysis\", attrs=[\"bold\"]))\n",
    "    for label in labels:\n",
    "        print(f\"{label:>8}:\", {key: round(100*value, 1) \n",
    "              for key, value in hipe_scores['NE-COARSE-LIT']['TIME-ALL']['LED-ALL'][label]['exact'].items() \n",
    "              if key in ['P_micro', 'R_micro', 'F1_micro']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the main commands for the named entity recognition evaluation:\n",
    "\n",
    "1. Read the output of the machine\n",
    "2. Read the golden standard annotations\n",
    "3. Align the machine output with the annotations\n",
    "4. Perform the evaluation with the HIPE-scorer and show the results\n",
    "\n",
    "We evaluate the analysis from the NER file \"output_ner_5f12fa7c16d33ea378148197569f999f774f7481.json\". Change the name of this file to one of your own to analyse your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: machine token boundary error: text_id = 9; entity: {'start_char': 15, 'end_char': 26, 'text': 'Amenhotep I', 'label': 'PERSON'}; token: {'id': 3, 'text': 'Amenhotep', 'start': 15, 'end': 24, 'ws': True, 'is_punct': False, 'sent_id': 0}\n",
      "true: 1 pred: 1\n",
      "data_format_true [[3790]]\n",
      "data_format_pred [[3790]]\n",
      "\u001b[1mHIPE Analysis\u001b[0m\n",
      "  PERSON: {'P_micro': 88.5, 'R_micro': 78.0, 'F1_micro': 82.9}\n",
      "LOCATION: {'P_micro': 53.4, 'R_micro': 78.0, 'F1_micro': 63.4}\n"
     ]
    }
   ],
   "source": [
    "NER_OUTPUT_FILE = \"../tasks/output_ner_5f12fa7c16d33ea378148197569f999f774f7481.json\"\n",
    "NER_ANNOTATIONS_FILE = \"Wikimedia-random-100-annotations.txt\"\n",
    "DOC_SEPARATOR = \"-DOCSTART-\"\n",
    "label_names = {\"p\": \"PERSON\", \"l\": \"LOCATION\"}\n",
    "\n",
    "text_tokens = read_machine_data(NER_OUTPUT_FILE)\n",
    "gold_lines = read_lines_from_file(NER_ANNOTATIONS_FILE)\n",
    "text_tokens =  add_gold_entities_to_tokens(gold_lines, text_tokens)\n",
    "evaluate(text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation of named entity linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gold_linking_entities_to_text_tokens(text_tokens, gold_entities):\n",
    "    text_tokens_text_id = 0\n",
    "    text_tokens_token_id = 0\n",
    "    for gold_entities_text in gold_entities:\n",
    "        for gold_entity in gold_entities_text:\n",
    "            while text_tokens[text_tokens_text_id][text_tokens_token_id][\"start\"] < gold_entity[\"start\"]:\n",
    "                text_tokens[text_tokens_text_id][text_tokens_token_id][\"wikidata_id\"] = \"_\"\n",
    "                text_tokens_token_id += 1\n",
    "                if text_tokens_token_id >= len(text_tokens[text_tokens_text_id]):\n",
    "                    break\n",
    "            while text_tokens_token_id < len(text_tokens[text_tokens_text_id]) and text_tokens[text_tokens_text_id][text_tokens_token_id][\"end\"] <= gold_entity[\"end\"]:\n",
    "                if \"wikidata_id\" not in gold_entity:\n",
    "                    text_tokens[text_tokens_text_id][text_tokens_token_id][\"wikidata_id\"] = \"_\"\n",
    "                else:\n",
    "                   text_tokens[text_tokens_text_id][text_tokens_token_id][\"wikidata_id\"] = gold_entity[\"wikidata_id\"][\"id\"]\n",
    "                text_tokens_token_id += 1\n",
    "        while text_tokens_token_id < len(text_tokens[text_tokens_text_id]):\n",
    "            text_tokens[text_tokens_text_id][text_tokens_token_id][\"wikidata_id\"] = \"_\"\n",
    "            text_tokens_token_id += 1\n",
    "        text_tokens_text_id += 1\n",
    "        text_tokens_token_id = 0\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_offsets_of_entities_linking(text_id, text, raw_entities):\n",
    "    \"\"\"lookup list of entities in original text and return list with character offsets\"\"\"\n",
    "    cleaned_entities = []\n",
    "    for raw_entity in raw_entities:\n",
    "        char_offsets = get_char_offsets_of_entity(text, raw_entity[\"text\"])\n",
    "        if not char_offsets:\n",
    "            print(f\"raw entity not found in text! text_id: {text_id}; entity: {raw_entity['text']}\")\n",
    "            continue\n",
    "        cleaned_entities.extend([char_offset | {\"text\": raw_entity[\"text\"], \"wikidata_id\": raw_entity[\"wikidata_id\"]} \n",
    "                               for char_offset in char_offsets])\n",
    "    return cleaned_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine_entities(file_name):\n",
    "    linking_json = utils.read_json_file(LINKING_OUTPUT_FILE)\n",
    "    machine_entities = []\n",
    "    for text_id, text in enumerate(linking_json):\n",
    "        cleaned_entities = get_char_offsets_of_entities_linking(text_id, text[\"text_cleaned\"], text[\"entities\"])\n",
    "        cleaned_entities = remove_overlapping_entities(sorted(cleaned_entities, \n",
    "                                                              key=lambda entity: (entity[\"start_char\"], \n",
    "                                                                                  -entity[\"end_char\"])))\n",
    "        for cleaned_entity in cleaned_entities:\n",
    "            cleaned_entity[\"start\"] = cleaned_entity.pop(\"start_char\")\n",
    "            cleaned_entity[\"end\"] = cleaned_entity.pop(\"end_char\")\n",
    "        machine_entities.append(cleaned_entities)\n",
    "    return machine_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIPE_MACHINE_FILE = \"HIPE_machine.txt\"\n",
    "HIPE_GOLD_FILE = \"HIPE_gold.txt\"\n",
    "\n",
    "def save_linking_data_to_file(text_tokens, data_column_name, file_name):\n",
    "    \"\"\"Save the data for evaluation to a single specified file\"\"\"\n",
    "    with open(file_name, \"w\") as hipe_file:\n",
    "        print(\"TOKEN\\tNE-COARSE-LIT\\tNE-COARSE-METO\\tNEL-LIT\\tNEL-METO\\tMISC\", file=hipe_file)\n",
    "        for text in text_tokens:\n",
    "            sent_id = 0\n",
    "            for token in text:\n",
    "                if token[\"sent_id\"] != sent_id:\n",
    "                    print(\"-\\tO\\tO\\t_\\t_\\tO\", file=hipe_file)\n",
    "                    sent_id = token[\"sent_id\"]\n",
    "                print(token[\"text\"], token[data_column_name], token[data_column_name], \n",
    "                                     token[\"wikidata_id\"], token[\"wikidata_id\"], \"O\", sep=\"\\t\", file=hipe_file)\n",
    "            print(\"-\\tO\\tO\\t_\\t_\\tO\", file=hipe_file)\n",
    "        hipe_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORER_DIR = \"HIPE-scorer\"\n",
    "\n",
    "def evaluate_linking(text_tokens_gold, text_tokens_machine):\n",
    "    \"\"\"Evaluate the data present in text tokens: save them, run the scorer and show the results\"\"\"\n",
    "    save_linking_data_to_file(text_tokens_gold, \"gold_tag\", HIPE_GOLD_FILE)\n",
    "    save_linking_data_to_file(text_tokens_machine, \"machine_tag\", HIPE_MACHINE_FILE)\n",
    "    os.chdir(os.path.join(BASE_DIR, SCORER_DIR))\n",
    "    run_scorer(args={\"--ref\": os.path.join(\"..\", HIPE_GOLD_FILE),\n",
    "                     \"--pred\": os.path.join(\"..\", HIPE_MACHINE_FILE),\n",
    "                     \"--task\": \"nel\",\n",
    "                     \"--outdir\": \".\"})\n",
    "    os.chdir(BASE_DIR)\n",
    "    show_scores_linking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_scores_linking(labels=[\"ALL\"]):\n",
    "    \"\"\"Extract relevant evaluation scores from the output file of the scorer and show them\"\"\"\n",
    "    hipe_scores = utils.read_json_file(os.path.join(SCORER_DIR, HIPE_MACHINE_FILE))\n",
    "    print(colored(\"HIPE Analysis\", attrs=[\"bold\"]))\n",
    "    for label in labels:\n",
    "        print(f\"{label}:\", {key: round(100*value, 1) \n",
    "              for key, value in hipe_scores['1']['NEL-LIT']['TIME-ALL']['LED-ALL'][label]['exact'].items() \n",
    "              if regex.search(\"micro\", key)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true: 1 pred: 1\n",
      "data_format_true [[3790]]\n",
      "data_format_pred [[3790]]\n",
      "\u001b[1mHIPE Analysis\u001b[0m\n",
      "ALL: {'P_micro': 62.1, 'R_micro': 78.6, 'F1_micro': 69.4}\n"
     ]
    }
   ],
   "source": [
    "LINKING_OUTPUT_FILE = \"../tasks/output_linking_34e26bfd19c837e400a5fcb214cd1e7a25304a12.json\"\n",
    "\n",
    "gold_entities = utils.read_json_file(\"linking_gold.json\")\n",
    "text_tokens_gold = add_gold_linking_entities_to_text_tokens(copy.deepcopy(text_tokens), gold_entities)\n",
    "\n",
    "machine_entities = get_machine_entities(LINKING_OUTPUT_FILE)\n",
    "text_tokens_machine = add_gold_linking_entities_to_text_tokens(copy.deepcopy(text_tokens), machine_entities)\n",
    "\n",
    "evaluate_linking(text_tokens_gold, text_tokens_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare analysis with gold data\n",
    "\n",
    "We need helper functions for accessing the HIPE-scorer, for converting the data to the scorer format and for interpreting the analysis of the scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_value(entity, label):\n",
    "    if label not in entity:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return entity[label][list(entity[label].keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_FILE_TASKS = \"../tasks/output_linking_34e26bfd19c837e400a5fcb214cd1e7a25304a12.json\"\n",
    "\n",
    "with open(os.path.join(BASE_DIR, PRED_FILE_TASKS), \"r\") as infile:\n",
    "    pred_data = json.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "all_tokens = []\n",
    "for text in pred_data:\n",
    "    tokens = {token[\"start\"]: {\"text\": token[\"text\"], \"label\": \"O\"}\n",
    "              for token in text[\"tokens\"]}\n",
    "    next_id = -1\n",
    "    for char_id in reversed(tokens.keys()):\n",
    "        tokens[char_id][\"next_id\"] = next_id\n",
    "        next_id = char_id\n",
    "    for entity in text[\"entities\"]:\n",
    "        try:\n",
    "            wikidata_id = get_label_value(entity, \"wikidata_id\")\n",
    "            link = get_label_value(entity, \"link\")\n",
    "            tokens[entity[\"start_char\"]][\"label\"] = \"B-\" + entity[\"label\"]\n",
    "            tokens[entity[\"start_char\"]][\"wikidata_id\"] = wikidata_id\n",
    "            tokens[entity[\"start_char\"]][\"link\"] = link \n",
    "            next_id = tokens[entity[\"start_char\"]][\"next_id\"]\n",
    "            while next_id >= 0 and next_id < entity[\"end_char\"]:\n",
    "                tokens[next_id][\"label\"] = \"I-\" + entity[\"label\"]\n",
    "                tokens[next_id][\"wikidata_id\"] = wikidata_id\n",
    "                tokens[next_id][\"link\"] = link\n",
    "                next_id = tokens[next_id][\"next_id\"]\n",
    "        except Exception as e:\n",
    "            print(f\"warning: no token at character index {e}\")\n",
    "    all_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_FILE = \"example.tsv\"\n",
    "GOLD_FILE = \"example.tsv\"\n",
    "SCORER_DIR = \"HIPE-scorer\"\n",
    "os.chdir(os.path.join(BASE_DIR, \"HIPE-scorer\"))\n",
    "run_scorer(args={\"--ref\": GOLD_FILE,\n",
    "                 \"--pred\": PRED_FILE,\n",
    "                 \"--task\": \"nerc_coarse\",\n",
    "                 \"--outdir\": \"RESULT_FOLDER\"})\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXj6rEaox9DE"
   },
   "source": [
    "## 5. Preparation for manual assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jUrkTLcuawP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Function for visualising the entities with link\n",
    "\n",
    "def highlight_entities(\n",
    "    data,\n",
    "    iob_column = \"NE-COARSE-LIT\",\n",
    "    base_url=\"https://www.wikidata.org/wiki/\"\n",
    "    ):\n",
    "    # 1) Rebuild the text with spacing rules\n",
    "    text_parts = []\n",
    "    for idx, row in data.iterrows():\n",
    "        tok = row[\"TOKEN\"]\n",
    "        if \"NoSpaceAfter\" in row[\"MISC\"]:\n",
    "            text_parts.append(tok)\n",
    "        else:\n",
    "            text_parts.append(tok + \" \")\n",
    "    text = \"\".join(text_parts)\n",
    "\n",
    "    # 2) Merge contiguous IOB entities of the same type\n",
    "    entities = []\n",
    "    current = None  # {\"start_char\": int, \"end_char\": int, \"label\": str}\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        tag = row[iob_column]\n",
    "        if tag == \"_\" or tag == \"O\":\n",
    "            # close any open entity\n",
    "            if current is not None:\n",
    "                entities.append(current)\n",
    "                current = None\n",
    "            continue\n",
    "\n",
    "        # Extract type and prefix\n",
    "        if tag.startswith(\"B-\"):\n",
    "            etype = tag[2:]\n",
    "            eid = row[\"NEL-LIT\"]\n",
    "            # close previous if open\n",
    "            if current is not None:\n",
    "                entities.append(current)\n",
    "            # start new\n",
    "            current = {\n",
    "                \"start_char\": int(row[\"start_char\"]),\n",
    "                \"end_char\": int(row[\"end_char\"]),\n",
    "                \"label\": etype,\n",
    "                \"id\": eid\n",
    "            }\n",
    "\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            etype = tag[2:]\n",
    "            eid = row[\"NEL-LIT\"]\n",
    "            if current is not None and current[\"label\"] == etype:\n",
    "                # extend current run\n",
    "                current[\"end_char\"] = int(row[\"end_char\"])\n",
    "            else:\n",
    "                # stray I- (no open run or different type) → treat as B-\n",
    "                if current is not None:\n",
    "                    entities.append(current)\n",
    "                current = {\n",
    "                    \"start_char\": int(row[\"start_char\"]),\n",
    "                    \"end_char\": int(row[\"end_char\"]),\n",
    "                    \"label\": etype,\n",
    "                    \"id\": eid\n",
    "                }\n",
    "\n",
    "        else:\n",
    "            # Unknown tag → close any open entity\n",
    "            if current is not None:\n",
    "                entities.append(current)\n",
    "                current = None\n",
    "\n",
    "    # flush any remaining entity\n",
    "    if current is not None:\n",
    "        entities.append(current)\n",
    "\n",
    "    # 3) Render with spans (note: end_char is inclusive → slice to en+1)\n",
    "    entities.sort(key=lambda e: e[\"start_char\"])\n",
    "    result = \"\"\n",
    "    last_idx = 0\n",
    "\n",
    "    for e in entities:\n",
    "        s, en = int(e[\"start_char\"]), int(e[\"end_char\"])\n",
    "        etext = text[s:en + 1]  # inclusive end\n",
    "        etype = e.get(\"label\", \"Other\")\n",
    "        eid = e.get(\"id\", \"\")\n",
    "        color = label_to_color.get(etype, \"#dddddd\")\n",
    "\n",
    "        # decide whether to show eid as link or not\n",
    "        if eid !=\"_\" and eid !=\"NIL\":\n",
    "          eid_html = f'<a href=\"{base_url}{eid}\">{eid}</a>'\n",
    "        else:\n",
    "          eid_html = \"\"  # if entity linking was not successful no link is shown\n",
    "\n",
    "        result += text[last_idx:s]\n",
    "        result += (\n",
    "            f'<span style=\"background-color:{color}; color:black; padding:3px 6px; '\n",
    "            f'border-radius:16px; margin:0 2px; display:inline-block; '\n",
    "            f'box-shadow: 1px 1px 3px rgba(0,0,0,0.1);\">'\n",
    "            f'{etext}'\n",
    "            f'<span style=\"font-size:0.75em; font-weight:normal; margin-left:6px; '\n",
    "            f'background-color:rgba(0,0,0,0.05); padding:1px 6px; border-radius:12px;\">'\n",
    "            f'{etype} {eid_html}</span></span>'\n",
    "        )\n",
    "        last_idx = en + 1  # continue after inclusive end\n",
    "\n",
    "    result += text[last_idx:]\n",
    "    return result,text,entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNlHNhvOzIRv"
   },
   "source": [
    "## 6. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3We7GDtnzMpc"
   },
   "outputs": [],
   "source": [
    "# we need a function to convert the British Museum groundtruth data into the format expected by the HIPE scorer.\n",
    "# The data is found here: http://145.38.185.232/enriching/bm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZYqwx_j0gNv"
   },
   "outputs": [],
   "source": [
    "def get_demo_data():\n",
    "  import pandas as pd\n",
    "  # Loading the data\n",
    "  data = pd.read_csv(\n",
    "      'https://raw.githubusercontent.com/wjbmattingly/llm-lod-recipes/refs/heads/main/output/sample.tsv',\n",
    "      sep='\\t'\n",
    "  )\n",
    "\n",
    "  # Adding the start and end characters per token to the dataframe\n",
    "  data['start_char'] = 0\n",
    "  data['end_char'] = 0\n",
    "  current_char = 0\n",
    "\n",
    "  for index, row in data.iterrows():\n",
    "      data.loc[index, 'start_char'] = current_char\n",
    "      token = row['TOKEN']\n",
    "      # Check if the next token should not have a space before it\n",
    "      if index + 1 < len(data) and 'NoSpaceAfter' in data.loc[index, 'MISC']:\n",
    "          current_char += len(token)\n",
    "      else:\n",
    "          current_char += len(token) + 1  # Add 1 for the space after the token\n",
    "\n",
    "      data.loc[index, 'end_char'] = current_char - 1 # Subtract 1 because end_char is inclusive\n",
    "\n",
    "  # Just for testing purposes: adds a Wikidata ID to one entity\n",
    "  data.loc[0, 'NEL-LIT'] = 'Q1744'\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXdVkbdB1MyL"
   },
   "outputs": [],
   "source": [
    "df = get_demo_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1757600777541,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "XkjQCP9D3pla"
   },
   "outputs": [],
   "source": [
    "# preparing data for groundtruth evaluation\n",
    "import regex as re\n",
    "import csv\n",
    "\n",
    "def clean_format(input_file,output_file):\n",
    "  file = open(input_file)\n",
    "  output = open(output_file,mode='w')\n",
    "  reader = file.readlines() #(file,delimiter=\"\\t\")\n",
    "  #writer = csv.writer(output,delimiter=\"\\t\")\n",
    "  output.write('\\t'.join([\"TOKEN\",\"NE-COARSE-LIT\",\"NE-COARSE-METO\",\"NE-FINE-LIT\",\"NE-FINE-METO\",\"NE-FINE-COMP\",\"NE-NESTED\",\"NEL-LIT\",\"NEL-METO\",\"MISC\\n\"]))\n",
    "  i = 1\n",
    "  for line in reader:\n",
    "    line = line.strip()\n",
    "    mod_line = re.sub('_','-',line)\n",
    "    if re.search('-DOCSTART- -DOCSTART- -DOCSTART-',mod_line):\n",
    "      mod_line = re.sub('-DOCSTART- -DOCSTART- -DOCSTART-',f'# document_{i}',mod_line)\n",
    "      i+=1\n",
    "    mod_line = mod_line.split(' ')\n",
    "    try:\n",
    "      if len(mod_line)==2:\n",
    "        output.write(\"\\n\"+\" \".join(mod_line)+\"\\n\")\n",
    "      else:\n",
    "        output.write('\\t'.join([mod_line[0],mod_line[1],'-','-','-','-','-',mod_line[2],'-','-\\n',]))\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1757600829932,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "n88gpP0NEXbC"
   },
   "outputs": [],
   "source": [
    "clean_format('./data/bm_labels.txt','./data/gold/sample.tsv')\n",
    "clean_format('./data/bm-2-ner-format.txt','./data/predictions/sample.tsv')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kw9pN2CwP4r"
   },
   "source": [
    "## 7. NER evaluation with groundtruth data by using the HIPE scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1757596009513,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "LZUcJ2GwxJV8",
    "outputId": "b8a0bd41-4023-4653-a365-8e772e53e1a9"
   },
   "outputs": [],
   "source": [
    "! python clef_evaluation.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPsSJaqBxgC9"
   },
   "source": [
    "### 7.1 Running the scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_D7MR5jvy9M"
   },
   "source": [
    " ‼️ In the cell below it is important to note the parameter `--task`. This parameter value needs to be adjusted depending on the task one wants to evaluate (i.e. NER or EL). When evaluating NER we use `--task nerc_coarse`, while for evaluating EL we use `--task nel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 911,
     "status": "ok",
     "timestamp": 1757600924627,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "2rBJAfJK95s7",
    "outputId": "08386331-8b5e-4d3b-8dbd-b7c2b9fb6f55"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import regex as re\n",
    "\n",
    "for doc in glob.glob('./data/predictions/*'):\n",
    "  gold = re.sub('predictions','gold',doc)\n",
    "  ! python clef_evaluation.py --ref \"{gold}\" --pred \"{doc}\" --task nerc_coarse --outdir ./data/evaluations/ --hipe_edition hipe-2020 --log=./data/evaluations/scorer.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t9DO4bGxkfb"
   },
   "source": [
    "Let's now look at the various bits of data produced by the scorer. They are in the folder specified in the `--outdir` parameter of the scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1757594115035,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "HIDU3Ybe0AQG",
    "outputId": "1a053c59-f380-4b21-cdd0-d59bb76fe375"
   },
   "outputs": [],
   "source": [
    "ls -la ./evaluation_data/evaluations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoP5jug3zZL7"
   },
   "source": [
    "Here is an overview of the files created by the scorer:\n",
    "- `scorer.log` – the log produced by the scorer\n",
    "- `01_sample_nerc_coarse.tsv` – a TSV file contaning the evaluation results for document `01_sample` and task `nerc_coarse`, at different levels of aggregation etc.\n",
    "- `01_sample_nerc_coarse.json` – a JSON file with a more granular breakdown of the evaluation, which can be useful for error analysis and to better understand systems' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1757594273845,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "2MQra_sox9wM",
    "outputId": "c4e32312-8d4a-4413-c60d-87f8c41f1391"
   },
   "outputs": [],
   "source": [
    "! cat ./evaluation_data/evaluations/scorer.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk6BsUcsyX_5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('./evaluation_data/evaluations/01_sample_nerc_coarse.tsv', sep='\\t')\n",
    "eval_df.drop(columns=['System'], inplace=True)\n",
    "eval_df.set_index('Evaluation', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nJV-PQZ1hTL"
   },
   "source": [
    "Let's print the **micro-averaged** precision, recall and F-score in a **strict** evaluation regime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1757595176524,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "fFl1avvF0dM-",
    "outputId": "18c3029e-9c35-43b1-a623-d5e61bc4c5a9"
   },
   "outputs": [],
   "source": [
    "eval_df.loc['NE-COARSE-LIT-micro-strict-TIME-ALL-LED-ALL'][['P', 'R', 'F1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZnXDpc81sl8"
   },
   "source": [
    "Let's print the **micro-averaged** precision, recall and F-score in a **fuzzy** evaluation regime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1757595138721,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "fBs739hiygL6",
    "outputId": "8056f19a-e18f-446f-a0b9-a97f96b199f0"
   },
   "outputs": [],
   "source": [
    "# Let's print the micro-averaged precision, recall and F-score in a *fuzzy* evaluation regime\n",
    "eval_df.loc['NE-COARSE-LIT-micro-fuzzy-TIME-ALL-LED-ALL'][['P', 'R', 'F1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzLqjECH1y9g"
   },
   "source": [
    "There is more data in the TSV file, as it can be seen when printing the whole content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1757595297145,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "gGkGzpA514JI",
    "outputId": "91d9c9a3-83e6-4d0b-ac15-149b334c788a"
   },
   "outputs": [],
   "source": [
    "! cat ./evaluation_data/evaluations/01_sample_nerc_coarse.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quWWEXdylgGn"
   },
   "source": [
    "## 8. Manual assessment of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cSUXxR32rlg"
   },
   "source": [
    "### 8.1. Displaying the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL_HOz9Alk7c"
   },
   "source": [
    "To allow for manual assessment of output quality, the following cells display the identified entities in form of color-coded annotations via HTML. These kinds of insights into the results can both complement quantitative statistics and work as another way to estimate output quality. The latter is especially important for the frequent cases where no gold (or silver or bronze) standard is available that the NER output can be evaluated on. (Also, note that there are also other tools or modules out there that provide similar visualisations, e.g. [displaCy](https://spacy.io/usage/visualizers#ent) when using spaCy for NER.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1757514964781,
     "user": {
      "displayName": "Nina Rastinger",
      "userId": "04623370430015730380"
     },
     "user_tz": -120
    },
    "id": "Tn3rTmWD1Wia",
    "outputId": "a6e18557-8b7d-41ac-9823-4a8b51407286"
   },
   "outputs": [],
   "source": [
    "data = get_demo_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1757514964787,
     "user": {
      "displayName": "Nina Rastinger",
      "userId": "04623370430015730380"
     },
     "user_tz": -120
    },
    "id": "1GNxQ16E9rc5",
    "outputId": "19f56c06-ff88-4c04-c773-e4f55e6b44cf"
   },
   "outputs": [],
   "source": [
    "# Highlighting the identified entities in form of color-coded annotations with links to authority files where available\n",
    "\n",
    "# Color palette - add more colors if more labels are used or change them here\n",
    "colors = ['#F0D9EF', '#FCDCE1', '#FFE6BB', '#E9ECCE', '#CDE9DC', '#C4DFE5', '#D9E5F0', '#F0E6D9', '#E0D9F0', '#E6FFF0', '#9CC5DF']\n",
    "\n",
    "# Name Labels that should be shown in color, not mentioned labels will be shown in grey (this makes it easier to focus on certain categories if needed)\n",
    "labels = [\"PERSON\", \"DATE\"]\n",
    "\n",
    "# Mapping each label from the label set to a color from the palette\n",
    "label_to_color = {label: colors[i % len(colors)] for i, label in enumerate(labels)}\n",
    "\n",
    "# Generating the HTML - two changes can be made here:\n",
    "# 1) by default, the column \"NE-COARSE-LIT\" is used for the entities, this can be changed via the argument \"iob_column\"\n",
    "# 2) the entity identifiers are taken from the column \"NEL-LIT\"; by default, these are expected to be Wikidata identifiers (e.g. Q1744) and are combined with the Wikidate base URL; for another authority file, the base URL can be changed via the argument \"base_url\"\n",
    "res,text,entities = highlight_entities(data)\n",
    "\n",
    "# displaying the annotations\n",
    "display(HTML(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIBej6Wf2R6P"
   },
   "source": [
    "### 8.2. Giving feedback on the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUZoSndbyQkB"
   },
   "source": [
    "The following cell gives a very simple example for how manual assessment of entities could be integrated into the data. Here, the user is asked for feedback on each identified entity which then shows up in designated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "id": "XbQAPhmq2Rmt",
    "outputId": "38221ef7-e629-4010-8fc5-9b22d8b4d3b8"
   },
   "outputs": [],
   "source": [
    "# Create a new column for manual assessment\n",
    "data['manual_assessment'] = ''\n",
    "\n",
    "# Display results again for better overview (no scrolling back and front)\n",
    "display(HTML(res))\n",
    "\n",
    "# Iterate through the identified entities\n",
    "for e in entities:\n",
    "    s, en = int(e[\"start_char\"]), int(e[\"end_char\"])\n",
    "    etext = text[s:en + 1]\n",
    "    etype = e.get(\"label\", \"Other\")\n",
    "\n",
    "    # Ask for feedback on the entity\n",
    "    feedback = input(f'Is the entity \"{etext.strip()}\" with label \"{etype}\" correct? (y/n/feedback): ')\n",
    "\n",
    "    # Store the feedback for all tokens within the entity span\n",
    "    for index, row in data.iterrows():\n",
    "        token_start = int(row[\"start_char\"])\n",
    "        token_end = int(row[\"end_char\"])\n",
    "        if max(s, token_start) <= min(en, token_end):\n",
    "            data.loc[index, 'manual_assessment'] = feedback\n",
    "\n",
    "# Show the altered data (now with user assessments)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1svSZ93EJlfz"
   },
   "source": [
    "## 9. EL evaluation with groundtruth data\n",
    "\n",
    "TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-riY-m5r7xv_"
   },
   "source": [
    "## 10. Variations and alternatives\n",
    "\n",
    "Another approach if the user does not have groundtruth data could be to use an **LLM-judge** approach to evaluate the NER output in absence of labelled golden data. The task of the LLM is then to “review” the NER output and to assess its quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sXj6rEaox9DE",
    "-cSUXxR32rlg",
    "hIBej6Wf2R6P"
   ],
   "provenance": [
    {
     "file_id": "1haoEa5gwTMuohrCRqLYtiJfuAjBxUPDY",
     "timestamp": 1757498886116
    },
    {
     "file_id": "1WXzG2eRZDc0Cxi212s-6guzd-zr1qIvm",
     "timestamp": 1757421007606
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
