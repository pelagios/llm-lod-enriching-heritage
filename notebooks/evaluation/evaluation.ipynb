{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BozTO4J6YDV"
   },
   "source": [
    "# Evaluation of three entity tasks\n",
    "\n",
    "This recipe describes how to perform the evaluation of **named entity recognition (NER)**, **entity disambiguation (ED)** and **entity linking (EL)** output in different scenarios:\n",
    "\n",
    "1. The user does have groundtruth data, where groundtruth data is the manually verified entity tags of entities found in a given text. In this case **quantitative evaluation** is possible.  \n",
    "2. The user does not have groundtruth data, but they are willing to manually inspect the NER output in order to spot and flag errors, inconsistencies, hallucinations, etc. In th|is case, **qualitative evaluation** is necessary. As this process is time-consuming, it can be supported by in-notebook visualizations for quick data inspection.  \n",
    "\n",
    "Note that the recipe only showcases a subset of the possible approaches, cf. [Variations and alternatives](#scrollTo=-riY-m5r7xv_&line=3&uniqifier=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5lOGUoI62D6"
   },
   "source": [
    "### Rationale\n",
    "\n",
    "These methods help the user assess the quality of **named entity recognition (NER)**, **entity disambiguation (ED)** and **entity linking (EL)** outputs. This is essential for any application, but especially when communicating with lay people, who often have reservations about new technologies.\n",
    "\n",
    "The cookbook also allows for evaluation both in a situation in which data comes with ground truth labels (quantitative evaluation) and in a situation where data is not labeled (qualitative evaluation, a.k.a. eye-balling).\n",
    "\n",
    "To run the quantitative evaluation with use the [`HIPE-scorer`](https://github.com/hipe-eval/HIPE-scorer), a set of Python scripts developed as part of the [HIPE shared task](https://hipe-eval.github.io/), focused on named entity processing of historical documents. As such, these scripts have certain requirements, for example when it comes to file naming or data format.\n",
    "\n",
    "Output data format can be fed to application recipes for visualizing and analyzing errors, making the estimation of the performance easier also for lay people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu11_x1n656e"
   },
   "source": [
    "### Process overview\n",
    "\n",
    "The evaluation module takes as input a tsv file where the first column is the token and the others are used to classify the token.\n",
    "\n",
    "If the file includes gold labels, the user can perform the quantitative evaluation of the annotated test data. The process uses the following steps:\n",
    "\n",
    "1. Installing the HIPE scorer\n",
    "2. Downloading the evaluation data and ground truths\n",
    "3. Reshape data to the format required by the scorer\n",
    "4. Running the scorer and saving the results\n",
    "\n",
    "If the file does not include gold labels, the cookbook returns a visualization of the annotation and gives the possibility to the user to give a free-text feedback about the annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5Apqc50tRhu"
   },
   "source": [
    "## 1. Evaluation preparation\n",
    "\n",
    "The notebook cells in this section contain the defintion of functions that are used further down in the notebook. These cells **must be run** but you don't need to inspect them closely unless you want to modify the behaviour of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we install some standard Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docopt import docopt\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../tasks')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fPMK5cuxbnn"
   },
   "source": [
    "Next, we test if the HIPE-scorer is available. If it is not, we install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 917,
     "status": "ok",
     "timestamp": 1757600746016,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "Qotqjj0iwqv3",
    "outputId": "b33f895d-9ea6-4ae7-b589-90ad5f25d73a"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "if not os.path.exists(\"HIPE-scorer/clef_evaluation.py\"):\n",
    "    ! git clone https://github.com/enriching-digital-heritage/HIPE-scorer.git\n",
    "    os.chdir(os.path.join(BASE_DIR, \"HIPE-scorer\"))\n",
    "    ! pip install -r requirements.txt\n",
    "    ! pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the HIPE-scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(BASE_DIR, \"HIPE-scorer\"))\n",
    "import clef_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare analysis with gold data\n",
    "\n",
    "We need helper functions for accessing the HIPE-scorer, for converting the data to the scorer format and for interpreting the analysis of the scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS_VALUES = {'--glue': None, '--help': False, '--hipe_edition': 'hipe-2020',\n",
    " '--log': 'log.txt', '--n_best': '1', '--noise-level': None,\n",
    " '--original_nel': False, '--skip-check': False, '--suffix': None,\n",
    " '--tagset': None, '--task': 'nerc_coarse', '--time-period': None}\n",
    "\n",
    "\n",
    "def run_scorer(args):\n",
    "    tasks = (\"nerc_coarse\", \"nerc_fine\", \"nel\")\n",
    "    if args[\"--task\"] not in tasks:\n",
    "        msg = \"Please restrict to one of the available evaluation tasks: \" + \", \".join(\n",
    "            tasks\n",
    "        )\n",
    "        logging.error(msg)\n",
    "        sys.exit(1)\n",
    "    logging.debug(f\"ARGUMENTS {args}\")\n",
    "    clef_evaluation.main(args= args | ARGS_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_value(entity, label):\n",
    "    if label not in entity:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return entity[label][list(entity[label].keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: no token at character index 23\n",
      "warning: no token at character index 195\n",
      "warning: no token at character index 32\n",
      "warning: no token at character index 39\n",
      "warning: no token at character index 52\n",
      "warning: no token at character index 17\n",
      "warning: no token at character index 79\n",
      "warning: no token at character index 24\n",
      "warning: no token at character index 31\n",
      "warning: no token at character index 19\n",
      "warning: no token at character index 244\n",
      "warning: no token at character index 214\n",
      "warning: no token at character index 236\n",
      "warning: no token at character index 40\n",
      "warning: no token at character index 60\n",
      "warning: no token at character index 101\n",
      "warning: no token at character index 128\n",
      "warning: no token at character index 107\n",
      "warning: no token at character index 89\n",
      "warning: no token at character index 103\n",
      "warning: no token at character index 13\n",
      "warning: no token at character index 29\n",
      "warning: no token at character index 32\n",
      "warning: no token at character index 7\n",
      "warning: no token at character index 25\n",
      "warning: no token at character index 14\n",
      "warning: no token at character index 154\n",
      "warning: no token at character index 11\n",
      "warning: no token at character index 11\n",
      "warning: no token at character index 17\n",
      "warning: no token at character index 19\n",
      "warning: no token at character index 27\n",
      "warning: no token at character index 36\n",
      "warning: no token at character index 51\n",
      "warning: no token at character index 59\n",
      "warning: no token at character index 10\n",
      "warning: no token at character index 5\n",
      "warning: no token at character index 14\n",
      "warning: no token at character index 25\n",
      "warning: no token at character index 10\n",
      "warning: no token at character index 23\n",
      "warning: no token at character index 36\n",
      "warning: no token at character index 50\n",
      "warning: no token at character index 110\n",
      "warning: no token at character index 44\n"
     ]
    }
   ],
   "source": [
    "PRED_FILE_TASKS = \"../tasks/output_linking_34e26bfd19c837e400a5fcb214cd1e7a25304a12.json\"\n",
    "\n",
    "with open(os.path.join(BASE_DIR, PRED_FILE_TASKS), \"r\") as infile:\n",
    "    pred_data = json.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "all_tokens = []\n",
    "for text in pred_data:\n",
    "    tokens = {token[\"start\"]: {\"text\": token[\"text\"], \"label\": \"O\"}\n",
    "              for token in text[\"tokens\"]}\n",
    "    next_id = -1\n",
    "    for char_id in reversed(tokens.keys()):\n",
    "        tokens[char_id][\"next_id\"] = next_id\n",
    "        next_id = char_id\n",
    "    for entity in text[\"entities\"]:\n",
    "        try:\n",
    "            wikidata_id = get_label_value(entity, \"wikidata_id\")\n",
    "            link = get_label_value(entity, \"link\")\n",
    "            tokens[entity[\"start_char\"]][\"label\"] = \"B-\" + entity[\"label\"]\n",
    "            tokens[entity[\"start_char\"]][\"wikidata_id\"] = wikidata_id\n",
    "            tokens[entity[\"start_char\"]][\"link\"] = link \n",
    "            next_id = tokens[entity[\"start_char\"]][\"next_id\"]\n",
    "            while next_id >= 0 and next_id < entity[\"end_char\"]:\n",
    "                tokens[next_id][\"label\"] = \"I-\" + entity[\"label\"]\n",
    "                tokens[next_id][\"wikidata_id\"] = wikidata_id\n",
    "                tokens[next_id][\"link\"] = link\n",
    "                next_id = tokens[next_id][\"next_id\"]\n",
    "        except Exception as e:\n",
    "            print(f\"warning: no token at character index {e}\")\n",
    "    all_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': 'Cult', 'label': 'O', 'next_id': 5},\n",
       " 5: {'text': 'statue', 'label': 'O', 'next_id': 12},\n",
       " 12: {'text': 'of', 'label': 'O', 'next_id': 15},\n",
       " 15: {'text': 'Amenhotep',\n",
       "  'label': 'B-PERSON',\n",
       "  'next_id': 25,\n",
       "  'wikidata_id': 'Q158052',\n",
       "  'link': '1'},\n",
       " 25: {'text': 'I.',\n",
       "  'label': 'I-PERSON',\n",
       "  'next_id': 28,\n",
       "  'wikidata_id': 'Q158052',\n",
       "  'link': '1'},\n",
       " 28: {'text': 'Limestone', 'label': 'O', 'next_id': 37},\n",
       " 37: {'text': '.', 'label': 'O', 'next_id': 39},\n",
       " 39: {'text': 'New', 'label': 'O', 'next_id': 43},\n",
       " 43: {'text': 'Kingdom', 'label': 'O', 'next_id': 50},\n",
       " 50: {'text': ',', 'label': 'O', 'next_id': 52},\n",
       " 52: {'text': '19th', 'label': 'O', 'next_id': 57},\n",
       " 57: {'text': 'Dynasty', 'label': 'O', 'next_id': 65},\n",
       " 65: {'text': '(', 'label': 'O', 'next_id': 66},\n",
       " 66: {'text': '1292–1190', 'label': 'O', 'next_id': 76},\n",
       " 76: {'text': 'BC', 'label': 'O', 'next_id': 78},\n",
       " 78: {'text': ')', 'label': 'O', 'next_id': 79},\n",
       " 79: {'text': '.', 'label': 'O', 'next_id': 81},\n",
       " 81: {'text': 'Deir',\n",
       "  'label': 'B-PLACE',\n",
       "  'next_id': 86,\n",
       "  'wikidata_id': 'Q750212',\n",
       "  'link': '7'},\n",
       " 86: {'text': 'el',\n",
       "  'label': 'I-PLACE',\n",
       "  'next_id': 88,\n",
       "  'wikidata_id': 'Q750212',\n",
       "  'link': '7'},\n",
       " 88: {'text': '-',\n",
       "  'label': 'I-PLACE',\n",
       "  'next_id': 89,\n",
       "  'wikidata_id': 'Q750212',\n",
       "  'link': '7'},\n",
       " 89: {'text': 'Medina',\n",
       "  'label': 'I-PLACE',\n",
       "  'next_id': 95,\n",
       "  'wikidata_id': 'Q750212',\n",
       "  'link': '7'},\n",
       " 95: {'text': '..', 'label': 'O', 'next_id': 98},\n",
       " 98: {'text': 'Drovetti', 'label': 'O', 'next_id': 107},\n",
       " 107: {'text': 'collection', 'label': 'O', 'next_id': 118},\n",
       " 118: {'text': '(', 'label': 'O', 'next_id': 119},\n",
       " 119: {'text': '1824', 'label': 'O', 'next_id': 123},\n",
       " 123: {'text': ')', 'label': 'O', 'next_id': 124},\n",
       " 124: {'text': '.', 'label': 'O', 'next_id': 126},\n",
       " 126: {'text': 'C.', 'label': 'O', 'next_id': 129},\n",
       " 129: {'text': '1372', 'label': 'O', 'next_id': -1}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No tags in the column '['NE-COARSE-METO']' of the system response file: 'example.tsv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true: 1 pred: 1\n",
      "data_format_true [[5]]\n",
      "data_format_pred [[5]]\n",
      "2025-10-16 17:20:14,799 - WARNING - example.tsv - No tags in the column '['NE-COARSE-METO']' of the system response file: 'example.tsv'\n",
      "2025-10-16 17:20:14,799 - WARNING - example.tsv - No tags in the column '['NE-COARSE-METO']' of the system response file: 'example.tsv'\n",
      "2025-10-16 17:20:14,799 - WARNING - example.tsv - No tags in the column '['NE-COARSE-METO']' of the system response file: 'example.tsv'\n"
     ]
    }
   ],
   "source": [
    "PRED_FILE = \"example.tsv\"\n",
    "GOLD_FILE = \"example.tsv\"\n",
    "\n",
    "os.chdir(os.path.join(BASE_DIR, \"HIPE-scorer\"))\n",
    "run_scorer(args={\"--ref\": GOLD_FILE,\n",
    "                 \"--pred\": PRED_FILE,\n",
    "                 \"--task\": \"nerc_coarse\",\n",
    "                 \"--outdir\": \"RESULT_FOLDER\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXj6rEaox9DE"
   },
   "source": [
    "### Preparation for manual assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jUrkTLcuawP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Function for visualising the entities with link\n",
    "\n",
    "def highlight_entities(\n",
    "    data,\n",
    "    iob_column = \"NE-COARSE-LIT\",\n",
    "    base_url=\"https://www.wikidata.org/wiki/\"\n",
    "    ):\n",
    "    # 1) Rebuild the text with spacing rules\n",
    "    text_parts = []\n",
    "    for idx, row in data.iterrows():\n",
    "        tok = row[\"TOKEN\"]\n",
    "        if \"NoSpaceAfter\" in row[\"MISC\"]:\n",
    "            text_parts.append(tok)\n",
    "        else:\n",
    "            text_parts.append(tok + \" \")\n",
    "    text = \"\".join(text_parts)\n",
    "\n",
    "    # 2) Merge contiguous IOB entities of the same type\n",
    "    entities = []\n",
    "    current = None  # {\"start_char\": int, \"end_char\": int, \"label\": str}\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        tag = row[iob_column]\n",
    "        if tag == \"_\" or tag == \"O\":\n",
    "            # close any open entity\n",
    "            if current is not None:\n",
    "                entities.append(current)\n",
    "                current = None\n",
    "            continue\n",
    "\n",
    "        # Extract type and prefix\n",
    "        if tag.startswith(\"B-\"):\n",
    "            etype = tag[2:]\n",
    "            eid = row[\"NEL-LIT\"]\n",
    "            # close previous if open\n",
    "            if current is not None:\n",
    "                entities.append(current)\n",
    "            # start new\n",
    "            current = {\n",
    "                \"start_char\": int(row[\"start_char\"]),\n",
    "                \"end_char\": int(row[\"end_char\"]),\n",
    "                \"label\": etype,\n",
    "                \"id\": eid\n",
    "            }\n",
    "\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            etype = tag[2:]\n",
    "            eid = row[\"NEL-LIT\"]\n",
    "            if current is not None and current[\"label\"] == etype:\n",
    "                # extend current run\n",
    "                current[\"end_char\"] = int(row[\"end_char\"])\n",
    "            else:\n",
    "                # stray I- (no open run or different type) → treat as B-\n",
    "                if current is not None:\n",
    "                    entities.append(current)\n",
    "                current = {\n",
    "                    \"start_char\": int(row[\"start_char\"]),\n",
    "                    \"end_char\": int(row[\"end_char\"]),\n",
    "                    \"label\": etype,\n",
    "                    \"id\": eid\n",
    "                }\n",
    "\n",
    "        else:\n",
    "            # Unknown tag → close any open entity\n",
    "            if current is not None:\n",
    "                entities.append(current)\n",
    "                current = None\n",
    "\n",
    "    # flush any remaining entity\n",
    "    if current is not None:\n",
    "        entities.append(current)\n",
    "\n",
    "    # 3) Render with spans (note: end_char is inclusive → slice to en+1)\n",
    "    entities.sort(key=lambda e: e[\"start_char\"])\n",
    "    result = \"\"\n",
    "    last_idx = 0\n",
    "\n",
    "    for e in entities:\n",
    "        s, en = int(e[\"start_char\"]), int(e[\"end_char\"])\n",
    "        etext = text[s:en + 1]  # inclusive end\n",
    "        etype = e.get(\"label\", \"Other\")\n",
    "        eid = e.get(\"id\", \"\")\n",
    "        color = label_to_color.get(etype, \"#dddddd\")\n",
    "\n",
    "        # decide whether to show eid as link or not\n",
    "        if eid !=\"_\" and eid !=\"NIL\":\n",
    "          eid_html = f'<a href=\"{base_url}{eid}\">{eid}</a>'\n",
    "        else:\n",
    "          eid_html = \"\"  # if entity linking was not successful no link is shown\n",
    "\n",
    "        result += text[last_idx:s]\n",
    "        result += (\n",
    "            f'<span style=\"background-color:{color}; color:black; padding:3px 6px; '\n",
    "            f'border-radius:16px; margin:0 2px; display:inline-block; '\n",
    "            f'box-shadow: 1px 1px 3px rgba(0,0,0,0.1);\">'\n",
    "            f'{etext}'\n",
    "            f'<span style=\"font-size:0.75em; font-weight:normal; margin-left:6px; '\n",
    "            f'background-color:rgba(0,0,0,0.05); padding:1px 6px; border-radius:12px;\">'\n",
    "            f'{etype} {eid_html}</span></span>'\n",
    "        )\n",
    "        last_idx = en + 1  # continue after inclusive end\n",
    "\n",
    "    result += text[last_idx:]\n",
    "    return result,text,entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNlHNhvOzIRv"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3We7GDtnzMpc"
   },
   "outputs": [],
   "source": [
    "# we need a function to convert the British Museum groundtruth data into the format expected by the HIPE scorer.\n",
    "# The data is found here: http://145.38.185.232/enriching/bm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZYqwx_j0gNv"
   },
   "outputs": [],
   "source": [
    "def get_demo_data():\n",
    "  import pandas as pd\n",
    "  # Loading the data\n",
    "  data = pd.read_csv(\n",
    "      'https://raw.githubusercontent.com/wjbmattingly/llm-lod-recipes/refs/heads/main/output/sample.tsv',\n",
    "      sep='\\t'\n",
    "  )\n",
    "\n",
    "  # Adding the start and end characters per token to the dataframe\n",
    "  data['start_char'] = 0\n",
    "  data['end_char'] = 0\n",
    "  current_char = 0\n",
    "\n",
    "  for index, row in data.iterrows():\n",
    "      data.loc[index, 'start_char'] = current_char\n",
    "      token = row['TOKEN']\n",
    "      # Check if the next token should not have a space before it\n",
    "      if index + 1 < len(data) and 'NoSpaceAfter' in data.loc[index, 'MISC']:\n",
    "          current_char += len(token)\n",
    "      else:\n",
    "          current_char += len(token) + 1  # Add 1 for the space after the token\n",
    "\n",
    "      data.loc[index, 'end_char'] = current_char - 1 # Subtract 1 because end_char is inclusive\n",
    "\n",
    "  # Just for testing purposes: adds a Wikidata ID to one entity\n",
    "  data.loc[0, 'NEL-LIT'] = 'Q1744'\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXdVkbdB1MyL"
   },
   "outputs": [],
   "source": [
    "df = get_demo_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1757600777541,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "XkjQCP9D3pla"
   },
   "outputs": [],
   "source": [
    "# preparing data for groundtruth evaluation\n",
    "import regex as re\n",
    "import csv\n",
    "\n",
    "def clean_format(input_file,output_file):\n",
    "  file = open(input_file)\n",
    "  output = open(output_file,mode='w')\n",
    "  reader = file.readlines() #(file,delimiter=\"\\t\")\n",
    "  #writer = csv.writer(output,delimiter=\"\\t\")\n",
    "  output.write('\\t'.join([\"TOKEN\",\"NE-COARSE-LIT\",\"NE-COARSE-METO\",\"NE-FINE-LIT\",\"NE-FINE-METO\",\"NE-FINE-COMP\",\"NE-NESTED\",\"NEL-LIT\",\"NEL-METO\",\"MISC\\n\"]))\n",
    "  i = 1\n",
    "  for line in reader:\n",
    "    line = line.strip()\n",
    "    mod_line = re.sub('_','-',line)\n",
    "    if re.search('-DOCSTART- -DOCSTART- -DOCSTART-',mod_line):\n",
    "      mod_line = re.sub('-DOCSTART- -DOCSTART- -DOCSTART-',f'# document_{i}',mod_line)\n",
    "      i+=1\n",
    "    mod_line = mod_line.split(' ')\n",
    "    try:\n",
    "      if len(mod_line)==2:\n",
    "        output.write(\"\\n\"+\" \".join(mod_line)+\"\\n\")\n",
    "      else:\n",
    "        output.write('\\t'.join([mod_line[0],mod_line[1],'-','-','-','-','-',mod_line[2],'-','-\\n',]))\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1757600829932,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "n88gpP0NEXbC"
   },
   "outputs": [],
   "source": [
    "clean_format('./data/bm_labels.txt','./data/gold/sample.tsv')\n",
    "clean_format('./data/bm-2-ner-format.txt','./data/predictions/sample.tsv')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kw9pN2CwP4r"
   },
   "source": [
    "## NER evaluation with groundtruth data by using the HIPE scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1757596009513,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "LZUcJ2GwxJV8",
    "outputId": "b8a0bd41-4023-4653-a365-8e772e53e1a9"
   },
   "outputs": [],
   "source": [
    "! python clef_evaluation.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPsSJaqBxgC9"
   },
   "source": [
    "### Running the scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_D7MR5jvy9M"
   },
   "source": [
    " ‼️ In the cell below it is important to note the parameter `--task`. This parameter value needs to be adjusted depending on the task one wants to evaluate (i.e. NER or EL). When evaluating NER we use `--task nerc_coarse`, while for evaluating EL we use `--task nel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 911,
     "status": "ok",
     "timestamp": 1757600924627,
     "user": {
      "displayName": "Marco Antonio Stranisci",
      "userId": "04449254454382882224"
     },
     "user_tz": -120
    },
    "id": "2rBJAfJK95s7",
    "outputId": "08386331-8b5e-4d3b-8dbd-b7c2b9fb6f55"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import regex as re\n",
    "\n",
    "for doc in glob.glob('./data/predictions/*'):\n",
    "  gold = re.sub('predictions','gold',doc)\n",
    "  ! python clef_evaluation.py --ref \"{gold}\" --pred \"{doc}\" --task nerc_coarse --outdir ./data/evaluations/ --hipe_edition hipe-2020 --log=./data/evaluations/scorer.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t9DO4bGxkfb"
   },
   "source": [
    "Let's now look at the various bits of data produced by the scorer. They are in the folder specified in the `--outdir` parameter of the scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1757594115035,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "HIDU3Ybe0AQG",
    "outputId": "1a053c59-f380-4b21-cdd0-d59bb76fe375"
   },
   "outputs": [],
   "source": [
    "ls -la ./evaluation_data/evaluations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoP5jug3zZL7"
   },
   "source": [
    "Here is an overview of the files created by the scorer:\n",
    "- `scorer.log` – the log produced by the scorer\n",
    "- `01_sample_nerc_coarse.tsv` – a TSV file contaning the evaluation results for document `01_sample` and task `nerc_coarse`, at different levels of aggregation etc.\n",
    "- `01_sample_nerc_coarse.json` – a JSON file with a more granular breakdown of the evaluation, which can be useful for error analysis and to better understand systems' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1757594273845,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "2MQra_sox9wM",
    "outputId": "c4e32312-8d4a-4413-c60d-87f8c41f1391"
   },
   "outputs": [],
   "source": [
    "! cat ./evaluation_data/evaluations/scorer.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk6BsUcsyX_5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('./evaluation_data/evaluations/01_sample_nerc_coarse.tsv', sep='\\t')\n",
    "eval_df.drop(columns=['System'], inplace=True)\n",
    "eval_df.set_index('Evaluation', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nJV-PQZ1hTL"
   },
   "source": [
    "Let's print the **micro-averaged** precision, recall and F-score in a **strict** evaluation regime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1757595176524,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "fFl1avvF0dM-",
    "outputId": "18c3029e-9c35-43b1-a623-d5e61bc4c5a9"
   },
   "outputs": [],
   "source": [
    "eval_df.loc['NE-COARSE-LIT-micro-strict-TIME-ALL-LED-ALL'][['P', 'R', 'F1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZnXDpc81sl8"
   },
   "source": [
    "Let's print the **micro-averaged** precision, recall and F-score in a **fuzzy** evaluation regime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1757595138721,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "fBs739hiygL6",
    "outputId": "8056f19a-e18f-446f-a0b9-a97f96b199f0"
   },
   "outputs": [],
   "source": [
    "# Let's print the micro-averaged precision, recall and F-score in a *fuzzy* evaluation regime\n",
    "eval_df.loc['NE-COARSE-LIT-micro-fuzzy-TIME-ALL-LED-ALL'][['P', 'R', 'F1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzLqjECH1y9g"
   },
   "source": [
    "There is more data in the TSV file, as it can be seen when printing the whole content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1757595297145,
     "user": {
      "displayName": "Matteo Romanello",
      "userId": "09570579360933454703"
     },
     "user_tz": -120
    },
    "id": "gGkGzpA514JI",
    "outputId": "91d9c9a3-83e6-4d0b-ac15-149b334c788a"
   },
   "outputs": [],
   "source": [
    "! cat ./evaluation_data/evaluations/01_sample_nerc_coarse.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quWWEXdylgGn"
   },
   "source": [
    "## Manual assessment of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cSUXxR32rlg"
   },
   "source": [
    "### Displaying the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL_HOz9Alk7c"
   },
   "source": [
    "To allow for manual assessment of output quality, the following cells display the identified entities in form of color-coded annotations via HTML. These kinds of insights into the results can both complement quantitative statistics and work as another way to estimate output quality. The latter is especially important for the frequent cases where no gold (or silver or bronze) standard is available that the NER output can be evaluated on. (Also, note that there are also other tools or modules out there that provide similar visualisations, e.g. [displaCy](https://spacy.io/usage/visualizers#ent) when using spaCy for NER.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1757514964781,
     "user": {
      "displayName": "Nina Rastinger",
      "userId": "04623370430015730380"
     },
     "user_tz": -120
    },
    "id": "Tn3rTmWD1Wia",
    "outputId": "a6e18557-8b7d-41ac-9823-4a8b51407286"
   },
   "outputs": [],
   "source": [
    "data = get_demo_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1757514964787,
     "user": {
      "displayName": "Nina Rastinger",
      "userId": "04623370430015730380"
     },
     "user_tz": -120
    },
    "id": "1GNxQ16E9rc5",
    "outputId": "19f56c06-ff88-4c04-c773-e4f55e6b44cf"
   },
   "outputs": [],
   "source": [
    "# Highlighting the identified entities in form of color-coded annotations with links to authority files where available\n",
    "\n",
    "# Color palette - add more colors if more labels are used or change them here\n",
    "colors = ['#F0D9EF', '#FCDCE1', '#FFE6BB', '#E9ECCE', '#CDE9DC', '#C4DFE5', '#D9E5F0', '#F0E6D9', '#E0D9F0', '#E6FFF0', '#9CC5DF']\n",
    "\n",
    "# Name Labels that should be shown in color, not mentioned labels will be shown in grey (this makes it easier to focus on certain categories if needed)\n",
    "labels = [\"PERSON\", \"DATE\"]\n",
    "\n",
    "# Mapping each label from the label set to a color from the palette\n",
    "label_to_color = {label: colors[i % len(colors)] for i, label in enumerate(labels)}\n",
    "\n",
    "# Generating the HTML - two changes can be made here:\n",
    "# 1) by default, the column \"NE-COARSE-LIT\" is used for the entities, this can be changed via the argument \"iob_column\"\n",
    "# 2) the entity identifiers are taken from the column \"NEL-LIT\"; by default, these are expected to be Wikidata identifiers (e.g. Q1744) and are combined with the Wikidate base URL; for another authority file, the base URL can be changed via the argument \"base_url\"\n",
    "res,text,entities = highlight_entities(data)\n",
    "\n",
    "# displaying the annotations\n",
    "display(HTML(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIBej6Wf2R6P"
   },
   "source": [
    "### Giving feedback on the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUZoSndbyQkB"
   },
   "source": [
    "The following cell gives a very simple example for how manual assessment of entities could be integrated into the data. Here, the user is asked for feedback on each identified entity which then shows up in designated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "id": "XbQAPhmq2Rmt",
    "outputId": "38221ef7-e629-4010-8fc5-9b22d8b4d3b8"
   },
   "outputs": [],
   "source": [
    "# Create a new column for manual assessment\n",
    "data['manual_assessment'] = ''\n",
    "\n",
    "# Display results again for better overview (no scrolling back and front)\n",
    "display(HTML(res))\n",
    "\n",
    "# Iterate through the identified entities\n",
    "for e in entities:\n",
    "    s, en = int(e[\"start_char\"]), int(e[\"end_char\"])\n",
    "    etext = text[s:en + 1]\n",
    "    etype = e.get(\"label\", \"Other\")\n",
    "\n",
    "    # Ask for feedback on the entity\n",
    "    feedback = input(f'Is the entity \"{etext.strip()}\" with label \"{etype}\" correct? (y/n/feedback): ')\n",
    "\n",
    "    # Store the feedback for all tokens within the entity span\n",
    "    for index, row in data.iterrows():\n",
    "        token_start = int(row[\"start_char\"])\n",
    "        token_end = int(row[\"end_char\"])\n",
    "        if max(s, token_start) <= min(en, token_end):\n",
    "            data.loc[index, 'manual_assessment'] = feedback\n",
    "\n",
    "# Show the altered data (now with user assessments)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1svSZ93EJlfz"
   },
   "source": [
    "## EL evaluation with groundtruth data\n",
    "\n",
    "TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-riY-m5r7xv_"
   },
   "source": [
    "## Variations and alternatives\n",
    "\n",
    "Another approach if the user does not have groundtruth data could be to use an **LLM-judge** approach to evaluate the NER output in absence of labelled golden data. The task of the LLM is then to “review” the NER output and to assess its quality."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sXj6rEaox9DE",
    "-cSUXxR32rlg",
    "hIBej6Wf2R6P"
   ],
   "provenance": [
    {
     "file_id": "1haoEa5gwTMuohrCRqLYtiJfuAjBxUPDY",
     "timestamp": 1757498886116
    },
    {
     "file_id": "1WXzG2eRZDc0Cxi212s-6guzd-zr1qIvm",
     "timestamp": 1757421007606
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
