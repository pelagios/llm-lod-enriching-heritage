{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pelagios/llm-lod-enriching-heritage/blob/main/notebooks/tasks/demo_qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hLkfqo0gRKP"
   },
   "source": [
    "# 1. Prepare Cultural Heritage Data for Named Entity Analysis\n",
    "\n",
    "This demo notebook is a combination of the four notebooks on data preparation, named entity recognition (NER), entity disambiguation and entity linking using Qwen. \n",
    "\n",
    "### Rationale\n",
    "\n",
    "This recipe takes some source content and preprocesses it for use across the rest of the cookbook.\n",
    "\n",
    "You can either:\n",
    "\n",
    "* upload your own sample text (e.g. transcribed text from a digitised item), or\n",
    "* run queries to get sample content from the Cleveland Museum of Art API from a keyword (e.g. \"Manet\") or record ID search\n",
    "\n",
    "The notebook shows the necessary steps to process the results and format them into a JSON file that can then be fed into a NER process.\n",
    "\n",
    "### Overview of the process\n",
    "\n",
    "Most parts of the recipe simply need to be run in a Notebook environment like Colab, Binder, Jupyter Notebooks.\n",
    "\n",
    "1. Initialise the notebook: run the first 'cells' to import the required libraries\n",
    "2. Fetch input: this is the part where you can provide some specific input, in csv format\n",
    "3. Preprocess text: clean text, detect language and split the text in tokens and sentences\n",
    "4. Save the results: combine results into a structured json record: text (original and cleaned), language, sentences, tokens and meta information (counts and ids)\n",
    "\n",
    "The fifth chapter provides alternatives for handling texts from other sources than websites:\n",
    "\n",
    "5. Alternatives for reading text data: text defined in code, text from file, text from pdf file and text from a website\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook depends on four files:\n",
    "\n",
    "* utils.py: helper functions\n",
    "* input_em.csv: default example input file (table)\n",
    "* input_wanderer.pdf: example pdf input file\n",
    "* input_wikipedia.txt: example text input file\n",
    "\n",
    "Please make sure they are available in this folder so that the notebook can run smoothly. You can download them from [Github](https://github.com/pelagios/llm-lod-enriching-heritage/tree/main/notebooks/tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g_sWNZ7HLN2"
   },
   "source": [
    "## 1.1. Install required software libraries\n",
    "\n",
    "Preprocessing data requires importing some standard software libraries. This step may take some time when run for the first time but in successive runs it will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with checking if the notebook is running on Google Colab. If that is the case, we need to connect to the notebook's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_notebook_environment_on_colab():\n",
    "    \"\"\"Test if run on Colab, if so test in environment is available, if not install it\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        try:\n",
    "            os.chdir(\"/content/llm-lod-enriching-heritage/notebooks/tasks\")\n",
    "            print(\"Found notebook environment\")\n",
    "        except:\n",
    "            print(\"notebook environment not found, installing...\")\n",
    "            !git clone https://github.com/pelagios/llm-lod-enriching-heritage.git\n",
    "            os.chdir(\"/content/llm-lod-enriching-heritage/notebooks/tasks\")\n",
    "    except:\n",
    "        print(\"Not running on Google Colab\")\n",
    "\n",
    "check_notebook_environment_on_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import standard libraries which should always be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_4ibh6-tsb-n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ dotenv not found. Installing...\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [dotenv]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dotenv-0.9.9 python-dotenv-1.2.1\n",
      "Finished installing dotenv\n",
      "üì¶ langid not found. Installing...\n",
      "Collecting langid\n",
      "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from langid) (2.2.6)\n",
      "Building wheels for collected packages: langid\n",
      "  Building wheel for langid (pyproject.toml): started\n",
      "  Building wheel for langid (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941216 sha256=d36e0195f0af04c97994de57049aeaf0415584591c679576f84562d94c5b0e0f\n",
      "  Stored in directory: /Users/marco/Library/Caches/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
      "Successfully built langid\n",
      "Installing collected packages: langid\n",
      "Successfully installed langid-1.1.6\n",
      "Finished installing langid\n",
      "üì¶ openai not found. Installing...\n",
      "Collecting openai\n",
      "  Downloading openai-2.8.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from openai) (2.12.4)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Downloading openai-2.8.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.12.0-cp310-cp310-macosx_11_0_arm64.whl (319 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, jiter, h11, distro, httpcore, anyio, httpx, openai\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8/8\u001b[0m [openai]2m7/8\u001b[0m [openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed anyio-4.11.0 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.12.0 openai-2.8.0 sniffio-1.3.1\n",
      "Finished installing openai\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import hashlib\n",
    "import importlib\n",
    "from IPython.display import clear_output, HTML\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import unicodedata\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import packages which may require installation on this device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ pypdf not found. Installing...\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.2.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /opt/homebrew/anaconda3/envs/enriching/lib/python3.10/site-packages (from pypdf) (4.15.0)\n",
      "Downloading pypdf-6.2.0-py3-none-any.whl (326 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-6.2.0\n",
      "Finished installing pypdf\n"
     ]
    }
   ],
   "source": [
    "spacy = utils.safe_import(\"spacy\")\n",
    "langid = utils.safe_import(\"langid\")\n",
    "regex = utils.safe_import(\"regex\")\n",
    "pl = utils.safe_import(\"polars\")\n",
    "pypdf = utils.safe_import(\"pypdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we set setting required for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_colab = utils.check_google_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Reading texts from a csv file\n",
    "\n",
    "We use the artefact descriptions from the Egyptian Museum in Turin as example texts. We use two columns of the file, one with the identifier of the artefact and one with the description text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function for reading the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'museum': {'name': 'egizio', 'file_name': 'input_em.csv', 'data_source': 'EMT', 'id_column_name': 'Inventory Number', 'text_column_name': 'Description'}, 'global': {'architecture': 'mac', 'model_name': 'qwen3:8b', 'max_processed': 6}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../config.yml') as yaml_file:\n",
    "    data = yaml.safe_load(yaml_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = data['museum']['file_name'] # to comment\n",
    "data_source = data['museum']['data_source']\n",
    "id_column_name = data['museum']['id_column_name']\n",
    "text_column_name = data['museum']['text_column_name']\n",
    "\n",
    "def read_emt_data(file_name):\n",
    "    \"\"\"Read texts from the Egyptian Museum in Turin from a csv file\"\"\"\n",
    "    try:\n",
    "        table_pl = pl.read_csv(file_name)[id_column_name, text_column_name]\n",
    "        table_pl.write_csv(\"tmp.csv\")\n",
    "        return [{\"id\": row[0], \"data_source\": data_source, \"text_original\": row[1]}\n",
    "                for row in table_pl.iter_rows()]\n",
    "    except:\n",
    "        print(f\"{utils.CHAR_FAILURE} Cannot read data from file {file_name}!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the function and store the variable in the variable `texts`. We show the first text to check if the process was successful. Note that the text includes (`id`) the text identifier as metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 'C. 0115', 'data_source': 'EMT', 'text_original': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts = read_emt_data(file_name)\n",
    "if len(texts) <= 0:\n",
    "    print(f\"{utils.CHAR_FAILURE} No texts found in file {file_name}!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkcitK7zHcuK"
   },
   "source": [
    "## 1.3. Preprocess text for named entity analysis\n",
    "\n",
    "Three steps are performed while preprocessing the texts:\n",
    "\n",
    "1. Text cleanup: remove non-text characters, urls and email addresses\n",
    "2. Detect the language of the text\n",
    "3. Split the text in sentences and tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining five functions for performing the preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_Yzx1zaEv9Dp"
   },
   "outputs": [],
   "source": [
    "def cleanup_text(text: str) -> str:\n",
    "    \"\"\"Cleanup text: remove non-text characters, urls and email addresses\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = regex.sub(r\"[\\u0000-\\u0008\\u000B\\u000C\\u000E-\\u001F\\u007F]\", \"\", text)\n",
    "    text = regex.sub(r\"[ \\t\\u00A0]+\", \" \", text)\n",
    "    text = regex.sub(r\"\\s+\", \" \", text)\n",
    "    text = regex.sub(r\"https?://\\S+\", \"<URL>\", text)\n",
    "    text = regex.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"<EMAIL>\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language_id):\n",
    "    \"\"\"Preprocess a single text: divide it in sentences and tokens\"\"\"\n",
    "    try:\n",
    "        spacy_model = spacy.blank(language_id)\n",
    "    except:\n",
    "        print(f\"{utils.CHAR_FAILURE} Cannot load model for language {language_id}\")\n",
    "        spacy_model = spacy.blank(\"xx\")\n",
    "    spacy_model.add_pipe(\"sentencizer\")\n",
    "    preprocessed_text = spacy_model(text)\n",
    "    sentences = [{\"id\": sentence_id, \n",
    "                  \"start\": sentence.start_char, \n",
    "                  \"end\": sentence.end_char, \n",
    "                  \"text\": sentence.text} for sentence_id, sentence in enumerate(preprocessed_text.sents)]\n",
    "    tok2sent = {token.i: sentence_id for sentence_id, sentence in enumerate(preprocessed_text.sents) \n",
    "                                     for token in sentence}\n",
    "    tokens = [{\"id\": token.i,\n",
    "               \"text\": token.text,\n",
    "               \"start\": token.idx,\n",
    "               \"end\": token.idx + len(token.text),\n",
    "               \"ws\": token.whitespace_ != \"\",\n",
    "               \"is_punct\": token.is_punct,\n",
    "               \"sent_id\": tok2sent.get(token.i)} for token in preprocessed_text]\n",
    "    return sentences, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "e6xknyGKwQJz"
   },
   "outputs": [],
   "source": [
    "def preprocess_texts(texts) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Preprocess a list of texts and return the results as a list of dictionaries\"\"\"\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for index, text in enumerate(texts):\n",
    "        utils.squeal(f\"Processing text {index + 1}\")\n",
    "        sentences, tokens = preprocess_text(text[\"text_cleaned\"], text[\"language_id\"])\n",
    "        results.append({\"meta\": {**{key: text[key] for key in text if not regex.search(\"text\", key)},\n",
    "                                 \"char_count\": len(text),\n",
    "                                 \"token_count\": len(tokens),\n",
    "                                 \"sentence_count\": len(sentences)},\n",
    "                        \"text_original\": text[\"text_original\"],\n",
    "                        \"text_cleaned\": text[\"text_cleaned\"],\n",
    "                        \"sentences\": sentences,\n",
    "                        \"tokens\": tokens})\n",
    "    print(\"Finished processing\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_text(text, skipped_fields=[]):\n",
    "    \"\"\"Show example text\"\"\"\n",
    "    text_shown = {key: text[key] for key in text if key not in skipped_fields}\n",
    "    if \"tokens\" in skipped_fields:\n",
    "        text_shown = text_shown | {\"tokens\": text[\"tokens\"][:3] + \n",
    "                                             [\"...\"] if len(text[\"tokens\"]) >= 3 else []}\n",
    "    print(text_shown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the cleanup function to the texts and store the results in the variable `text_cleaned`. We show the first text to check if the process was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sw9jFDPTxflU",
    "outputId": "68fd96cd-31e4-4e76-ce96-075cf1646a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'C. 0115', 'data_source': 'EMT', 'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts_cleaned = [text | {\"text_cleaned\": cleanup_text(text[\"text_original\"])} for text in texts]\n",
    "show_example_text(texts_cleaned[0], skipped_fields=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we apply the language derivation function to the texts and store the results in the variable `text_with_language_ids`. Again, we show the first text to check if the process was successful. A parameter `language_id` should appear at the start of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U0q26bQxzKQ",
    "outputId": "f378c7d6-b146-49f2-9015-d31bc5c52d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language_id': 'en', 'id': 'C. 0115', 'data_source': 'EMT', 'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts_with_language_ids = [{\"language_id\": utils.detect_text_language(text[\"text_cleaned\"])} | text\n",
    "                            for text in texts_cleaned]\n",
    "show_example_text(texts_with_language_ids[0], skipped_fields=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply the preprocess function to the texts and store the results in the variable `text_preprocessed`. We show the first text to check if the process was successful. The texts have been divided in sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sklE05mCyemA",
    "outputId": "28d1072c-23b8-408c-9fd0-aa5aaa3846e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 100\n",
      "Finished processing\n",
      "{'meta': {'language_id': 'en', 'id': 'C. 0115', 'data_source': 'EMT', 'char_count': 5, 'token_count': 23, 'sentence_count': 4}, 'sentences': [{'id': 0, 'start': 0, 'end': 28, 'text': 'Statuette of the god Anubis.'}, {'id': 1, 'start': 29, 'end': 36, 'text': 'Bronze.'}, {'id': 2, 'start': 37, 'end': 85, 'text': 'Late Period (722-332 BC).. Acquired before 1882.'}, {'id': 3, 'start': 86, 'end': 92, 'text': 'C. 115'}], 'tokens': [{'id': 0, 'text': 'Statuette', 'start': 0, 'end': 9, 'ws': True, 'is_punct': False, 'sent_id': 0}, {'id': 1, 'text': 'of', 'start': 10, 'end': 12, 'ws': True, 'is_punct': False, 'sent_id': 0}, {'id': 2, 'text': 'the', 'start': 13, 'end': 16, 'ws': True, 'is_punct': False, 'sent_id': 0}, '...']}\n"
     ]
    }
   ],
   "source": [
    "texts_preprocessed = preprocess_texts(texts_with_language_ids)\n",
    "show_example_text(texts_preprocessed[0], skipped_fields=[\"text_original\", \"text_cleaned\", \"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "402d2026"
   },
   "source": [
    "# 2. Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task that involves identifying and classifying named entities (like people, places, organizations) within text. For example, in the sentence \"Shakespeare wrote Romeo and Juliet in London\", a NER system would identify \"Shakespeare\" as a person, \"Romeo and Juliet\" as a work of art, and \"London\" as a location. NER is crucial for extracting structured information from unstructured text, making it valuable for tasks like information retrieval, question answering, and metadata enrichment. In this notebook, we'll explore how to perform NER using both traditional NLP approaches and modern Large Language Models.\n",
    "\n",
    "### Rationale\n",
    "\n",
    "This notebook demonstrates how to use OpenAI's GPT models to perform Named Entity Recognition (NER) by converting input text into annotated markdown format. Rather than using traditional NLP libraries, we leverage a Large Language Model's natural language understanding capabilities to identify and classify named entities. The notebook takes plain text as input and outputs markdown where entities are annotated in the format [Entity](TYPE), such as [London](LOCATION). This approach showcases how LLMs can be used for structured information extraction tasks in cultural heritage metadata enrichment.\n",
    "\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "The process consists of the following steps:\n",
    "\n",
    "1. **Import required software libraries**: We start with importing required software libraries\n",
    "2. **Read the text that requires processing**: Next we obtain the input text from the preprocessing notebook\n",
    "3. **Named entity recognition**: The text is sent to GPT with a prompt that instructs it to identify entities. The LLM marks entities in markdown format: \\[entity text\\]\\(entity type\\)\n",
    "4. **Named entity visualization**: The annotated text is displayed with colour-coded entity highlighting\n",
    "5. **Save results**: Save the results of named entity recognition for future processing\n",
    "\n",
    "This approach leverages the LLM's natural language understanding while producing structured, machine-readable output.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook depends on three files:\n",
    "\n",
    "* utils.py: helper functions\n",
    "* output_data_preparation_11f98441067263d80ee1a6bac27babf0f2c6734b.json: output file of data preparation task\n",
    "* ner_cache.json: context-dependent cache of names found earlier\n",
    "\n",
    "Please make sure they are available in this folder so that the notebook can run smoothly. You can download them from [Github](https://github.com/pelagios/llm-lod-enriching-heritage/tree/main/notebooks/tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "168ccb96"
   },
   "source": [
    "## 2.1. Import required software libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MC0Dlg3HVWmL"
   },
   "source": [
    "This function is needed in different sections, we define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "f4c2434c-50a4-4c1f-82a2-610fad9d247d",
    "outputId": "f1642628-4170-4a8d-8a0c-3ae5b54db5c0"
   },
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"extracted from records of objects in the collection of\n",
    "             the Egyptian museum, Torino, the Museo Egizio ‚Äì Torino\"\"\"\n",
    "\n",
    "def make_prompt(texts_input, target_labels):\n",
    "    \"\"\"Create an LLM prompt, given a text and target labels and return it\"\"\"\n",
    "    return f\"\"\"\n",
    "Convert the following text {CONTEXT} into a structured markdown format,\n",
    "where you annotate the entities in the text in the following format:\n",
    "[Tom](PERSON) went to [New York](PLACE).\n",
    "\n",
    "Look for the following entities types:\n",
    "{target_labels}\n",
    "\n",
    "Do this for the following text:\n",
    "{texts_input}\n",
    "\n",
    "Only return the markdown output, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b422d93f-eb91-422e-a6c3-c6c56a16d6d0"
   },
   "source": [
    "## 2.2. Read the text that requires processing\n",
    "\n",
    "The text should have been preprocessed by the `data_preparation.ipynb` notebook. The file read here is an output file of that notebook. We select the first text and show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "f01e81fa-5a4a-4ed3-abfd-4008f645dac4",
    "outputId": "ac507042-48f1-4219-c771-3fdcf24e1259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115\n"
     ]
    }
   ],
   "source": [
    "input_data = copy.deepcopy(texts_preprocessed)\n",
    "texts_input = [text[\"text_cleaned\"] for text in input_data]\n",
    "print(texts_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cf4c708"
   },
   "source": [
    "## 2.3. Named entity recognition with Qwen\n",
    "\n",
    "Here we use [Qwen](https://en.wikipedia.org/wiki/Qwen), a locally-run model developed by the company Alibaba. The model has hardware requirements which your computer may not satisfy: it needs a GPU and about 12 gigabytes of memory. When running this model on Google Colab, it is recommended to use the runtime environment `T4 GPU`. If the model is too slow for you, the model `qwen3:0.6b` can be used as an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/marco/Documents/unito/llm-lod-enriching-heritage/notebooks/tasks/utils.py'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "99d29d7e-8a26-411f-ba5b-05cda6d63adc",
    "outputId": "04a3d64c-9db0-4f40-a649-793979989ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 6 with model qwen3:8b\n",
      "[GIN] 2025/11/14 - 11:20:45 | 200 | 42.656292875s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "Finished processing\n",
      "Ô∏è‚úÖ Saved data to file ner_cache_005121f7aa72cf9a7ed1f0b00db3836e9a948352.json\n",
      "Statue of the goddess Sekhmet. Granodiorite. New Kingdom, 18th Dynasty, reign of [Amenhotep III](PERSON) (1390-1353 BC). [Thebes](LOCATION).. [Drovetti](PERSON) collection (1824). C. 256\n"
     ]
    }
   ],
   "source": [
    "MODEL = data['global']['model_name']\n",
    "MAX_PROCESSED = data['global']['max_processed']\n",
    "\n",
    "texts_output = utils.ollama_run(MODEL, texts_input[:MAX_PROCESSED], make_prompt, in_colab)\n",
    "print(texts_output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76f086af"
   },
   "source": [
    "## 2.4. Named entity visualization\n",
    "\n",
    "Here we show the results of named entity recognition in a more readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c69070e-4b5f-40af-adb2-18266231db1c"
   },
   "source": [
    "First we define three helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ff163088-2219-42f6-9778-c72f786e698e",
    "outputId": "77611425-5c36-46d4-964f-f8f77223915f"
   },
   "outputs": [],
   "source": [
    "def extract_entities_from_markdown(texts_output):\n",
    "    \"\"\"Extract the locations and labels of the entities from the markdown and return these\"\"\"\n",
    "    pattern = r'\\[([^\\]]+)\\]\\(([^)]+)\\)'\n",
    "    text_prefix = \"\"\n",
    "    current_char = 0\n",
    "    entities = []\n",
    "    for match in regex.finditer(pattern, texts_output):\n",
    "        text_prefix += texts_output[current_char: match.start()]\n",
    "        entities.append({\"text\": match.group(1),\n",
    "                         \"label\": match.group(2),\n",
    "                         \"start_char\": len(text_prefix),\n",
    "                         \"end_char\": len(text_prefix) + len(match.group(1))})\n",
    "        text_prefix += match.group(1)\n",
    "        current_char = match.end()\n",
    "    text_prefix += texts_output[current_char:]\n",
    "    return entities, text_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ccdb03eb-b38d-4ee5-af7b-9676de01a335",
    "outputId": "0d1571fe-7afa-4bb0-eb60-56ac44b55e73"
   },
   "outputs": [],
   "source": [
    "def check_llm_output_text(text_llm_output, texts_input):\n",
    "    if text_llm_output != texts_input:\n",
    "        print(f\"{utils.CHAR_FAILURE} Output text of named entity recognition is different from input text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44639606-06d2-442e-b2ec-8fb31d71d921"
   },
   "source": [
    "Next we call the function to extract the entities from the results, check the output text and visualize the results, for the first five results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "5b7f5a6e-247d-4c2c-aeff-22023cee039f",
    "outputId": "bc505714-14a3-4a8e-e474-831d8ba4f3bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Statue of the goddess Sekhmet. Granodiorite. New Kingdom, 18th Dynasty, reign of <span style=\"border: 1px solid black; color: red;\">Amenhotep III</span> (1390-1353 BC). <span style=\"border: 1px solid black; color: green;\">Thebes</span>.. <span style=\"border: 1px solid black; color: red;\">Drovetti</span> collection (1824). C. 256"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Output text of named entity recognition is different from input text!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "**Statuette of the goddess Bastet. Bronze. Late Period (722-332 BC).. <span style=\"border: 1px solid black; color: red;\">Drovetti</span> collection (1824). C. 271**"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Amulet depicting the <span style=\"border: 1px solid black; color: red;\">god Harpocrates</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Output text of named entity recognition is different from input text!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "**Statuette of the goddess Satet. Bronze. Late Period (722-332 BC).** <span style=\"border: 1px solid black; color: red;\">Drovetti</span> collection (1824). C. 515"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entities_all = []\n",
    "for index, text in enumerate(texts_output):\n",
    "    entities, text_llm_output = extract_entities_from_markdown(text)\n",
    "    entities_all.append({\"entities\": entities, \"text_llm_output\": text_llm_output})\n",
    "    if index < 5:\n",
    "        check_llm_output_text(text_llm_output, texts_input[index])\n",
    "        display(HTML(utils.mark_entities_in_text(text_llm_output, entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35713f16-99bd-4392-a894-dd59b594bded"
   },
   "source": [
    "## 2.5. Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24d2f355-ceb0-423e-99ea-17489f289189"
   },
   "source": [
    "Here we add the named entity analysis to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "bf595c60-2d89-4baf-83a7-e5a00c2e25f8",
    "outputId": "e3c9ca8c-8dae-4487-df27-07c500d37a48"
   },
   "outputs": [],
   "source": [
    "for index, entities in enumerate(entities_all):\n",
    "    input_data[index][\"entities\"] = entities[\"entities\"]\n",
    "    input_data[index][\"text_llm_output\"] = entities[\"text_llm_output\"]\n",
    "for rest_index in range(index, len(input_data)):\n",
    "    input_data[rest_index][\"entities\"] = []\n",
    "    input_data[rest_index][\"text_llm_output\"] = []   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Disambiguation with Candidates\n",
    "\n",
    "This notebook disambiguates the entities found by the NER notebook by linking them to WikiData concepts\n",
    "\n",
    "### Rationale\n",
    "\n",
    "When using LLMs, we can leverage context to help determine correct identifiers for entities found. One of the largest challenges with LLMs is getting them to generate the correct identifier for specific entities. Without context, an LLM will confidently generate a believable looking identifier code. When checked, however, users will often find these codes do not exist or are entirely wrong.\n",
    "\n",
    "We solve this problem with context. LLMs can receive context in one of two ways: either we can give it the context or we can use an LLM agentically with tools so that it can retrieve the context for itself. Both have their advantages, but both work within the same principal: context allows the LLM to get the correct identifier code so that it does not need to hallucinate one. While hallucinations are still possible, the chances are reduced if we provide a list of options to an LLM to choose from.\n",
    "\n",
    "In this notebook, we will explore the first of these options, where we provide the LLM with a list of candidates that were generated in the previous data notebook. To make things easier, we have pasted the output from that notebook here.\n",
    "\n",
    "It is also worth noting that providing the LLM with the necessary context is often quite cheaper (assuming you are using a paid-model), than letting the model agentically query the web or use other tools. We will see this in the next notebook.\n",
    "\n",
    "### Processing overview\n",
    "\n",
    "The process consists of the following steps:\n",
    "\n",
    "1. **Import required software libraries**: We start with importing required software libraries\n",
    "2. **Read the text that requires processing**: Next we obtain the input text from the NER notebook\n",
    "3. **Candidate extraction**\n",
    "4. **Disambiguation**: The text is sent to an LLM with a prompt that instructs it to disambiguate entities based on the available candidates.\n",
    "5. **Disambiguation visualization**: The annotated text is displayed with colour-coded entity highlighting\n",
    "6. **Save results**: Save the results of the disambiguation process for future processing\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook depends on four files:\n",
    "\n",
    "* utils.py: helper functions\n",
    "* output_ner_5f12fa7c16d33ea378148197569f999f774f7481.json: output file of NER task\n",
    "* disambiguation_cache_wikidata.json: context-dependent cache of WikiData information found earlier\n",
    "* disambiguation_cache_llm.json: context-dependent cache of LLM choices make earlier\n",
    "\n",
    "Please make sure the files are available in this folder so that the notebook can run smoothly. You can download the files from Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Import required software libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two helper functions are needed in different sections, we define them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"extracted from records of objects in the collection of \n",
    "             the Egyptian museum, Torino, the Museo Egizio ‚Äì Torino\"\"\"\n",
    "\n",
    "def make_prompt(entity, text, candidates):\n",
    "    \"\"\"Create an LLM prompt, given a text and target labels and return it\"\"\"\n",
    "    return f\"\"\"\n",
    "Disambiguate the entity \"{entity}\" in the following text {CONTEXT}.\n",
    "\n",
    "{text}\n",
    "\n",
    "Here are the candidates, in json format:\n",
    "\n",
    "{candidates}\n",
    "\n",
    "Only return the relevant id, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wikidata_ids_to_texts_input(texts_input, entities):\n",
    "    \"\"\"Insert the retrieved wikidata ids into the variable text_inputs and return it\"\"\"\n",
    "    entities_per_text = {}\n",
    "    for entity in entities:\n",
    "        if entity[\"text_id\"] not in entities_per_text:\n",
    "            entities_per_text[entity[\"text_id\"]] = {}\n",
    "        entities_per_text[entity[\"text_id\"]][entity[\"entity_text\"]] = entity\n",
    "    for text_id, text in enumerate(texts_input):\n",
    "        for entity in text[\"entities\"]:\n",
    "            if text_id in entities_per_text and entity[\"text\"] in entities_per_text[text_id]:\n",
    "                entity[\"wikidata_id\"] = entities_per_text[text_id][entity[\"text\"]][\"wikidata_id\"]\n",
    "    return texts_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Read the texts that require processing\n",
    "\n",
    "The texts should have been processed by the ner.ipynb notebook. The file read here is an output file of that notebook. We read the texts and the associated metadata and show the first text with its entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115', 'entities': []}\n"
     ]
    }
   ],
   "source": [
    "texts_input = copy.deepcopy(input_data)\n",
    "print({\"text_cleaned\": texts_input[0][\"text_cleaned\"], \n",
    "       \"entities\": texts_input[0][\"entities\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Disambiguation candidate generation\n",
    "\n",
    "We need candidate ids for the entities to make the task for the LLM easier. We obtain these candidate ids by searching for the entities in wikidata.org. The search step returns seven candidate ids with a description text for each of them. In order not to overload the website, we use a cache for storing entities that were looked up earlier. We also wait for 10 seconds (variable `SLEEP_TIME_FETCH_PAGE`) between the searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define five helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_TIME_FETCH_PAGE = data['global']['sleep_time']\n",
    "USER_AGENT = data['global']['user_agent']\n",
    "\n",
    "def query_wikidata_for_single_entity(entity_text):\n",
    "    \"\"\"Read Wikidata data on entity_text and return it\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": USER_AGENT,\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'language': 'en',\n",
    "        'format': 'json',\n",
    "        'limit': '7',\n",
    "        'search': entity_text \n",
    "    }\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    time.sleep(SLEEP_TIME_FETCH_PAGE)\n",
    "    wikidata_data = requests.get(url, params=params, headers=headers)\n",
    "    return wikidata_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_ner_input(texts_input):\n",
    "    \"\"\"For each entity in the input text return the entity text, context text and context text id\"\"\" \n",
    "    return [{\"entity_text\": entity[\"text\"],\n",
    "             \"text_id\": index,\n",
    "             \"text\": text[\"text_cleaned\"]} \n",
    "            for index, text in enumerate(texts_input) for entity in text[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_WIKIDATA = data[\"utils\"][\"disambiguation_cache_wikidata\"]\n",
    "\n",
    "\n",
    "def find_wikidata_candidates_for_entities(entities):\n",
    "    \"\"\"Lookup candidate ids for the entities on wikidata.org and return them\"\"\"\n",
    "    wikidata_cache = utils.read_json_file(DISAMBUGUATION_CACHE_FILE_WIKIDATA)\n",
    "    for entity in entities:\n",
    "        if entity[\"entity_text\"] not in wikidata_cache.keys():\n",
    "            utils.squeal(f\"Looking up entity {entity['entity_text']} (text id {entity['text_id'] + 1}) on WikiData.org...\")\n",
    "            wikidata_data = query_wikidata_for_single_entity(entity[\"entity_text\"])\n",
    "            wikidata_cache[entity[\"entity_text\"]] = [{\"id\": candidate[\"id\"], \"description\": candidate[\"description\"]} \n",
    "                                                     for candidate in json.loads(wikidata_data.text)[\"search\"]\n",
    "                                                     if \"description\" in candidate.keys()]\n",
    "            utils.write_json_file(DISAMBUGUATION_CACHE_FILE_WIKIDATA, wikidata_cache)\n",
    "    print(\"Finished processing\")\n",
    "    utils.save_data_to_json_file(wikidata_cache, file_name=DISAMBUGUATION_CACHE_FILE_WIKIDATA, in_colab=in_colab)\n",
    "    for entity in entities:\n",
    "        entity[\"candidates\"] = wikidata_cache[entity[\"entity_text\"]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract the entities from the input data and call a helper function for finding the candidate ids. These will be stored in the `entities` variable. We show the first item of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up entity god Harpocrates (text id 4) on WikiData.org...\n",
      "Finished processing\n",
      "Ô∏è‚úÖ Saved data to file disambiguation_cache_wikidata_c04128363cdef2ae6a8a1a81c597e38285555a11.json\n",
      "{'entity_text': 'Amenhotep III', 'candidates': [{'id': 'Q42606', 'description': 'ninth Pharaoh of the Eighteenth dynasty of Egypt'}, {'id': 'Q55018696', 'description': 'operatic character in the opera Akhnaten by Philip Glass; father of Akhenaton'}, {'id': 'Q96185335', 'description': 'Facsimile, Anen (TT 120), Amenhotep III, Queen Tiye, kiosk by Nina de Garis Davies (MET, 33.8.8)'}, {'id': 'Q96185392', 'description': 'Facsimile, Anen, TT 226, Amenhotep III, Mutemwia by Nina de Garis Davies (MET, 15.5.1)'}, {'id': 'Q97778073', 'description': 'scholarly book'}, {'id': 'Q116247775', 'description': 'head, Amenhotep III, Blue Crown at the Metropolitan Museum of Art (MET, 56.138)'}]}\n"
     ]
    }
   ],
   "source": [
    "entities = extract_entities_from_ner_input(texts_input)\n",
    "entities = find_wikidata_candidates_for_entities(entities)\n",
    "print({key: entities[0][key] for key in ['entity_text', 'candidates']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Disambiguate entities with Qwen\n",
    "\n",
    "Here we use [Qwen](https://en.wikipedia.org/wiki/Qwen), a locally-run model developed by the company Alibaba. The model has hardware requirements which your computer may not satisfy: it needs a GPU and about 12 gigabytes of memory. When running this model on Google Colab, it is recommended to use the runtime environment `T4 GPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define two helper functions|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(response, candidates, model):\n",
    "    \"\"\"cleanup llm response and return associated candidate wikidata lemma\"\"\"\n",
    "    wikidata_id = regex.sub(r\"^.*(Q\\d+).*$\", r\"\\1\", response)\n",
    "    if not regex.search(r\"^Q\\d+$\", wikidata_id):\n",
    "        return {\"id\": \"\", \"description\": \"\", \"model\": model}\n",
    "    for candidate in candidates:\n",
    "        if candidate[\"id\"] == wikidata_id:\n",
    "            return candidate | {\"model\": model}\n",
    "    return {\"id\": \"\", \"description\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_LLM = data[\"utils\"][\"disambiguation_cache_llm\"]\n",
    "\n",
    "def ollama_wikidata_id_selection(model, entities):\n",
    "    llm_cache = utils.read_json_file(DISAMBUGUATION_CACHE_FILE_LLM)\n",
    "    for entity in entities:\n",
    "        if (entity[\"entity_text\"] in llm_cache and \n",
    "            entity[\"text\"] in llm_cache[entity[\"entity_text\"]] and\n",
    "            model in llm_cache[entity[\"entity_text\"]][entity[\"text\"]]):\n",
    "            utils.squeal(f\"Retrieving entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} from cache\")\n",
    "            entity[\"wikidata_id\"] = llm_cache[entity[\"entity_text\"]][entity[\"text\"]][model] | {\"model\": model}\n",
    "        else:\n",
    "            if \"ollama\" in sys.modules:\n",
    "                ollama = utils.importlib.import_module(\"ollama\")\n",
    "            else:\n",
    "                ollama = utils.import_ollama_module()\n",
    "            utils.install_ollama_model(model, ollama)\n",
    "            utils.squeal(f\"Sending entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} to {model}\")\n",
    "            prompt = make_prompt(entity[\"entity_text\"], entity[\"text\"], entity[\"candidates\"])\n",
    "            response = utils.process_text_with_ollama(model, prompt, ollama)\n",
    "            entity[\"wikidata_id\"] = process_llm_response(response, entity[\"candidates\"], model)\n",
    "            if entity[\"entity_text\"] not in llm_cache:\n",
    "                llm_cache[entity[\"entity_text\"]] = {}\n",
    "            if entity[\"text\"] not in llm_cache[entity[\"entity_text\"]]:\n",
    "                llm_cache[entity[\"entity_text\"]][entity[\"text\"]] = {}\n",
    "            llm_cache[entity[\"entity_text\"]][entity[\"text\"]][model] = {key: value \n",
    "                                                                       for key, value in entity[\"wikidata_id\"].items()}\n",
    "            utils.write_json_file(DISAMBUGUATION_CACHE_FILE_LLM, llm_cache)\n",
    "            time.sleep(2)\n",
    "    print(\"Finished processing\")\n",
    "    utils.save_data_to_json_file(llm_cache, file_name=DISAMBUGUATION_CACHE_FILE_LLM, in_colab=in_colab)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending entity \"Drovetti\" of text 5 to qwen3:8b\n",
      "[GIN] 2025/11/14 - 11:35:10 | 200 | 43.168037208s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "Finished processing\n",
      "Ô∏è‚úÖ Saved data to file disambiguation_cache_llm_82906bd5bcb60d6477641cc8b9126729125d4a6d.json\n",
      "{'entity_text': 'Amenhotep III', 'text': 'Statue of the goddess Sekhmet. Granodiorite. New Kingdom, 18th Dynasty, reign of Amenhotep III (1390-1353 BC). Thebes.. Drovetti collection (1824). C. 256', 'wikidata_id': {'id': 'Q42606', 'description': 'ninth Pharaoh of the Eighteenth dynasty of Egypt', 'model': 'qwen3:8b'}}\n"
     ]
    }
   ],
   "source": [
    "MODEL = data['global']['model_name']\n",
    "MAX_PROCESSED = data['global']['max_processed']\n",
    "MAX_PROCESSED_ENTITIES = len([entity for entity in entities if entity[\"text_id\"] < MAX_PROCESSED])\n",
    "\n",
    "processed_entities = ollama_wikidata_id_selection(MODEL, entities[:MAX_PROCESSED_ENTITIES])\n",
    "texts_output = add_wikidata_ids_to_texts_input(texts_input[:MAX_PROCESSED], processed_entities)\n",
    "print({key: value for key, value in processed_entities[0].items() if key in [\"entity_text\", \"text\", \"wikidata_id\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Disambiguation visualization\n",
    "\n",
    "Here we show the results of the disambiguation task in a more readable format. We use the helper function `mark_entities_in_text` for this. But you will not find the definition of this helper function here: it is defined in the file `utils.py` because it is used by other notebooks as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call the helper function and show the first three texts with their entities and the WikiData ids. We do not supply it with the cleaned text but with the text that was output of the named entity recognition (`text_llm_output`), because that is sometimes different from the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Statue of the goddess Sekhmet. Granodiorite. New Kingdom, 18th Dynasty, reign of <span style=\"border: 1px solid black; color: red;\">Amenhotep III</span><sup>Q42606</sup> (1390-1353 BC). <span style=\"border: 1px solid black; color: green;\">Thebes</span><sup>Q101583</sup>.. <span style=\"border: 1px solid black; color: red;\">Drovetti</span><sup>Q822895</sup> collection (1824). C. 256"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "**Statuette of the goddess Bastet. Bronze. Late Period (722-332 BC).. <span style=\"border: 1px solid black; color: red;\">Drovetti</span><sup>Q822895</sup> collection (1824). C. 271**"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for text_id, text in enumerate(texts_output):\n",
    "    if text_id < 3:\n",
    "        display(HTML(utils.mark_entities_in_text(text[\"text_llm_output\"], text[\"entities\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Entity linking\n",
    "\n",
    "This notebook links the entities found in the previous steps (NER, Disambiguation) to the artefacts\n",
    "\n",
    "### Rationale\n",
    "\n",
    "Making explicit the relations between the named entities helps us to better understand the data. In the context of museum artefact descriptions there are two major types of relations: relation between named entities among each other and relations between named entities and the artefacts. In the main data files used for testing the software, from the Egyptian Museum of Turin, the first group of relations is quite rare. Therefore we focus on finding relations/links between artefacts and the named entities in their description texts.\n",
    "\n",
    "We use large language models (LLMs) to derive the relations. To make the task manageable, we have restricted the relations to eleven types:\n",
    "\n",
    "1. the entity is a person depicted on or by the artefact\n",
    "2. the entity is a person that created the artefact\n",
    "3. the entity is a person that discovered the artefact\n",
    "4. the entity is a person that owned the artefact\n",
    "5. the entity is a person in power during the period of the creation of the artefact\n",
    "6. the entity is a location depicted on or by the artefact\n",
    "7. the entity is a location where the artefact was created\n",
    "8. the entity is a location where the artefact was produced\n",
    "9. the entity is a location where the artefact was discovered\n",
    "10. the entity is the artefact's current location\n",
    "11. other entity type or other relation between entity and artefact\n",
    "\n",
    "We include the definitions of these types in the prompt sent to the LLMs and ask them to select the best matching one. We offer each entity to the LLMs separately.\n",
    "\n",
    "### Processing overview\n",
    "\n",
    "The process consists of the following steps:\n",
    "\n",
    "1. Import required software libraries: We start with importing required software libraries\n",
    "2. Read the text that requires processing: Next we obtain the input text from the Disambiguation notebook\n",
    "3. Linking: The text is sent to GPT with a prompt that instructs it to select the best type for the link between the artefact and the entity\n",
    "4. Linking visualization: The link type is displayed in text with colour-coded entities\n",
    "5. Save results: Save the results of the linking process for future processing\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook depends on three files:\n",
    "\n",
    "* utils.py: helper functions\n",
    "* output_disambiguation_ba25101ddbe8830789bfdfdb3a5ba6312d6853e6.json: output file of disambiguation task\n",
    "* linking_cache.json: context-dependent cache of linking analysis performed earlier\n",
    "\n",
    "Please make sure they are available in this folder so that the notebook can run smoothly. You can download them from Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Import required software libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two helper functions are needed in different sections, we define them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"extracted from records of objects in the collection of \n",
    "             the Egyptian museum, Torino, the Museo Egizio ‚Äì Torino\"\"\"\n",
    "\n",
    "def make_linking_prompt(entity, text):\n",
    "    \"\"\"Create an LLM prompt, given a text and target labels and return it\"\"\"\n",
    "    return f\"\"\"\n",
    "Considering the following description of a museum artefact {CONTEXT}:\n",
    "\n",
    "{text}\n",
    "\n",
    "Retrieve the relationship between this artefact and the following named entity, \n",
    "mentioned in the description:\n",
    "\n",
    "{entity}\n",
    "\n",
    "Please answer the following question: Why is this entity mentioned in the description? \n",
    "Please select your answer from the following options:\n",
    "\n",
    "1. the entity is a person depicted on or by the artefact\n",
    "2. the entity is a person that created the artefact\n",
    "3. the entity is a person that discovered the artefact\n",
    "4. the entity is a person that owned the artefact\n",
    "5. the entity is a person in power during the period of the creation of the artefact\n",
    "6. the entity is a location depicted on or by the artefact\n",
    "7. the entity is a location where the artefact was created\n",
    "8. the entity is a location where the artefact was produced\n",
    "9. the entity is a location where the artefact was discovered\n",
    "10. the entity is the artefact's current location\n",
    "11. other entity type or other relation between entity and artefact\n",
    "\n",
    "Answer only with a number. If you choose for option 11, you  may add a clarification text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linking_data_to_texts_input(texts_input, entities):\n",
    "    \"\"\"Insert the retrieved linking data into the variable text_inputs and return it\"\"\"\n",
    "    entities_per_text = {}\n",
    "    for entity in entities:\n",
    "        if entity[\"text_id\"] not in entities_per_text:\n",
    "            entities_per_text[entity[\"text_id\"]] = {}\n",
    "        entities_per_text[entity[\"text_id\"]][entity[\"entity_text\"]] = entity\n",
    "    for text_id, text in enumerate(texts_input):\n",
    "        for entity in text[\"entities\"]:\n",
    "            if text_id in entities_per_text and entity[\"text\"] in entities_per_text[text_id]:\n",
    "                entity[\"link\"] = entities_per_text[text_id][entity[\"text\"]][\"link\"]\n",
    "    return texts_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Read the texts that require processing\n",
    "\n",
    "The texts should have been processed by the `ner.ipynb` notebook. The file read here is an output file of the `disambiguation-candidates.ipynb` notebook which in turn processed the `ner.ipynb` output. We read the texts and the associated metadata and show the first text with its entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_cleaned': 'Statue of the goddess Sekhmet. Granodiorite. New Kingdom, 18th Dynasty, reign of Amenhotep III (1390-1353 BC). Thebes.. Drovetti collection (1824). C. 256', 'entities': [{'text': 'Amenhotep III', 'label': 'PERSON', 'start_char': 81, 'end_char': 94, 'wikidata_id': {'id': 'Q42606', 'description': 'ninth Pharaoh of the Eighteenth dynasty of Egypt', 'model': 'qwen3:8b'}}, {'text': 'Thebes', 'label': 'LOCATION', 'start_char': 111, 'end_char': 117, 'wikidata_id': {'id': 'Q101583', 'description': 'ancient Egyptian city', 'model': 'qwen3:8b'}}, {'text': 'Drovetti', 'label': 'PERSON', 'start_char': 120, 'end_char': 128, 'wikidata_id': {'id': 'Q822895', 'description': 'Italian diplomat, explorer and scholar', 'model': 'qwen3:8b'}}]}\n"
     ]
    }
   ],
   "source": [
    "texts_input = copy.deepcopy(texts_output)\n",
    "print({\"text_cleaned\": texts_input[1][\"text_cleaned\"], \n",
    "       \"entities\": texts_input[1][\"entities\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Disambiguate entities with Qwen\n",
    "\n",
    "Here we use [Qwen](https://en.wikipedia.org/wiki/Qwen), a locally-run model developed by the company Alibaba. The model has hardware requirements which your computer may not satisfy: it needs a GPU and about 12 gigabytes of memory. When running this model on Google Colab, it is recommended to use the runtime environment `T4 GPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKING_CACHE_FILE = data['utils']['link_cache_file']\n",
    "\n",
    "\n",
    "def ollama_link_suggestion(model, texts_input):\n",
    "    entities = utils.extract_entities_from_ner_input(texts_input)\n",
    "    linking_cache = utils.read_json_file(LINKING_CACHE_FILE)\n",
    "    for entity in entities:\n",
    "        if (entity[\"entity_text\"] in linking_cache and \n",
    "            entity[\"text\"] in linking_cache[entity[\"entity_text\"]] and\n",
    "            model in linking_cache[entity[\"entity_text\"]][entity[\"text\"]]):\n",
    "            utils.squeal(f\"Retrieving entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} from cache\")\n",
    "            if \"link\" not in entity: entity[\"link\"] = {}\n",
    "            entity[\"link\"][model] = linking_cache[entity[\"entity_text\"]][entity[\"text\"]][model]\n",
    "        else:\n",
    "            utils.squeal(f\"Sending entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} to {model}\")\n",
    "            if \"ollama\" in sys.modules:\n",
    "                ollama = utils.importlib.import_module(\"ollama\")\n",
    "            else:\n",
    "                ollama = utils.import_ollama_module()\n",
    "            utils.install_ollama_model(model, ollama)\n",
    "            prompt = make_linking_prompt(entity[\"entity_text\"], entity[\"text\"])\n",
    "            if \"link\" not in entity: entity[\"link\"] = {}\n",
    "            entity[\"link\"][model] = utils.process_text_with_ollama(model, prompt, ollama)\n",
    "            if entity[\"entity_text\"] not in linking_cache:\n",
    "                linking_cache[entity[\"entity_text\"]] = {}\n",
    "            if entity[\"text\"] not in linking_cache[entity[\"entity_text\"]]:\n",
    "                linking_cache[entity[\"entity_text\"]][entity[\"text\"]] = {}\n",
    "            linking_cache[entity[\"entity_text\"]][entity[\"text\"]][model] = entity[\"link\"][model]\n",
    "            utils.write_json_file(LINKING_CACHE_FILE, linking_cache)\n",
    "    print(\"Finished processing\")\n",
    "    utils.save_data_to_json_file(linking_cache, file_name=LINKING_CACHE_FILE, in_colab=in_colab)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply the helper function to choose the best link type for each entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending entity \"Drovetti\" of text 3 to qwen3:8b\n",
      "[GIN] 2025/11/14 - 11:39:05 | 200 |    1.136625ms |       127.0.0.1 | GET      \"/api/tags\"\n",
      "[GIN] 2025/11/14 - 11:39:56 | 200 | 50.421623625s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "Finished processing\n",
      "Ô∏è‚úÖ Saved data to file linking_cache_97fa74d52115689dc382dba7c347d052c0eb9fbd.json\n",
      "{'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115', 'entities': []}\n"
     ]
    }
   ],
   "source": [
    "model = data['global']['model_name']\n",
    "MAX_PROCESSED = data['global']['max_processed']\n",
    "\n",
    "processed_entities = ollama_link_suggestion(model, texts_input[:MAX_PROCESSED])\n",
    "texts_output = add_linking_data_to_texts_input(texts_input[:MAX_PROCESSED], processed_entities)\n",
    "print({\"text_cleaned\": texts_output[0][\"text_cleaned\"],\n",
    "       \"entities\": [{\"entity_text\": entity[\"text\"], \n",
    "                     \"wikidata_id\": entity[\"wikidata_id\"], \n",
    "                     \"link\": list(entity[\"link\"].values())[0]} for entity in texts_output[0][\"entities\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Linking visualization\n",
    "\n",
    "We visualize the results of the linking process by displaying the numeric linking code in superscript next to the entity in its context. Please note that the six numeric codes represent the relations between the entity and the artefact and stand for the following:\n",
    "\n",
    "1. the entity is a person depicted on or by the artefact\n",
    "2. the entity is a person that created the artefact\n",
    "3. the entity is a person that discovered the artefact\n",
    "4. the entity is a person that owned the artefact\n",
    "5. the entity is a person in power during the period of the creation of the artefact\n",
    "6. the entity is a location depicted on or by the artefact\n",
    "7. the entity is a location where the artefact was created\n",
    "8. the entity is a location where the artefact was produced\n",
    "9. the entity is a location where the artefact was discovered\n",
    "10. the entity is the artefact's current location\n",
    "11. other entity type or other relation between entity and artefact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Statue of the goddess Sekhmet. Granodiorite. New Kingdom, 18th Dynasty, reign of <span style=\"border: 1px solid black; color: red;\">Amenhotep III</span><sup>Q42606,5</sup> (1390-1353 BC). <span style=\"border: 1px solid black; color: green;\">Thebes</span><sup>Q101583,7</sup>.. <span style=\"border: 1px solid black; color: red;\">Drovetti</span><sup>Q822895,4</sup> collection (1824). C. 256"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "**Statuette of the goddess Bastet. Bronze. Late Period (722-332 BC).. <span style=\"border: 1px solid black; color: red;\">Drovetti</span><sup>Q822895,4</sup> collection (1824). C. 271**"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for text_id, text in enumerate(texts_output):\n",
    "    if text_id < 3:\n",
    "        display(HTML(utils.mark_entities_in_text(text[\"text_llm_output\"], text[\"entities\"], linking_model=model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YDuesoBf067g"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "enriching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
