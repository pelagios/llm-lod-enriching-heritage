{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6a1c79",
   "metadata": {},
   "source": [
    "# Disambiguation with Candidates\n",
    "\n",
    "This notebook disambiguates the entities found by the NER notebook by linking them to WikiData concepts\n",
    "\n",
    "### Rationale\n",
    "\n",
    "When using LLMs, we can leverage context to help determine correct identifiers for entities found. One of the largest challenges with LLMs is getting them to generate the correct identifier for specific entities. Without context, an LLM will confidently generate a believable looking identifier code. When checked, however, users will often find these codes do not exist or are entirely wrong.\n",
    "\n",
    "We solve this problem with context. LLMs can receive context in one of two ways: either we can give it the context or we can use an LLM agentically with tools so that it can retrieve the context for itself. Both have their advantages, but both work within the same principal: context allows the LLM to get the correct identifier code so that it does not need to hallucinate one. While hallucinations are still possible, the chances are reduced if we provide a list of options to an LLM to choose from.\n",
    "\n",
    "In this notebook, we will explore the first of these options, where we provide the LLM with a list of candidates that were generated in the previous data notebook. To make things easier, we have pasted the output from that notebook here.\n",
    "\n",
    "It is also worth noting that providing the LLM with the necessary context is often quite cheaper (assuming you are using a paid-model), than letting the model agentically query the web or use other tools. We will see this in the next notebook.\n",
    "\n",
    "### Processing overview\n",
    "\n",
    "The process consists of the following steps:\n",
    "\n",
    "1. **Import required software libraries**: We start with importing required software libraries\n",
    "2. **Read the text that requires processing**: Next we obtain the input text from the NER notebook\n",
    "3. **Candidate extraction**\n",
    "4. **Disambiguation**: The text is sent to GPT with a prompt that instructs it to disambiguate entities based on the available candidates.\n",
    "5. **Disambiguation visualization**: The annotated text is displayed with colour-coded entity highlighting\n",
    "6. **Save results**: Save the results of the disambiguation process for future processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce3540",
   "metadata": {},
   "source": [
    "## 1. Import required software libraries\n",
    "\n",
    "Disambiguation requires importing some standard software libraries. This step may take some time when run for the first time but in successive runs it will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb2b75-48c8-4336-b647-d7b9bbedb631",
   "metadata": {},
   "source": [
    "First we import standard libraries which should always be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a02df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "import importlib\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37222d3a-cdd5-455a-99c3-0f9413795202",
   "metadata": {},
   "source": [
    "Next we import packages which may require installation on this device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684d7adb-4839-49be-9a32-3b0fa15700e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_package = \"游닍\"\n",
    "char_success = \"九\"\n",
    "char_failure = \"仇\"\n",
    "\n",
    "\n",
    "def safe_import(package_name):\n",
    "    \"\"\"Import a package;. If it missing, download it first\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        print(f\"{char_package} {package_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"Finished installing {package_name}\")\n",
    "        return importlib.import_module(package_name)\n",
    "\n",
    "\n",
    "openai = safe_import(\"openai\")\n",
    "pd = safe_import(\"pandas\")\n",
    "pydantic = safe_import(\"pydantic\")\n",
    "spacy = safe_import(\"spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccb443-6726-4448-828b-1dbb7248924a",
   "metadata": {},
   "source": [
    "Here is code for monitoring the progress of the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e29cb7a-142b-40ba-8b0e-752593db94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    \"\"\"Clear the output buffer of the current cell and print the given text\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: \n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ddd87-872d-494e-9250-456739fc2221",
   "metadata": {},
   "source": [
    "Finally we set settings required for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44afc346-6d51-4e5f-a2c0-d62961732d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def set_css():\n",
    "    \"\"\"Fix line wrapping of output texts for Google Colab\"\"\"\n",
    "    display(HTML(\"<style> pre { white-space: pre-wrap; </style>\"))\n",
    "\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    get_ipython().events.register('pre_run_cell', set_css)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559246c-da0e-4d94-b857-65527220e7f6",
   "metadata": {},
   "source": [
    "## 2. Read the texts that require processing\n",
    "\n",
    "The texts should have been processed by the ner.ipynb notebook. The file read here is an output file of that notebook. We read the texts and the associated metadata and show the first text with its entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f3d702-647d-4c64-acab-8da3028f7acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115', 'entities': [{'text': 'Anubis', 'label': 'PERSON', 'start_char': 21, 'end_char': 27}]}\n"
     ]
    }
   ],
   "source": [
    "infile_name = \"output_ner_5f12fa7c16d33ea378148197569f999f774f7481.json\"\n",
    "\n",
    "with open(infile_name, \"r\") as infile:\n",
    "    texts_input = json.load(infile)\n",
    "    infile.close()\n",
    "print({\"text_cleaned\": texts_input[0][\"text_cleaned\"], \n",
    "       \"entities\": texts_input[0][\"entities\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6363b-74e5-4177-b3f4-5cf2f7c3fafe",
   "metadata": {},
   "source": [
    "## 3. Disambiguation candidate generation\n",
    "\n",
    "We need candidate ids for the entities to make the task for the LLM easier. We obtain these candidate ids by searching for the entities in wikidata.org. The search step returns seven candidate ids with a description text for each of them. In order not to overload the website, we use a cache for storing entities that were looked up earlier. We also wait for 10 seconds (variable `SLEEP_TIME_FETCH_PAGE`) between the searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e38391-d3ff-476c-b4ef-56f795a19f60",
   "metadata": {},
   "source": [
    "First we define five helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "495787e4-d72b-43c4-9834-dbd65174cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_TIME_FETCH_PAGE = 10\n",
    "USER_AGENT = \"Pelagios/0.0 (https://github.com/pelagios/; e.tjongkimsang@esciencecenter.nl) generic-library/0.0\"\n",
    "\n",
    "def query_wikidata_for_single_entity(entity_text):\n",
    "    \"\"\"Read Wikidata data on entity_text and return it\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": USER_AGENT,\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'language': 'en',\n",
    "        'format': 'json',\n",
    "        'limit': '7',\n",
    "        'search': entity_text \n",
    "    }\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    time.sleep(SLEEP_TIME_FETCH_PAGE)\n",
    "    wikidata_data = requests.get(url, params=params, headers=headers)\n",
    "    return wikidata_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb658c10-b74e-4a2e-851a-9fb4a2dfba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_WIKIDATA = \"disambiguation_cache_wikidata.json\"\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    \"\"\"Read a json file and return its contents\"\"\"\n",
    "    with open(file_name, \"r\") as infile:\n",
    "        json_data = json.load(infile)\n",
    "        infile.close()\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59f1969-2ad2-4bf9-aff1-d200255d2bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_file(file_name, json_data):\n",
    "    \"\"\"Write json data to a file with the specified name\"\"\"\n",
    "    json_string = json.dumps(json_data, ensure_ascii=False, indent=2)\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        print(json_string, end=\"\", file=outfile)\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4087c95-0ee6-4b3e-865e-1258c7d4fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_ner_input(texts_input):\n",
    "    \"\"\"For each entity in the input text return the entity text, context text and context text id\"\"\" \n",
    "    return [{\"entity_text\": entity[\"text\"],\n",
    "             \"text_id\": index,\n",
    "             \"text\": text[\"text_cleaned\"]} \n",
    "            for index, text in enumerate(texts_input) for entity in text[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31eaf53f-ce28-4125-81bd-567a3ab793fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wikidata_candidates_for_entities(entities):\n",
    "    \"\"\"Lookup candidate ids for the entities on wikidata.org and return them\"\"\"\n",
    "    wikidata_cache = read_json_file(DISAMBUGUATION_CACHE_FILE_WIKIDATA)\n",
    "    for entity in entities:\n",
    "        if entity[\"entity_text\"] not in wikidata_cache.keys():\n",
    "            squeal(f\"Looking up entity {entity['entity_text']} (text id {entity['text_id'] + 1}) on WikiData.org...\")\n",
    "            wikidata_data = query_wikidata_for_single_entity(entity[\"entity_text\"])\n",
    "            wikidata_cache[entity[\"entity_text\"]] = [{\"id\": candidate[\"id\"], \n",
    "                                                            \"description\": candidate[\"description\"]} \n",
    "                                                            for candidate in json.loads(wikidata_data.text)[\"search\"]\n",
    "                                                            if \"description\" in candidate.keys()]\n",
    "    write_json_file(DISAMBUGUATION_CACHE_FILE_WIKIDATA, wikidata_cache)\n",
    "    for entity in entities:\n",
    "        entity[\"candidates\"] = wikidata_cache[entity[\"entity_text\"]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c4b7a-0281-4ba4-bebe-84b5de6d03c9",
   "metadata": {},
   "source": [
    "Next we extract the entities from the input data and call a helper function for finding the candidate ids. These will be stored in the `entities` variable. We show the first item of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df11bef-291b-4dab-b907-8291370ebff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_text': 'Anubis', 'candidates': [{'id': 'Q14896497', 'description': 'genus of insects'}, {'id': 'Q47534', 'description': 'Egyptian deity of mummification and the afterlife, usually depicted as a man with a canine head'}, {'id': 'Q134301689', 'description': 'anti-web scraping software'}, {'id': 'Q135055514', 'description': 'video game developed by OA Game Studio'}, {'id': 'Q145772', 'description': 'asteroid'}, {'id': 'Q5514020', 'description': 'daemon that sits between the Mail User Agent (MUA) and the Mail Transfer Agent (MTA)'}, {'id': 'Q108330387', 'description': 'British Drag Queen'}]}\n"
     ]
    }
   ],
   "source": [
    "entities = extract_entities_from_ner_input(texts_input)\n",
    "entities = find_wikidata_candidates_for_entities(entities)\n",
    "print({key: entities[0][key] for key in ['entity_text', 'candidates']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48267cd-7e1b-4128-9f32-4ac20a54628b",
   "metadata": {},
   "source": [
    "## 4. Disambiguate entities with GPT\n",
    "\n",
    "We disambiguate entities by sending a prompt with each entity text, the context text and the WikiData candidates ids and their descriptions to an LLM. We ask the LLM to return the id associated with the description that best matches the entity in the given context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb61643-a253-4dc4-a705-6475a4a78e55",
   "metadata": {},
   "source": [
    "First we define five helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01eb98d1-f840-48fb-a342-fbe992323e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(entity, text, candidates):\n",
    "    \"\"\"Create an LLM prompt, given a text and target labels and return it\"\"\"\n",
    "    return f\"\"\"\n",
    "Disambiguate the entity \"{entity}\" in the following text.\n",
    "\n",
    "{text}\n",
    "\n",
    "Here are the candidates, in json format:\n",
    "\n",
    "{candidates}\n",
    "\n",
    "Only return the relevant id, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c70ab90-9295-4c8f-9297-71353572c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_api_key():\n",
    "    \"\"\"Extract OpenAI API key from environment or file and return it\"\"\"\n",
    "    load_dotenv()\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai_api_key:\n",
    "        try:\n",
    "            with open(\"OPENAI_API_KEY\", \"r\") as infile:\n",
    "                openai_api_key = infile.read().strip()\n",
    "                infile.close()\n",
    "        except:\n",
    "            pass\n",
    "    if not openai_api_key:\n",
    "        print(f\"{char_failure} no openai_api_key found!\")\n",
    "        return \"\"\n",
    "    return openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88389bc3-40ce-48fc-9178-b3c3e58edf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_openai(openai_api_key):\n",
    "    \"\"\"Connect to OpenAI and return processing space\"\"\"\n",
    "    return openai.OpenAI(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "541cb7d5-8807-4156-9d4f-ae35cd2def12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_with_gpt(openai_client, model, prompt):\n",
    "    \"\"\"Send text to OpenAI via prompt and return results\"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        print(f\"{char_failure} GPT call failed\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef64fd3d-e7fc-41b7-b505-57c0425ef9a3",
   "metadata": {},
   "source": [
    "Next, we create a prompt for each entity, send it to the LLM and collect the responses. When the combination of the entity and the context text is available in the cache of previously processed entities, we skip consulting the gpt and use the wikidata id stored in the cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75deea80-7bab-41cf-b0ac-fb4217c3010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_GPT = \"disambiguation_cache_gpt.json\"\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "def openai_wikidata_id_selection(model, entities):\n",
    "    openai_api_key = get_openai_api_key()\n",
    "    openai_client = connect_to_openai(openai_api_key)\n",
    "    gpt_cache = read_json_file(DISAMBUGUATION_CACHE_FILE_GPT)\n",
    "    for entity in entities:\n",
    "        if (entity[\"entity_text\"] in gpt_cache and \n",
    "            entity[\"text\"] in gpt_cache[entity[\"entity_text\"]] and\n",
    "            model in gpt_cache[entity[\"entity_text\"]][entity[\"text\"]]):\n",
    "            squeal(f\"Retrieving entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} from cache\")\n",
    "            entity[\"wikidata_id\"] = gpt_cache[entity[\"entity_text\"]][entity[\"text\"]][model] | {\"model\": model}\n",
    "        else:\n",
    "            squeal(f\"Sending entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} to GPT\")\n",
    "            prompt = make_prompt(entity[\"entity_text\"], entity[\"text\"], entity[\"candidates\"])\n",
    "            entity[\"wikidata_id\"] = {model: process_text_with_gpt(openai_client, model, prompt)}\n",
    "            if entity[\"entity_text\"] not in gpt_cache:\n",
    "                gpt_cache[entity[\"entity_text\"]] = {}\n",
    "            if entity[\"text\"] not in gpt_cache[entity[\"entity_text\"]]:\n",
    "                gpt_cache[entity[\"entity_text\"]][entity[\"text\"]] = {}\n",
    "            gpt_cache[entity[\"entity_text\"]][entity[\"text\"]][model] = {key: value \n",
    "                                                                       for key, value in entity[\"wikidata_id\"].items()\n",
    "                                                                       if key != model}\n",
    "    write_json_file(DISAMBUGUATION_CACHE_FILE_GPT, gpt_cache)\n",
    "    print(\"Finished processing\")\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2b5fd-85e5-472c-8119-e2e02e3276ff",
   "metadata": {},
   "source": [
    "Next, we call the function to select the right wikidata id with gpt. The results are stored in the `entities` variable. We show the first of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2cf750e-8af4-40c3-9951-bb32b5df2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wikidata_ids_to_texts_input(texts_input, entities):\n",
    "    \"\"\"Insert the retrieved wikidata ids into the variable text_inputs and return it\"\"\"\n",
    "    entities_per_text = {}\n",
    "    for entity in entities:\n",
    "        if entity[\"text_id\"] not in entities_per_text:\n",
    "            entities_per_text[entity[\"text_id\"]] = {}\n",
    "        entities_per_text[entity[\"text_id\"]][entity[\"entity_text\"]] = entity\n",
    "    for text_id, text in enumerate(texts_input):\n",
    "        for entity in text[\"entities\"]:\n",
    "            if text_id in entities_per_text and entity[\"text\"] in entities_per_text[text_id]:\n",
    "                entity[\"wikidata_id\"] = entities_per_text[text_id][entity[\"text\"]][\"wikidata_id\"]\n",
    "    return texts_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61717672-8733-4f59-b564-b060e5e88ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving entity \"Bagnani\" of text 100 from cache\n",
      "Finished processing\n",
      "{'entity_text': 'Anubis', 'text': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115', 'wikidata_id': {'id': 'Q47534', 'description': 'Egyptian deity of mummification and the afterlife, usually depicted as a man with a canine head', 'model': 'gpt-4o-mini'}}\n"
     ]
    }
   ],
   "source": [
    "entities = openai_wikidata_id_selection(model, entities)\n",
    "texts_output = add_wikidata_ids_to_texts_input(texts_input, entities)\n",
    "print({key: value for key, value in entities[0].items() if key in [\"entity_text\", \"text\", \"wikidata_id\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd36a9b-9bb7-4f25-b32a-015293afcc27",
   "metadata": {},
   "source": [
    "## 5. Disambiguation visualization\n",
    "\n",
    "Here we show the results of the disambiguation task in a more readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b95c1-a034-4d84-989e-ceaf3ace9456",
   "metadata": {},
   "source": [
    "First we define a helper function for performing the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67b0e600-18a6-4f96-baf5-8b158f9f104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = {\"PERSON\": \"red\", \"LOCATION\": \"green\", \"OTHER\": \"blue\"}\n",
    "\n",
    "\n",
    "def mark_entities_in_text(texts_input, entities):\n",
    "    \"\"\"Convert the text to HTML with colored antities and return these\"\"\"\n",
    "    for entity in reversed(entities):\n",
    "        entity_label = entity[\"label\"] if entity[\"label\"] in COLORS.keys() else \"OTHER\"\n",
    "        if \"wikidata_id\" in entity:\n",
    "            texts_input = texts_input[:entity[\"end_char\"]] + f\"<sup>{entity['wikidata_id']['id']}</sup>\" + texts_input[entity[\"end_char\"]:]\n",
    "        texts_input = texts_input[:entity[\"end_char\"]] + \"</span>\" + texts_input[entity[\"end_char\"]:]\n",
    "        texts_input = (texts_input[:entity[\"start_char\"]] + \n",
    "                      f\"<span style=\\\"border: 1px solid black; color: {COLORS[entity_label]};\\\">\" + \n",
    "                      texts_input[entity[\"start_char\"]:])\n",
    "    return texts_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842c94b-63c4-4d43-bfea-ef595b440bba",
   "metadata": {},
   "source": [
    "Then we call the helper function and show the first three texts with their entities and the WikiData ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "867bafc8-2fce-4d03-a8db-9f9296b1f16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Statuette of the god <span style=\"border: 1px solid black; color: red;\">Anubis</span><sup>Q47534</sup>. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"border: 1px solid black; color: red;\">Sekhmet</span><sup>Q146104</sup> was a goddess. <span style=\"border: 1px solid black; color: green;\">Thebes</span><sup>Q101583</sup> is referenced. The object is made of granodiorite and belongs to the Drovetti collection (1824). It dates back to the New Kingdom, 18th Dynasty, during the reign of <span style=\"border: 1px solid black; color: red;\">Amenhotep III</span><sup>Q42606</sup> (1390-1353 BC)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Statuette of the goddess <span style=\"border: 1px solid black; color: red;\">Bastet</span><sup>Q129106</sup>. Bronze. Late Period (722-332 BC).. <span style=\"border: 1px solid black; color: red;\">Drovetti</span><sup>Q822895</sup> collection (1824). C. 271"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entities_all = []\n",
    "for text_id, text in enumerate(texts_output):\n",
    "    if text_id < 3:\n",
    "        display(HTML(mark_entities_in_text(text[\"text_llm_output\"], text[\"entities\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64655dc-0d64-4746-96aa-ff008cf4f427",
   "metadata": {},
   "source": [
    "## 6. Save results\n",
    "\n",
    "We save the results in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07291f01-377d-400e-bf86-227f0db3a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(texts, key):\n",
    "    \"\"\"Save preprocessed texts in a json file\"\"\"\n",
    "    json_string = json.dumps(texts, ensure_ascii=False, indent=2)\n",
    "    hash = hashlib.sha1(json_string.encode(\"utf-8\")).hexdigest()\n",
    "    output_file_name = f\"output_{key}{hash}.json\"\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        print(json_string, end=\"\", file=output_file)\n",
    "        output_file.close()\n",
    "        if IN_COLAB:\n",
    "            try:\n",
    "                files.download(output_file_name)\n",
    "                print(f\"勇끝char_success} Downloaded preprocessed texts to file {output_file_name}\")\n",
    "            except:\n",
    "                print(f\"勇끝char_failure} Downloading preprocessed texts failed!\")\n",
    "        else:\n",
    "            print(f\"勇끝char_success} Saved preprocessed texts to file {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3db2ed8b-d12b-4945-b749-446c0f8b476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "勇九 Saved preprocessed texts to file output_disambiguation_ba25101ddbe8830789bfdfdb3a5ba6312d6853e6.json\n"
     ]
    }
   ],
   "source": [
    "save_results(texts_output, \"disambiguation_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca75d4-3d34-4f5f-9dfc-2dcf299c148d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
