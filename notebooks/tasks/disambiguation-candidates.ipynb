{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6a1c79",
   "metadata": {},
   "source": [
    "# Disambiguation with Candidates\n",
    "\n",
    "This notebook disambiguates the entities found by the NER notebook by linking them to WikiData concepts\n",
    "\n",
    "### Rationale\n",
    "\n",
    "When using LLMs, we can leverage context to help determine correct identifiers for entities found. One of the largest challenges with LLMs is getting them to generate the correct identifier for specific entities. Without context, an LLM will confidently generate a believable looking identifier code. When checked, however, users will often find these codes do not exist or are entirely wrong.\n",
    "\n",
    "We solve this problem with context. LLMs can receive context in one of two ways: either we can give it the context or we can use an LLM agentically with tools so that it can retrieve the context for itself. Both have their advantages, but both work within the same principal: context allows the LLM to get the correct identifier code so that it does not need to hallucinate one. While hallucinations are still possible, the chances are reduced if we provide a list of options to an LLM to choose from.\n",
    "\n",
    "In this notebook, we will explore the first of these options, where we provide the LLM with a list of candidates that were generated in the previous data notebook. To make things easier, we have pasted the output from that notebook here.\n",
    "\n",
    "It is also worth noting that providing the LLM with the necessary context is often quite cheaper (assuming you are using a paid-model), than letting the model agentically query the web or use other tools. We will see this in the next notebook.\n",
    "\n",
    "### Processing overview\n",
    "\n",
    "The process consists of the following steps:\n",
    "\n",
    "1. **Import required software libraries**: We start with importing required software libraries\n",
    "2. **Read the text that requires processing**: Next we obtain the input text from the NER notebook\n",
    "3. **Candidate extraction**\n",
    "4. **Disambiguation**: The text is sent to GPT with a prompt that instructs it to disambiguate entities based on the available candidates.\n",
    "5. **Disambiguation visualization**: The annotated text is displayed with colour-coded entity highlighting\n",
    "6. **Save results**: Save the results of the disambiguation process for future processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce3540",
   "metadata": {},
   "source": [
    "## 1. Import required software libraries\n",
    "\n",
    "Disambiguation requires importing some standard software libraries. This step may take some time when run for the first time but in successive runs it will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb2b75-48c8-4336-b647-d7b9bbedb631",
   "metadata": {},
   "source": [
    "First we import standard libraries which should always be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a02df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import importlib\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37222d3a-cdd5-455a-99c3-0f9413795202",
   "metadata": {},
   "source": [
    "Next we import packages which may require installation on this device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684d7adb-4839-49be-9a32-3b0fa15700e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_package = \"üì¶\"\n",
    "char_success = \"‚úÖ\"\n",
    "char_failure = \"‚ùå\"\n",
    "\n",
    "\n",
    "def safe_import(package_name):\n",
    "    \"\"\"Import a package;. If it missing, download it first\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        print(f\"{char_package} {package_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"Finished installing {package_name}\")\n",
    "        return importlib.import_module(package_name)\n",
    "\n",
    "\n",
    "openai = safe_import(\"openai\")\n",
    "pd = safe_import(\"pandas\")\n",
    "pydantic = safe_import(\"pydantic\")\n",
    "spacy = safe_import(\"spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccb443-6726-4448-828b-1dbb7248924a",
   "metadata": {},
   "source": [
    "Here is code for monitoring the progress of the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e29cb7a-142b-40ba-8b0e-752593db94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    \"\"\"Clear the output buffer of the current cell and print the given text\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: \n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559246c-da0e-4d94-b857-65527220e7f6",
   "metadata": {},
   "source": [
    "## 2. Read the texts that requires processing\n",
    "\n",
    "The texts should have been processed by the ner.ipynb notebook. The file read here is an output file of that notebook. We read the texts and the associated metadata and show the first text with its entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f3d702-647d-4c64-acab-8da3028f7acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115', 'entities': [{'text': 'Anubis', 'label': 'PERSON', 'start_char': 21, 'end_char': 27}]}\n"
     ]
    }
   ],
   "source": [
    "infile_name = \"output_ner_5f12fa7c16d33ea378148197569f999f774f7481.json\"\n",
    "\n",
    "with open(infile_name, \"r\") as infile:\n",
    "    texts_input = json.load(infile)\n",
    "    infile.close()\n",
    "print({\"text_cleaned\": texts_input[0][\"text_cleaned\"], \n",
    "       \"entities\": texts_input[0][\"entities\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6363b-74e5-4177-b3f4-5cf2f7c3fafe",
   "metadata": {},
   "source": [
    "## 3. Disambiguation candidate generation\n",
    "\n",
    "We need candidate ids for the entities to make the task for the LLM easier. We obtain these candidate ids by searching for the entities in wikidata.org. The search step returns seven candidate ids with a description text for each of them. In order not to overload the website, we use a cache for storing entities that were looked up earlier. We also wait for 10 seconds (variable `SLEEP_TIME_FETCH_PAGE`) between the searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e38391-d3ff-476c-b4ef-56f795a19f60",
   "metadata": {},
   "source": [
    "First we define five helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "495787e4-d72b-43c4-9834-dbd65174cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_TIME_FETCH_PAGE = 10\n",
    "USER_AGENT = \"Pelagios/0.0 (https://github.com/pelagios/; e.tjongkimsang@esciencecenter.nl) generic-library/0.0\"\n",
    "\n",
    "def query_wikidata_for_single_entity(entity_text):\n",
    "    \"\"\"Read Wikidata data on entity_text and return it\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": USER_AGENT,\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'language': 'en',\n",
    "        'format': 'json',\n",
    "        'limit': '7',\n",
    "        'search': entity_text \n",
    "    }\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    time.sleep(SLEEP_TIME_FETCH_PAGE)\n",
    "    wikidata_data = requests.get(url, params=params, headers=headers)\n",
    "    return wikidata_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb658c10-b74e-4a2e-851a-9fb4a2dfba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_WIKIDATA = \"disambiguation_cache_wikidata.json\"\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    \"\"\"Read a json file and return its contents\"\"\"\n",
    "    with open(file_name, \"r\") as infile:\n",
    "        json_data = json.load(infile)\n",
    "        infile.close()\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59f1969-2ad2-4bf9-aff1-d200255d2bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_file(file_name, json_data):\n",
    "    \"\"\"Write json data to a file with the specified name\"\"\"\n",
    "    json_string = json.dumps(json_data, ensure_ascii=False, indent=2)\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        print(json_string, end=\"\", file=outfile)\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4087c95-0ee6-4b3e-865e-1258c7d4fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_ner_input(texts_input):\n",
    "    \"\"\"For each entity in the input text return the entity text, context text and context text id\"\"\" \n",
    "    return [{\"entity_text\": entity[\"text\"],\n",
    "             \"text_id\": index,\n",
    "             \"text\": text[\"text_cleaned\"]} \n",
    "            for index, text in enumerate(texts_input) for entity in text[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31eaf53f-ce28-4125-81bd-567a3ab793fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wikidata_candidates_for_entities(entities):\n",
    "    \"\"\"Lookup candidate ids for the entities on wikidata.org and return them\"\"\"\n",
    "    wikidata_cache = read_json_file(DISAMBUGUATION_CACHE_FILE_WIKIDATA)\n",
    "    for entity in entities:\n",
    "        if entity[\"entity_text\"] not in wikidata_cache.keys():\n",
    "            squeal(f\"Looking up entity {entity['entity_text']} (text id {entity['text_id'] + 1}) on WikiData.org...\")\n",
    "            wikidata_data = query_wikidata_for_single_entity(entity[\"entity_text\"])\n",
    "            wikidata_cache[entity[\"entity_text\"]] = [{\"id\": candidate[\"id\"], \n",
    "                                                            \"description\": candidate[\"description\"]} \n",
    "                                                            for candidate in json.loads(wikidata_data.text)[\"search\"]\n",
    "                                                            if \"description\" in candidate.keys()]\n",
    "    write_json_file(DISAMBUGUATION_CACHE_FILE, wikidata_cache)\n",
    "    for entity in entities:\n",
    "        entity[\"candidates\"] = dwikidata_cache[entity[\"entity_text\"]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c4b7a-0281-4ba4-bebe-84b5de6d03c9",
   "metadata": {},
   "source": [
    "Next we extract the entities from the input data and call a helper function for finding the candidate ids. These will be stored in the entities variable. We show the first item of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2df11bef-291b-4dab-b907-8291370ebff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_text': 'Anubis', 'candidates': [{'id': 'Q14896497', 'description': 'genus of insects'}, {'id': 'Q47534', 'description': 'Egyptian deity of mummification and the afterlife, usually depicted as a man with a canine head'}, {'id': 'Q134301689', 'description': 'anti-web scraping software'}, {'id': 'Q135055514', 'description': 'video game developed by OA Game Studio'}, {'id': 'Q145772', 'description': 'asteroid'}, {'id': 'Q5514020', 'description': 'daemon that sits between the Mail User Agent (MUA) and the Mail Transfer Agent (MTA)'}, {'id': 'Q108330387', 'description': 'British Drag Queen'}]}\n"
     ]
    }
   ],
   "source": [
    "entities = extract_entities_from_ner_input(texts_input)\n",
    "entities = find_wikidata_candidates_for_entities(entities)\n",
    "print({key: entities[0][key] for key in ['entity_text', 'candidates']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48267cd-7e1b-4128-9f32-4ac20a54628b",
   "metadata": {},
   "source": [
    "## 4. Disambiguate entities with GPT\n",
    "\n",
    "We disambiguate entities by sending a prompt with each entity text, the context text and the WikiData candidates ids and their descriptions to an LLM. We ask the LLM to return the id associated with the description that best matches the entity in the given context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb61643-a253-4dc4-a705-6475a4a78e55",
   "metadata": {},
   "source": [
    "First we define four helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01eb98d1-f840-48fb-a342-fbe992323e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(entity, text, candidates):\n",
    "    \"\"\"Create an LLM prompt, given a text and target labels and return it\"\"\"\n",
    "    return f\"\"\"\n",
    "Disambiguate the entity \"{entity}\" in the following text.\n",
    "\n",
    "{text}\n",
    "\n",
    "Here are the candidates, in json format:\n",
    "\n",
    "{candidates}\n",
    "\n",
    "Only return the relevant id, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c70ab90-9295-4c8f-9297-71353572c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_api_key():\n",
    "    \"\"\"Extract OpenAI API key from environment or file and return it\"\"\"\n",
    "    load_dotenv()\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai_api_key:\n",
    "        try:\n",
    "            with open(\"OPENAI_API_KEY\", \"r\") as infile:\n",
    "                openai_api_key = infile.read().strip()\n",
    "                infile.close()\n",
    "        except:\n",
    "            pass\n",
    "    if not openai_api_key:\n",
    "        print(f\"{char_failure} no openai_api_key found!\")\n",
    "        return \"\"\n",
    "    return openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88389bc3-40ce-48fc-9178-b3c3e58edf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_openai(openai_api_key):\n",
    "    \"\"\"Connect to OpenAI and return processing space\"\"\"\n",
    "    return openai.OpenAI(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "541cb7d5-8807-4156-9d4f-ae35cd2def12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_with_gpt(openai_client, model, prompt):\n",
    "    \"\"\"Send text to OpenAI via prompt and return results\"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        print(f\"{char_failure} GPT call failed\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef64fd3d-e7fc-41b7-b505-57c0425ef9a3",
   "metadata": {},
   "source": [
    "Next, we create a prompt for each entity, send it to the LLM and collect the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0e26b3f8-1d1f-435b-91a5-989cfc599d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity_text': 'Anubis',\n",
       " 'text_id': 0,\n",
       " 'text': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115',\n",
       " 'candidates': [{'id': 'Q14896497', 'description': 'genus of insects'},\n",
       "  {'id': 'Q47534',\n",
       "   'description': 'Egyptian deity of mummification and the afterlife, usually depicted as a man with a canine head'},\n",
       "  {'id': 'Q134301689', 'description': 'anti-web scraping software'},\n",
       "  {'id': 'Q135055514',\n",
       "   'description': 'video game developed by OA Game Studio'},\n",
       "  {'id': 'Q145772', 'description': 'asteroid'},\n",
       "  {'id': 'Q5514020',\n",
       "   'description': 'daemon that sits between the Mail User Agent (MUA) and the Mail Transfer Agent (MTA)'},\n",
       "  {'id': 'Q108330387', 'description': 'British Drag Queen'}],\n",
       " 'wikidata_id': {'gpt-4o-mini': {'id': 'Q47534',\n",
       "   'description': 'Egyptian deity of mummification and the afterlife, usually depicted as a man with a canine head'}}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "75deea80-7bab-41cf-b0ac-fb4217c3010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_GPT = \"disambiguation_cache_gpt.json\"\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "openai_api_key = get_openai_api_key()\n",
    "openai_client = connect_to_openai(openai_api_key)\n",
    "gpt_cache = read_json_file(DISAMBUGUATION_CACHE_FILE_GPT)\n",
    "for entity in entities:\n",
    "    if (entity[\"entity_text\"] in gpt_cache and \n",
    "        entity[\"text\"] in gpt_cache[entity[\"entity_text\"]] and\n",
    "        model in gpt_cache[entity[\"entity_text\"]][entity[\"text\"]]):\n",
    "        wikidata_id = gpt_cache[entity[\"entity_text\"]][entity[\"text\"]][model]\n",
    "    else:\n",
    "        squeal(f\"Processing text: {entity['text_id'] + 1}; entity: {entity['entity_text']}\")\n",
    "        prompt = make_prompt(entity[\"entity_text\"], entity[\"text\"], entity[\"candidates\"])\n",
    "        entity[\"wikidata_id\"] = {model: process_text_with_gpt(openai_client, model, prompt)}\n",
    "        if entity[\"entity_text\"] not in gpt_cache:\n",
    "            gpt_cache[entity[\"entity_text\"]] = {}\n",
    "        if entity[\"text\"] not in gpt_cache[entity[\"entity_text\"]]:\n",
    "            gpt_cache[entity[\"entity_text\"]][entity[\"text\"]] = {}\n",
    "        gpt_cache[entity[\"entity_text\"]][entity[\"text\"]][model] = entity[\"wikidata_id\"]\n",
    "write_json_file(DISAMBUGUATION_CACHE_FILE_GPT, gpt_cache)\n",
    "print(\"Finished processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64655dc-0d64-4746-96aa-ff008cf4f427",
   "metadata": {},
   "source": [
    "## 5. Save results\n",
    "\n",
    "We save the results in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07291f01-377d-400e-bf86-227f0db3a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(texts, key):\n",
    "    \"\"\"Save preprocessed texts in a json file\"\"\"\n",
    "    json_string = json.dumps(texts, ensure_ascii=False, indent=2)\n",
    "    hash = hashlib.sha1(json_string.encode(\"utf-8\")).hexdigest()\n",
    "    output_file_name = f\"output_{key}{hash}.json\"\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        print(json_string, end=\"\", file=output_file)\n",
    "        output_file.close()\n",
    "        if IN_COLAB:\n",
    "            try:\n",
    "                files.download(output_file_name)\n",
    "                print(f\"Ô∏è{char_success} Downloaded preprocessed texts to file {output_file_name}\")\n",
    "            except:\n",
    "                print(f\"Ô∏è{char_failure} Downloading preprocessed texts failed!\")\n",
    "        else:\n",
    "            print(f\"Ô∏è{char_success} Saved preprocessed texts to file {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2f2a0fbf-e365-4e88-ac19-d0a0e1d603b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity_text': 'Anubis',\n",
       " 'text_id': 0,\n",
       " 'text': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115',\n",
       " 'candidates': [{'id': 'Q14896497', 'description': 'genus of insects'},\n",
       "  {'id': 'Q47534',\n",
       "   'description': 'Egyptian deity of mummification and the afterlife, usually depicted as a man with a canine head'},\n",
       "  {'id': 'Q134301689', 'description': 'anti-web scraping software'},\n",
       "  {'id': 'Q135055514',\n",
       "   'description': 'video game developed by OA Game Studio'},\n",
       "  {'id': 'Q145772', 'description': 'asteroid'},\n",
       "  {'id': 'Q5514020',\n",
       "   'description': 'daemon that sits between the Mail User Agent (MUA) and the Mail Transfer Agent (MTA)'},\n",
       "  {'id': 'Q108330387', 'description': 'British Drag Queen'}],\n",
       " 'wikidata_id': {'gpt-4o-mini': 'Q47534'}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fd6d2",
   "metadata": {},
   "source": [
    "## Old Visualization Code\n",
    "\n",
    "This bit of code is just for making it easier to visualize our data at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74853169",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "\n",
    "def annotated_text_to_spacy_doc(text, nlp=None):\n",
    "    \"\"\"\n",
    "    Converts annotated text in format [Entity](LABEL) to a spaCy Doc with entity spans.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text with annotations like \"[Tom](PERSON) worked for [Microsoft](ORGANIZATION)\"\n",
    "        nlp (spacy.Language, optional): spaCy language model. If None, uses blank English model.\n",
    "    \n",
    "    Returns:\n",
    "        spacy.tokens.Doc: spaCy document with entity spans set\n",
    "        \n",
    "    Example:\n",
    "        >>> text = \"[Tom](PERSON) worked for [Microsoft](ORGANIZATION) in 2020 before he lived in [Rome](LOCATION).\"\n",
    "        >>> doc = annotated_text_to_spacy_doc(text)\n",
    "        >>> spacy.displacy.render(doc, style=\"ent\")\n",
    "    \"\"\"\n",
    "    if nlp is None:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "    \n",
    "    # Pattern to match [text](LABEL) format\n",
    "    pattern = r'\\[([^\\]]+)\\]\\(([^)]+)\\)'\n",
    "    \n",
    "    # Parse the text to extract tokens and entity information\n",
    "    tokens = []\n",
    "    entity_spans = []  # List of (start_token_idx, end_token_idx, label)\n",
    "    custom_labels = set()\n",
    "    \n",
    "    # Split text by the pattern and process each part\n",
    "    last_end = 0\n",
    "    token_idx = 0\n",
    "    \n",
    "    for match in re.finditer(pattern, text):\n",
    "        # Add tokens before the entity\n",
    "        before_entity = text[last_end:match.start()]\n",
    "        if before_entity.strip():\n",
    "            # Tokenize the text before the entity\n",
    "            before_tokens = before_entity.split()\n",
    "            tokens.extend(before_tokens)\n",
    "            token_idx += len(before_tokens)\n",
    "        \n",
    "        # Add the entity tokens\n",
    "        entity_text = match.group(1)\n",
    "        entity_label = match.group(2)\n",
    "        custom_labels.add(entity_label)\n",
    "        \n",
    "        # Tokenize the entity text\n",
    "        entity_tokens = entity_text.split()\n",
    "        start_token_idx = token_idx\n",
    "        tokens.extend(entity_tokens)\n",
    "        token_idx += len(entity_tokens)\n",
    "        end_token_idx = token_idx\n",
    "        \n",
    "        # Store entity span information\n",
    "        entity_spans.append((start_token_idx, end_token_idx, entity_label))\n",
    "        \n",
    "        last_end = match.end()\n",
    "    \n",
    "    # Add any remaining tokens after the last entity\n",
    "    remaining = text[last_end:]\n",
    "    if remaining.strip():\n",
    "        remaining_tokens = remaining.split()\n",
    "        tokens.extend(remaining_tokens)\n",
    "    \n",
    "    # Add custom labels to the NLP model if they don't exist\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    for label in custom_labels:\n",
    "        ner.add_label(label)\n",
    "    \n",
    "    # Create spaces array (True for tokens that should have a space after them)\n",
    "    # Simple heuristic: all tokens except the last one get a space\n",
    "    spaces = [True] * len(tokens)\n",
    "    if tokens:\n",
    "        spaces[-1] = False\n",
    "    \n",
    "    # Create the Doc from tokens\n",
    "    doc = Doc(nlp.vocab, words=tokens, spaces=spaces)\n",
    "    \n",
    "    # Create entity spans\n",
    "    entities = []\n",
    "    for start_idx, end_idx, label in entity_spans:\n",
    "        if start_idx < len(doc) and end_idx <= len(doc):\n",
    "            span = Span(doc, start_idx, end_idx, label=label)\n",
    "            entities.append(span)\n",
    "    \n",
    "    # Set entities on the document\n",
    "    doc.ents = entities\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "def visualize_annotated_text(text, nlp=None, style=\"ent\", jupyter=True):\n",
    "    \"\"\"\n",
    "    Convenience function to convert annotated text and visualize it with displaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text with annotations like \"[Tom](PERSON) worked for [Microsoft](ORGANIZATION)\"\n",
    "        nlp (spacy.Language, optional): spaCy language model. If None, uses blank English model.\n",
    "        style (str): displaCy style (\"ent\" or \"dep\")\n",
    "        jupyter (bool): Whether to render for Jupyter notebook\n",
    "    \n",
    "    Returns:\n",
    "        Rendered visualization (HTML string if not in Jupyter)\n",
    "    \"\"\"\n",
    "    doc = annotated_text_to_spacy_doc(text, nlp)\n",
    "    \n",
    "    try:\n",
    "        import spacy\n",
    "        return spacy.displacy.render(doc, style=style, jupyter=jupyter)\n",
    "    except ImportError:\n",
    "        print(\"spaCy not installed. Please install with: pip install spacy\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3db775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_with_sources(text):\n",
    "    json_data = text.split(\"```json\")[1]\n",
    "    json_data, sources = json_data.split(\"```\")\n",
    "    json_data = json.loads(json_data)\n",
    "    return json_data, sources\n",
    "\n",
    "\n",
    "json_output, sources = parse_json_with_sources(output_text)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8622fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = annotated_text_to_spacy_doc(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ents = []\n",
    "pandas_output = []\n",
    "for ent in doc.ents:\n",
    "    found=False\n",
    "    for item in json_output:\n",
    "        if item[\"entity_text\"] == ent.text:\n",
    "            output_ents.append({\"start\": ent.start_char, \"end\": ent.end_char, \"label\": f'{ent.label_} <a href=\"https://www.wikidata.org/wiki/{item[\"wikidata_id\"]}\">{item[\"wikidata_id\"]}</a>'})\n",
    "            pandas_output.append({\"entity_text\": item[\"entity_text\"], \"label\": item[\"label\"], \"wikidata_id\": item[\"wikidata_id\"], \"ent_start\": ent.start_char, \"ent_end\": ent.end_char})\n",
    "            found=True\n",
    "    if found==False:\n",
    "        output_ents.append({\"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_})\n",
    "        pandas_output.append({\"entity_text\": ent.text, \"label\": ent.label_, \"wikidata_id\": None, \"ent_start\": ent.start_char, \"ent_end\": ent.end_char})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ents = {\n",
    "    \"text\": doc.text,\n",
    "    \"ents\": output_ents,\n",
    "    \"title\": None\n",
    "}\n",
    "\n",
    "displacy.render(dic_ents, manual=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb436b2",
   "metadata": {},
   "source": [
    "## Getting the Data as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pandas_output)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../output/entities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00711b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
