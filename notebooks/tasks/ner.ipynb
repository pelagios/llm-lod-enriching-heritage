{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30cbd04-00d9-4fd1-8a93-fb2b466d319c",
   "metadata": {
    "id": "c30cbd04-00d9-4fd1-8a93-fb2b466d319c"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pelagios/llm-lod-enriching-heritage/blob/main/notebooks/tasks/ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d2026",
   "metadata": {
    "id": "402d2026"
   },
   "source": [
    "# 2. Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task that involves identifying and classifying named entities (like people, places, organizations) within text. For example, in the sentence \"Shakespeare wrote Romeo and Juliet in London\", a NER system would identify \"Shakespeare\" as a person, \"Romeo and Juliet\" as a work of art, and \"London\" as a location. NER is crucial for extracting structured information from unstructured text, making it valuable for tasks like information retrieval, question answering, and metadata enrichment. In this notebook, we'll explore how to perform NER using both traditional NLP approaches and modern Large Language Models.\n",
    "\n",
    "### Rationale\n",
    "\n",
    "This notebook demonstrates how to use OpenAI's GPT models to perform Named Entity Recognition (NER) by converting input text into annotated markdown format. Rather than using traditional NLP libraries, we leverage a Large Language Model's natural language understanding capabilities to identify and classify named entities. The notebook takes plain text as input and outputs markdown where entities are annotated in the format [Entity](TYPE), such as [London](LOCATION). This approach showcases how LLMs can be used for structured information extraction tasks in cultural heritage metadata enrichment.\n",
    "\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "The process consists of the following steps:\n",
    "\n",
    "1. **Import required software libraries**: We start with importing required software libraries\n",
    "2. **Read the text that requires processing**: Next we obtain the input text from the preprocessing notebook\n",
    "3. **Named entity recognition**: The text is sent to GPT with a prompt that instructs it to identify entities. The LLM marks entities in markdown format: \\[entity text\\]\\(entity type\\)\n",
    "4. **Named entity visualization**: The annotated text is displayed with colour-coded entity highlighting\n",
    "5. **Save results**: Save the results of named entity recognition for future processing\n",
    "\n",
    "This approach leverages the LLM's natural language understanding while producing structured, machine-readable output.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook depends on three files:\n",
    "\n",
    "* utils.py: helper functions\n",
    "* output_data_preparation_11f98441067263d80ee1a6bac27babf0f2c6734b.json: output file of data preparation task\n",
    "* ner_cache.json: context-dependent cache of names found earlier\n",
    "\n",
    "Please make sure they are available in this folder so that the notebook can run smoothly. You can download them from [Github](https://github.com/pelagios/llm-lod-enriching-heritage/tree/main/notebooks/tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ccb96",
   "metadata": {
    "id": "168ccb96"
   },
   "source": [
    "## 2.1. Import required software libraries\n",
    "\n",
    "Preprocessing data requires importing some standard software libraries. This step may take some time when run for the first time but in successive runs it will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58abc2ca-a2c2-44cc-a239-75f2bac7229d",
   "metadata": {
    "id": "58abc2ca-a2c2-44cc-a239-75f2bac7229d"
   },
   "source": [
    "We start with checking if the notebook is running on Google Colab. If that is the case, we need to connect to the notebook's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f10f8d-d0e1-40e0-a896-336c3d0567b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71f10f8d-d0e1-40e0-a896-336c3d0567b2",
    "outputId": "711e8995-48c0-44a8-b837-08626248222e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_notebook_environment_on_colab():\n",
    "    \"\"\"Test if run on Colab, if so test if environment is available, if not install it\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        try:\n",
    "            os.chdir(\"/content/llm-lod-enriching-heritage/notebooks/tasks\")\n",
    "            print(\"Found notebook environment\")\n",
    "        except:\n",
    "            print(\"notebook environment not found, installing...\")\n",
    "            !git clone https://github.com/pelagios/llm-lod-enriching-heritage.git\n",
    "            os.chdir(\"/content/llm-lod-enriching-heritage/notebooks/tasks\")\n",
    "    except:\n",
    "        print(\"Not running on Google Colab\")\n",
    "\n",
    "check_notebook_environment_on_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c95e6-d75d-4394-904c-6d3819d07f67",
   "metadata": {
    "id": "649c95e6-d75d-4394-904c-6d3819d07f67"
   },
   "source": [
    "Next we import standard libraries which should always be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5ff2d-7192-4487-9ed1-c02abcbccf83",
   "metadata": {
    "id": "37f5ff2d-7192-4487-9ed1-c02abcbccf83"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "import importlib\n",
    "from IPython.display import clear_output, HTML\n",
    "import json\n",
    "import os\n",
    "import regex\n",
    "import subprocess\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771227af-0f34-4359-861a-c80ae07f38fc",
   "metadata": {
    "id": "771227af-0f34-4359-861a-c80ae07f38fc"
   },
   "source": [
    "Next we import packages which may require installation on this device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cef27a-cca0-45b9-a20d-893b3bf0ae38",
   "metadata": {
    "id": "51cef27a-cca0-45b9-a20d-893b3bf0ae38"
   },
   "outputs": [],
   "source": [
    "openai = utils.safe_import(\"openai\")\n",
    "spacy = utils.safe_import(\"spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5600a-810a-4843-b213-f5cbc00603cd",
   "metadata": {
    "id": "63d5600a-810a-4843-b213-f5cbc00603cd"
   },
   "source": [
    "We set settings required for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f1136-7f0c-47a4-8aaf-ce819ebcf45b",
   "metadata": {
    "id": "1b0f1136-7f0c-47a4-8aaf-ce819ebcf45b"
   },
   "outputs": [],
   "source": [
    "in_colab = utils.check_google_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MC0Dlg3HVWmL",
   "metadata": {
    "id": "MC0Dlg3HVWmL"
   },
   "source": [
    "This function is needed in different sections, we define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2434c-50a4-4c1f-82a2-610fad9d247d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "f4c2434c-50a4-4c1f-82a2-610fad9d247d",
    "outputId": "f1642628-4170-4a8d-8a0c-3ae5b54db5c0"
   },
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"extracted from records of objects in the collection of\n",
    "             the Egyptian museum, Torino, the Museo Egizio â€“ Torino\"\"\"\n",
    "\n",
    "def make_prompt(texts_input, target_labels):\n",
    "    \"\"\"Create an LLM prompt, given a text and target labels and return it\"\"\"\n",
    "    return f\"\"\"\n",
    "Convert the following text {CONTEXT} into a structured markdown format,\n",
    "where you annotate the entities in the text in the following format:\n",
    "[Tom](PERSON) went to [New York](PLACE).\n",
    "\n",
    "Look for the following entities types:\n",
    "{target_labels}\n",
    "\n",
    "Do this for the following text:\n",
    "{texts_input}\n",
    "\n",
    "Only return the markdown output, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422d93f-eb91-422e-a6c3-c6c56a16d6d0",
   "metadata": {
    "id": "b422d93f-eb91-422e-a6c3-c6c56a16d6d0"
   },
   "source": [
    "## 2.2. Read the text that requires processing\n",
    "\n",
    "The text should have been preprocessed by the `data_preparation.ipynb` notebook. The file read here is an output file of that notebook. We select the first text and show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e81fa-5a4a-4ed3-abfd-4008f645dac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "f01e81fa-5a4a-4ed3-abfd-4008f645dac4",
    "outputId": "ac507042-48f1-4219-c771-3fdcf24e1259"
   },
   "outputs": [],
   "source": [
    "infile_name = \"output_data_preparation_11f98441067263d80ee1a6bac27babf0f2c6734b.json\"\n",
    "\n",
    "\n",
    "with open(infile_name, \"r\") as infile:\n",
    "    input_data = json.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "texts_input = [text[\"text_cleaned\"] for text in input_data]\n",
    "print(texts_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4c708",
   "metadata": {
    "id": "2cf4c708"
   },
   "source": [
    "## 2.3. Named entity recognition with ChatGPT\n",
    "\n",
    "We use OpenAI's ChatGPT for recognizing named entities in the text. For this approach you need an OpenAI API key. Store it in the environment variable OPENAI_API_KEY or in the file OPENAI_API_KEY. If you are working in Google Colab, you may also store the key among the Secrets, using the name OPENAI_API_KEY.\n",
    "\n",
    "For alternative approaches to named entity recognition that do not require the OPENAI_API_KEY, see chapter 6. Also when you just want to process the example notebook data from Egyptian Museum, you can just proceed here without providing the OPENAI_API_KEY. This data has already been processed. The associated names will be fetched from the file `ner_cache.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf971106-8b8f-4a45-9a91-d81bbd360e62",
   "metadata": {
    "id": "cf971106-8b8f-4a45-9a91-d81bbd360e62"
   },
   "source": [
    "First we define two helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d29d7e-8a26-411f-ba5b-05cda6d63adc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "99d29d7e-8a26-411f-ba5b-05cda6d63adc",
    "outputId": "04a3d64c-9db0-4f40-a649-793979989ed3"
   },
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\"\n",
    "target_labels=[\"PERSON\", \"LOCATION\"]\n",
    "NER_CACHE_FILE = \"ner_cache.json\"\n",
    "\n",
    "def openai_ner(model, texts_input):\n",
    "    ner_cache = utils.read_json_file(NER_CACHE_FILE)\n",
    "    texts_output = []\n",
    "    for index, text in enumerate(texts_input):\n",
    "        if text in ner_cache and model in ner_cache[text]:\n",
    "            utils.squeal(f\"Retrieving entities for text {index + 1} from cache\")\n",
    "            texts_output.append(ner_cache[text][model])\n",
    "        else:\n",
    "            if \"openai_client\" not in vars():\n",
    "                openai_api_key = utils.get_openai_api_key()\n",
    "                openai_client = utils.connect_to_openai(openai_api_key)\n",
    "            utils.squeal(f\"Processing text {index + 1} with GPT\")\n",
    "            prompt = make_prompt(text, target_labels)\n",
    "            gpt_response = utils.process_text_with_gpt(openai_client, model, prompt)\n",
    "            texts_output.append(gpt_response)\n",
    "            if text not in ner_cache:\n",
    "                ner_cache[text] = {}\n",
    "            ner_cache[text][model] = gpt_response\n",
    "            utils.write_json_file(NER_CACHE_FILE, ner_cache)\n",
    "    print(\"Finished processing\")\n",
    "    utils.save_data_to_json_file(ner_cache, file_name=NER_CACHE_FILE, in_colab=in_colab)\n",
    "    return texts_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6455ab8-f742-41f8-b126-49b756725353",
   "metadata": {
    "id": "e6455ab8-f742-41f8-b126-49b756725353"
   },
   "source": [
    "Next, we use the functions to connect to GPT, send it the text and collect the results, which are shown in markdown format. Change the value of the `MAX_PROCESSED` variable if you do not want all texts to be processed by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66bff5e-3e90-40e9-8b9b-d595661006d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "a66bff5e-3e90-40e9-8b9b-d595661006d1",
    "outputId": "56e1bc2a-efc5-4aaa-b873-3e56ddedb3cb"
   },
   "outputs": [],
   "source": [
    "MAX_PROCESSED = 100\n",
    "\n",
    "texts_output = openai_ner(model, texts_input[:MAX_PROCESSED])\n",
    "print(texts_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f086af",
   "metadata": {
    "id": "76f086af"
   },
   "source": [
    "## 2.4. Named entity visualization\n",
    "\n",
    "Here we show the results of named entity recognition in a more readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c69070e-4b5f-40af-adb2-18266231db1c",
   "metadata": {
    "id": "9c69070e-4b5f-40af-adb2-18266231db1c"
   },
   "source": [
    "First we define three helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff163088-2219-42f6-9778-c72f786e698e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ff163088-2219-42f6-9778-c72f786e698e",
    "outputId": "77611425-5c36-46d4-964f-f8f77223915f"
   },
   "outputs": [],
   "source": [
    "def extract_entities_from_markdown(texts_output):\n",
    "    \"\"\"Extract the locations and labels of the entities from the markdown and return these\"\"\"\n",
    "    pattern = r'\\[([^\\]]+)\\]\\(([^)]+)\\)'\n",
    "    text_prefix = \"\"\n",
    "    current_char = 0\n",
    "    entities = []\n",
    "    for match in regex.finditer(pattern, texts_output):\n",
    "        text_prefix += texts_output[current_char: match.start()]\n",
    "        entities.append({\"text\": match.group(1),\n",
    "                         \"label\": match.group(2),\n",
    "                         \"start_char\": len(text_prefix),\n",
    "                         \"end_char\": len(text_prefix) + len(match.group(1))})\n",
    "        text_prefix += match.group(1)\n",
    "        current_char = match.end()\n",
    "    text_prefix += texts_output[current_char:]\n",
    "    return entities, text_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb03eb-b38d-4ee5-af7b-9676de01a335",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ccdb03eb-b38d-4ee5-af7b-9676de01a335",
    "outputId": "0d1571fe-7afa-4bb0-eb60-56ac44b55e73"
   },
   "outputs": [],
   "source": [
    "def check_llm_output_text(text_llm_output, texts_input):\n",
    "    if text_llm_output != texts_input:\n",
    "        print(f\"{utils.CHAR_FAILURE} Output text of named entity recognition is different from input text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44639606-06d2-442e-b2ec-8fb31d71d921",
   "metadata": {
    "id": "44639606-06d2-442e-b2ec-8fb31d71d921"
   },
   "source": [
    "Next we call the function to extract the entities from the results, check the output text and visualize the results, for the first five results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f5a6e-247d-4c2c-aeff-22023cee039f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "5b7f5a6e-247d-4c2c-aeff-22023cee039f",
    "outputId": "bc505714-14a3-4a8e-e474-831d8ba4f3bd"
   },
   "outputs": [],
   "source": [
    "entities_all = []\n",
    "for index, text in enumerate(texts_output):\n",
    "    entities, text_llm_output = extract_entities_from_markdown(text)\n",
    "    entities_all.append({\"entities\": entities, \"text_llm_output\": text_llm_output})\n",
    "    if index < 5:\n",
    "        check_llm_output_text(text_llm_output, texts_input[index])\n",
    "        display(HTML(utils.mark_entities_in_text(text_llm_output, entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35713f16-99bd-4392-a894-dd59b594bded",
   "metadata": {
    "id": "35713f16-99bd-4392-a894-dd59b594bded"
   },
   "source": [
    "## 2.5. Save results\n",
    "\n",
    "Just like in the data preprocessing notebook, we save the results in a json file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56d739-d55f-4260-8aed-113311a83822",
   "metadata": {
    "id": "cd56d739-d55f-4260-8aed-113311a83822"
   },
   "source": [
    "We define a function for saving the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2f355-ceb0-423e-99ea-17489f289189",
   "metadata": {
    "id": "24d2f355-ceb0-423e-99ea-17489f289189"
   },
   "source": [
    "We add the named entity analysis to the input data and save the results in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf595c60-2d89-4baf-83a7-e5a00c2e25f8",
   "metadata": {
    "id": "bf595c60-2d89-4baf-83a7-e5a00c2e25f8",
    "outputId": "e3c9ca8c-8dae-4487-df27-07c500d37a48"
   },
   "outputs": [],
   "source": [
    "for index, entities in enumerate(entities_all):\n",
    "    input_data[index][\"entities\"] = entities[\"entities\"]\n",
    "    input_data[index][\"text_llm_output\"] = entities[\"text_llm_output\"]\n",
    "for rest_index in range(index, len(input_data)):\n",
    "    input_data[rest_index][\"entities\"] = []\n",
    "    input_data[rest_index][\"text_llm_output\"] = []   \n",
    "utils.save_data_to_json_file(input_data, file_name=\"output_ner.json\", in_colab=in_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e3666-72c9-4c18-b237-9c0d60eccbfd",
   "metadata": {
    "id": "059e3666-72c9-4c18-b237-9c0d60eccbfd"
   },
   "source": [
    "## 2.6. Alternatives for named entity recognition\n",
    "\n",
    "Here we define some alternatives for recognising named entities, for example if you do not have an OpenAI API key or if you do not want to share your data with OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf5dd7-a009-4426-a068-71891e052578",
   "metadata": {
    "id": "9dcf5dd7-a009-4426-a068-71891e052578"
   },
   "source": [
    "### 2.6.1. Named entity recognition with Spacy\n",
    "\n",
    "This named entity recognizer runs at your own computer and does not require an access key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b3214-41c9-4218-abf5-a67f6257cb6f",
   "metadata": {
    "id": "f84b3214-41c9-4218-abf5-a67f6257cb6f"
   },
   "source": [
    "First, we define three helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a16d8-ce60-44b8-bcd7-c36bcadb508b",
   "metadata": {
    "id": "9e7a16d8-ce60-44b8-bcd7-c36bcadb508b"
   },
   "outputs": [],
   "source": [
    "LANG_MODELS = {\n",
    "    \"en\": \"en_core_web_sm\",\n",
    "    \"de\": \"de_core_news_sm\",\n",
    "    \"fr\": \"fr_core_news_sm\",\n",
    "    \"es\": \"es_core_news_sm\",\n",
    "    \"pt\": \"pt_core_news_sm\",\n",
    "    \"it\": \"it_core_news_sm\",\n",
    "    \"nl\": \"nl_core_news_sm\",\n",
    "    \"xx\": \"xx_ent_wiki_sm\"\n",
    "}\n",
    "\n",
    "def load_spacy_model(language_id):\n",
    "    \"\"\"Load the Spacy model for the current language and return it\"\"\"\n",
    "    if language_id not in LANG_MODELS.keys():\n",
    "        print(f\"{utils.CHAR_FAILURE} warning: unknown language {language_id}. Switching to multilingual model...\")\n",
    "        language_id = \"xx\"\n",
    "    try:\n",
    "        nlp = spacy.load(LANG_MODELS[language_id])\n",
    "    except:\n",
    "        try:\n",
    "            print(f\"Model {LANG_MODELS[language_id]} not found, trying to download it...\")\n",
    "            spacy.cli.download(LANG_MODELS[language_id])\n",
    "            nlp = spacy.load(LANG_MODELS[language_id])\n",
    "        except:\n",
    "            raise(RuntimeError(f\"{utils.CHAR_FAILURE} Cannot find language model {LANG_MODELS[language_id]}!\"))\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77287b-d00c-4780-ac6d-1586241e7275",
   "metadata": {
    "id": "0f77287b-d00c-4780-ac6d-1586241e7275"
   },
   "outputs": [],
   "source": [
    "def convert_spacy_entities_to_markdown(doc, texts_input):\n",
    "    \"\"\"Extract the entities recognised by Spacy and return them in a markdown string\"\"\"\n",
    "    entities = [{\"text\": entity.text,\n",
    "                 \"label\": entity.label_,\n",
    "                 \"start_char\": entity.start_char,\n",
    "                 \"end_char\": entity.end_char} for entity in doc.ents]\n",
    "    for entity in reversed(entities):\n",
    "        if entity[\"label\"] in [\"FAC\", \"GPE\", \"LOC\"]:\n",
    "            entity[\"label\"] = \"LOCATION\"\n",
    "        if entity[\"label\"] in [\"LOCATION\", \"PERSON\"]:\n",
    "            texts_input = (texts_input[:entity[\"end_char\"]] +\n",
    "                          f\"]({entity['label']})\" +\n",
    "                          texts_input[entity[\"end_char\"]:])\n",
    "            texts_input = texts_input[:entity[\"start_char\"]] + \"[\" + texts_input[entity[\"start_char\"]:]\n",
    "    return texts_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306bd0a-84bd-4b4e-b393-e96409856899",
   "metadata": {
    "id": "b306bd0a-84bd-4b4e-b393-e96409856899"
   },
   "source": [
    "Next, we call the helper functions for finding the language of the input text, selecting the right Spacy model, calling the model and converting the results to a markdown string, which is shown. The markdown string can be fed to the visualisation code of chapter 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406011a-9d11-4dbf-bdb7-a0809530acee",
   "metadata": {
    "id": "3406011a-9d11-4dbf-bdb7-a0809530acee",
    "outputId": "3dc80e82-85fb-40aa-b875-645e809f53d8"
   },
   "outputs": [],
   "source": [
    "MAX_PROCESSED = 100\n",
    "\n",
    "texts_output = []\n",
    "last_language_id = \"\"\n",
    "for index, text in enumerate(texts_input[:MAX_PROCESSED]):\n",
    "    utils.squeal(f\"Processing text {index + 1}\")\n",
    "    language_id = utils.detect_text_language(text)\n",
    "    if language_id != last_language_id:\n",
    "        nlp = load_spacy_model(language_id)\n",
    "        last_language_id = language_id\n",
    "    doc = nlp(text)\n",
    "    texts_output.append(convert_spacy_entities_to_markdown(doc, text))\n",
    "print(\"Finished processing\")\n",
    "print(texts_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OVzlQ0_61KUd",
   "metadata": {
    "id": "OVzlQ0_61KUd"
   },
   "source": [
    "### 2.6.2. Named entity recognition with Qwen\n",
    "\n",
    "Here we use [Qwen](https://en.wikipedia.org/wiki/Qwen), a locally-run model developed by the company Alibaba. The model has hardware requirements which your computer may not satisfy: it needs a GPU and about 12 gigabytes of memory. When running this model on Google Colab, it is recommended to use the runtime environment `T4 GPU`. If the model is too slow for you, the model `qwen3:0.6` can be used as an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GT3EauiG2ick",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "GT3EauiG2ick",
    "outputId": "ce823d74-d460-42de-8e98-7e4358488300"
   },
   "outputs": [],
   "source": [
    "MODEL = \"qwen3:8b\"\n",
    "MAX_PROCESSED = 5\n",
    "\n",
    "texts_output = utils.ollama_run(MODEL, texts_input[:MAX_PROCESSED], make_prompt, in_colab)\n",
    "print(texts_output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ag-2kUp5vVU3",
   "metadata": {
    "id": "Ag-2kUp5vVU3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
