{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86212093-694f-46c4-a9e1-100321cd75a7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/pelagios/llm-lod-enriching-heritage/blob/main/notebooks/tasks/disambiguation_candidates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a1c79",
   "metadata": {},
   "source": [
    "# Disambiguation with Candidates\n",
    "\n",
    "This notebook disambiguates the entities found by the NER notebook by linking them to WikiData concepts\n",
    "\n",
    "### Rationale\n",
    "\n",
    "When using LLMs, we can leverage context to help determine correct identifiers for entities found. One of the largest challenges with LLMs is getting them to generate the correct identifier for specific entities. Without context, an LLM will confidently generate a believable looking identifier code. When checked, however, users will often find these codes do not exist or are entirely wrong.\n",
    "\n",
    "We solve this problem with context. LLMs can receive context in one of two ways: either we can give it the context or we can use an LLM agentically with tools so that it can retrieve the context for itself. Both have their advantages, but both work within the same principal: context allows the LLM to get the correct identifier code so that it does not need to hallucinate one. While hallucinations are still possible, the chances are reduced if we provide a list of options to an LLM to choose from.\n",
    "\n",
    "In this notebook, we will explore the first of these options, where we provide the LLM with a list of candidates that were generated in the previous data notebook. To make things easier, we have pasted the output from that notebook here.\n",
    "\n",
    "It is also worth noting that providing the LLM with the necessary context is often quite cheaper (assuming you are using a paid-model), than letting the model agentically query the web or use other tools. We will see this in the next notebook.\n",
    "\n",
    "### Processing overview\n",
    "\n",
    "The process consists of the following steps:\n",
    "\n",
    "1. **Import required software libraries**: We start with importing required software libraries\n",
    "2. **Read the text that requires processing**: Next we obtain the input text from the NER notebook\n",
    "3. **Candidate extraction**\n",
    "4. **Disambiguation**: The text is sent to an LLM with a prompt that instructs it to disambiguate entities based on the available candidates.\n",
    "5. **Disambiguation visualization**: The annotated text is displayed with colour-coded entity highlighting\n",
    "6. **Save results**: Save the results of the disambiguation process for future processing\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook depends on four files:\n",
    "\n",
    "* utils.py: helper functions\n",
    "* output_ner_5f12fa7c16d33ea378148197569f999f774f7481.json: output file of NER task\n",
    "* disambiguation_cache_wikidata.json: context-dependent cache of WikiData information found earlier\n",
    "* disambiguation_cache_llm.json: context-dependent cache of LLM choices make earlier\n",
    "\n",
    "Please make sure the files are available in this folder so that the notebook can run smoothly. You can download the files from Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce3540",
   "metadata": {},
   "source": [
    "## 1. Import required software libraries\n",
    "\n",
    "Disambiguation requires importing some standard software libraries. This step may take some time when run for the first time but in successive runs it will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2747d7-d0fe-411b-9d2c-31dbf5f9b179",
   "metadata": {},
   "source": [
    "We start with checking if the notebook is running on Google Colab. If that is the case, we need to connect to the notebook's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08dc054-f637-4a0d-b244-6858a5888fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_notebook_environment_on_colab():\n",
    "    \"\"\"Test if run on Colab, if so test if environment is available, if not install it\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        try:\n",
    "            os.chdir(\"/content/llm-lod-enriching-heritage/notebooks/tasks\")\n",
    "            print(\"Found notebook environment\")\n",
    "        except:\n",
    "            print(\"notebook environment not found, installing...\")\n",
    "            !git clone https://github.com/pelagios/llm-lod-enriching-heritage.git\n",
    "            os.chdir(\"/content/llm-lod-enriching-heritage/notebooks/tasks\")\n",
    "    except:\n",
    "        print(\"Not running on Google Colab\")\n",
    "\n",
    "check_notebook_environment_on_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb2b75-48c8-4336-b647-d7b9bbedb631",
   "metadata": {},
   "source": [
    "Next we import standard libraries which should always be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a02df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37222d3a-cdd5-455a-99c3-0f9413795202",
   "metadata": {},
   "source": [
    "Next we import packages which may require installation on this device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d7adb-4839-49be-9a32-3b0fa15700e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = utils.safe_import(\"openai\")\n",
    "pd = utils.safe_import(\"pandas\")\n",
    "pydantic = utils.safe_import(\"pydantic\")\n",
    "spacy = utils.safe_import(\"spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ddd87-872d-494e-9250-456739fc2221",
   "metadata": {},
   "source": [
    "Finally we set settings required for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2751ca67-3c31-4a56-ab87-532231cb80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_colab = utils.check_google_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9a2f1-f189-4f24-b1d3-f05ea88eb59f",
   "metadata": {},
   "source": [
    "These two helper functions are needed in different sections, we define them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb98d1-f840-48fb-a342-fbe992323e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"extracted from records of objects in the collection of \n",
    "             the Egyptian museum, Torino, the Museo Egizio â€“ Torino\"\"\"\n",
    "\n",
    "def make_prompt(entity, text, candidates):\n",
    "    \"\"\"Create an LLM prompt, given a text and target labels and return it\"\"\"\n",
    "    return f\"\"\"\n",
    "Disambiguate the entity \"{entity}\" in the following text {CONTEXT}.\n",
    "\n",
    "{text}\n",
    "\n",
    "Here are the candidates, in json format:\n",
    "\n",
    "{candidates}\n",
    "\n",
    "Only return the relevant id, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf750e-8af4-40c3-9951-bb32b5df2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wikidata_ids_to_texts_input(texts_input, entities):\n",
    "    \"\"\"Insert the retrieved wikidata ids into the variable text_inputs and return it\"\"\"\n",
    "    entities_per_text = {}\n",
    "    for entity in entities:\n",
    "        if entity[\"text_id\"] not in entities_per_text:\n",
    "            entities_per_text[entity[\"text_id\"]] = {}\n",
    "        entities_per_text[entity[\"text_id\"]][entity[\"entity_text\"]] = entity\n",
    "    for text_id, text in enumerate(texts_input):\n",
    "        for entity in text[\"entities\"]:\n",
    "            if text_id in entities_per_text and entity[\"text\"] in entities_per_text[text_id]:\n",
    "                entity[\"wikidata_id\"] = entities_per_text[text_id][entity[\"text\"]][\"wikidata_id\"]\n",
    "    return texts_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559246c-da0e-4d94-b857-65527220e7f6",
   "metadata": {},
   "source": [
    "## 2. Read the texts that require processing\n",
    "\n",
    "The texts should have been processed by the ner.ipynb notebook. The file read here is an output file of that notebook. We read the texts and the associated metadata and show the first text with its entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3d702-647d-4c64-acab-8da3028f7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_name = \"output_ner_5f12fa7c16d33ea378148197569f999f774f7481.json\"\n",
    "\n",
    "with open(infile_name, \"r\") as infile:\n",
    "    texts_input = json.load(infile)\n",
    "    infile.close()\n",
    "print({\"text_cleaned\": texts_input[0][\"text_cleaned\"], \n",
    "       \"entities\": texts_input[0][\"entities\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6363b-74e5-4177-b3f4-5cf2f7c3fafe",
   "metadata": {},
   "source": [
    "## 3. Disambiguation candidate generation\n",
    "\n",
    "We need candidate ids for the entities to make the task for the LLM easier. We obtain these candidate ids by searching for the entities in wikidata.org. The search step returns seven candidate ids with a description text for each of them. In order not to overload the website, we use a cache for storing entities that were looked up earlier. We also wait for 10 seconds (variable `SLEEP_TIME_FETCH_PAGE`) between the searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e38391-d3ff-476c-b4ef-56f795a19f60",
   "metadata": {},
   "source": [
    "First we define five helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495787e4-d72b-43c4-9834-dbd65174cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_TIME_FETCH_PAGE = 10\n",
    "USER_AGENT = \"Pelagios/0.0 (https://github.com/pelagios/; e.tjongkimsang@esciencecenter.nl) generic-library/0.0\"\n",
    "\n",
    "def query_wikidata_for_single_entity(entity_text):\n",
    "    \"\"\"Read Wikidata data on entity_text and return it\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": USER_AGENT,\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'language': 'en',\n",
    "        'format': 'json',\n",
    "        'limit': '7',\n",
    "        'search': entity_text \n",
    "    }\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    time.sleep(SLEEP_TIME_FETCH_PAGE)\n",
    "    wikidata_data = requests.get(url, params=params, headers=headers)\n",
    "    return wikidata_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4087c95-0ee6-4b3e-865e-1258c7d4fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_ner_input(texts_input):\n",
    "    \"\"\"For each entity in the input text return the entity text, context text and context text id\"\"\" \n",
    "    return [{\"entity_text\": entity[\"text\"],\n",
    "             \"text_id\": index,\n",
    "             \"text\": text[\"text_cleaned\"]} \n",
    "            for index, text in enumerate(texts_input) for entity in text[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eaf53f-ce28-4125-81bd-567a3ab793fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_WIKIDATA = \"disambiguation_cache_wikidata.json\"\n",
    "\n",
    "\n",
    "def find_wikidata_candidates_for_entities(entities):\n",
    "    \"\"\"Lookup candidate ids for the entities on wikidata.org and return them\"\"\"\n",
    "    wikidata_cache = utils.read_json_file(DISAMBUGUATION_CACHE_FILE_WIKIDATA)\n",
    "    for entity in entities:\n",
    "        if entity[\"entity_text\"] not in wikidata_cache.keys():\n",
    "            utils.squeal(f\"Looking up entity {entity['entity_text']} (text id {entity['text_id'] + 1}) on WikiData.org...\")\n",
    "            wikidata_data = query_wikidata_for_single_entity(entity[\"entity_text\"])\n",
    "            wikidata_cache[entity[\"entity_text\"]] = [{\"id\": candidate[\"id\"], \n",
    "                                                            \"description\": candidate[\"description\"]} \n",
    "                                                            for candidate in json.loads(wikidata_data.text)[\"search\"]\n",
    "                                                            if \"description\" in candidate.keys()]\n",
    "            utils.write_json_file(DISAMBUGUATION_CACHE_FILE_WIKIDATA, wikidata_cache)\n",
    "    print(\"Finished processing\")\n",
    "    utils.save_data_to_json_file(wikidata_cache, file_name=DISAMBUGUATION_CACHE_FILE_WIKIDATA, in_colab=in_colab)\n",
    "    for entity in entities:\n",
    "        entity[\"candidates\"] = wikidata_cache[entity[\"entity_text\"]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c4b7a-0281-4ba4-bebe-84b5de6d03c9",
   "metadata": {},
   "source": [
    "Next we extract the entities from the input data and call a helper function for finding the candidate ids. These will be stored in the `entities` variable. We show the first item of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df11bef-291b-4dab-b907-8291370ebff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = extract_entities_from_ner_input(texts_input)\n",
    "entities = find_wikidata_candidates_for_entities(entities)\n",
    "print({key: entities[0][key] for key in ['entity_text', 'candidates']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48267cd-7e1b-4128-9f32-4ac20a54628b",
   "metadata": {},
   "source": [
    "## 4. Disambiguate entities with ChatGPT\n",
    "\n",
    "We disambiguate entities by sending a prompt with each entity text, the context text and the WikiData candidates ids and their descriptions to an LLM. We ask the LLM to return the id associated with the description that best matches the entity in the given context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb61643-a253-4dc4-a705-6475a4a78e55",
   "metadata": {},
   "source": [
    "First we define six helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef64fd3d-e7fc-41b7-b505-57c0425ef9a3",
   "metadata": {},
   "source": [
    "This helper function creates a prompt for each entity and sends it to the LLM and collect the responses. When the combination of the entity and the context text is available in the cache of previously processed entities, we skip consulting ChatGPT and use the wikidata id stored in the cache. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2b5fd-85e5-472c-8119-e2e02e3276ff",
   "metadata": {},
   "source": [
    "We call the function to select the right WikiData id with ChatGPT. Next, we add the selections to the texts_output variable. Change the value of the MAX_PROCESSED variable if you do not want all texts to be processed by the LLM. The results are stored in the `entities` variable. We show the first of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75deea80-7bab-41cf-b0ac-fb4217c3010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_LLM = \"disambiguation_cache_llm.json\"\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "def openai_wikidata_id_selection(model, entities):\n",
    "    llm_cache = utils.read_json_file(DISAMBUGUATION_CACHE_FILE_LLM)\n",
    "    for entity in entities:\n",
    "        if (entity[\"entity_text\"] in llm_cache and \n",
    "            entity[\"text\"] in llm_cache[entity[\"entity_text\"]] and\n",
    "            model in llm_cache[entity[\"entity_text\"]][entity[\"text\"]]):\n",
    "            utils.squeal(f\"Retrieving entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} from cache\")\n",
    "            entity[\"wikidata_id\"] = llm_cache[entity[\"entity_text\"]][entity[\"text\"]][model] | {\"model\": model}\n",
    "        else:\n",
    "            if \"openai_client\" not in vars():\n",
    "                openai_api_key = utils.get_openai_api_key()\n",
    "                openai_client = utils.connect_to_openai(openai_api_key)                \n",
    "            utils.squeal(f\"Sending entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} to {model}\")\n",
    "            prompt = make_prompt(entity[\"entity_text\"], entity[\"text\"], entity[\"candidates\"])\n",
    "            entity[\"wikidata_id\"] = {model: utils.process_text_with_gpt(openai_client, model, prompt)}\n",
    "            if entity[\"entity_text\"] not in llm_cache:\n",
    "                llm_cache[entity[\"entity_text\"]] = {}\n",
    "            if entity[\"text\"] not in llm_cache[entity[\"entity_text\"]]:\n",
    "                llm_cache[entity[\"entity_text\"]][entity[\"text\"]] = {}\n",
    "            llm_cache[entity[\"entity_text\"]][entity[\"text\"]][model] = {key: value \n",
    "                                                                       for key, value in entity[\"wikidata_id\"].items()}\n",
    "            utils.write_json_file(DISAMBUGUATION_CACHE_FILE_LLM, llm_cache)\n",
    "            time.sleep(2)\n",
    "    print(\"Finished processing\")\n",
    "    utils.save_data_to_json_file(llm_cache, file_name=DISAMBUGUATION_CACHE_FILE_LLM, in_colab=in_colab)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61717672-8733-4f59-b564-b060e5e88ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PROCESSED = 100\n",
    "MAX_PROCESSED_ENTITIES = len([entity for entity in entities if entity[\"text_id\"] < MAX_PROCESSED])\n",
    "\n",
    "processed_entities = openai_wikidata_id_selection(model, entities[MAX_PROCESSED_ENTITIES])\n",
    "texts_output = add_wikidata_ids_to_texts_input(texts_input[:MAX_PROCESSED], processed_entities)\n",
    "print({key: value for key, value in processed_entities[0].items() if key in [\"entity_text\", \"text\", \"wikidata_id\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd36a9b-9bb7-4f25-b32a-015293afcc27",
   "metadata": {},
   "source": [
    "## 5. Disambiguation visualization\n",
    "\n",
    "Here we show the results of the disambiguation task in a more readable format. We use the helper function `mark_entities_in_text` for this. But you will not find the definition of this helper function here: it is defined in the file `utils.py` because it is used by other notebooks as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842c94b-63c4-4d43-bfea-ef595b440bba",
   "metadata": {},
   "source": [
    "Here we call the helper function and show the first three texts with their entities and the WikiData ids. We do not supply it with the cleaned text but with the text that was output of the named entity recognition (`text_llm_output`), because that is sometimes different from the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bafc8-2fce-4d03-a8db-9f9296b1f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_id, text in enumerate(texts_output):\n",
    "    if text_id < 3:\n",
    "        display(HTML(utils.mark_entities_in_text(text[\"text_llm_output\"], text[\"entities\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64655dc-0d64-4746-96aa-ff008cf4f427",
   "metadata": {},
   "source": [
    "## 6. Save results\n",
    "\n",
    "We save the results in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2ed8b-d12b-4945-b749-446c0f8b476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_data_to_json_file(texts_output, file_name=\"output_disambiguation.json\", in_colab=in_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfcba0-304f-4c9a-8873-3890401edeb8",
   "metadata": {},
   "source": [
    "## 7. Alternatives for disambiguation\n",
    "\n",
    "Here we define some alternatives for disambiguation, for example if you do not have an OpenAI API key or if you do not want to share your data with OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bf85b-1df9-430a-86d5-9b6718635af8",
   "metadata": {},
   "source": [
    "### 7.1. Disambiguate entities with Qwen\n",
    "\n",
    "Here we use [Qwen](https://en.wikipedia.org/wiki/Qwen), a locally-run model developed by the company Alibaba. The model has hardware requirements which your computer may not satisfy: it needs a GPU and about 12 gigabytes of memory. When running this model on Google Colab, it is recommended to use the runtime environment `T4 GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c85cdf-ed72-4679-95b6-69e5cc449cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAMBUGUATION_CACHE_FILE_LLM = \"disambiguation_cache_llm.json\"\n",
    "\n",
    "def ollama_wikidata_id_selection(model, entities):\n",
    "    llm_cache = utils.read_json_file(DISAMBUGUATION_CACHE_FILE_LLM)\n",
    "    for entity in entities:\n",
    "        if (entity[\"entity_text\"] in llm_cache and \n",
    "            entity[\"text\"] in llm_cache[entity[\"entity_text\"]] and\n",
    "            model in llm_cache[entity[\"entity_text\"]][entity[\"text\"]]):\n",
    "            utils.squeal(f\"Retrieving entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} from cache\")\n",
    "            entity[\"wikidata_id\"] = llm_cache[entity[\"entity_text\"]][entity[\"text\"]][model] | {\"model\": model}\n",
    "        else:\n",
    "            utils.squeal(f\"Sending entity \\\"{entity['entity_text']}\\\" of text {entity['text_id'] + 1} to {model}\")\n",
    "            if \"ollama\" in sys.modules:\n",
    "                ollama = utils.importlib.import_module(\"ollama\")\n",
    "            else:\n",
    "                ollama = utils.import_ollama_module()\n",
    "            utils.install_ollama_model(model, ollama)\n",
    "            prompt = make_prompt(entity[\"entity_text\"], entity[\"text\"], entity[\"candidates\"])\n",
    "            entity[\"wikidata_id\"] = {model: utils.process_text_with_ollama(model, prompt, ollama)}\n",
    "            if entity[\"entity_text\"] not in llm_cache:\n",
    "                llm_cache[entity[\"entity_text\"]] = {}\n",
    "            if entity[\"text\"] not in llm_cache[entity[\"entity_text\"]]:\n",
    "                llm_cache[entity[\"entity_text\"]][entity[\"text\"]] = {}\n",
    "            llm_cache[entity[\"entity_text\"]][entity[\"text\"]][model] = {key: value \n",
    "                                                                       for key, value in entity[\"wikidata_id\"].items()}\n",
    "            utils.write_json_file(DISAMBUGUATION_CACHE_FILE_LLM, llm_cache)\n",
    "            time.sleep(2)\n",
    "    print(\"Finished processing\")\n",
    "    utils.save_data_to_json_file(llm_cache, file_name=DISAMBUGUATION_CACHE_FILE_LLM, in_colab=in_colab)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca75d4-3d34-4f5f-9dfc-2dcf299c148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"qwen3:8b\"\n",
    "MAX_PROCESSED = 3\n",
    "MAX_PROCESSED_ENTITIES = len([entity for entity in entities if entity[\"text_id\"] < MAX_PROCESSED])\n",
    "\n",
    "processed_entities = ollama_wikidata_id_selection(MODEL, entities[:MAX_PROCESSED_ENTITIES])\n",
    "texts_output = add_wikidata_ids_to_texts_input(texts_input[:MAX_PROCESSED], processed_entities)\n",
    "print({key: value for key, value in processed_entities[0].items() if key in [\"entity_text\", \"text\", \"wikidata_id\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0786e-fcca-4199-8b5d-98dafb796441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
