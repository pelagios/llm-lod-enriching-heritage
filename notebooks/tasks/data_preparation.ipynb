{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pelagios/llm-lod-enriching-heritage/blob/main/notebooks/tasks/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hLkfqo0gRKP"
   },
   "source": [
    "# 1. Prepare Cultural Heritage Data for Named Entity Analysis\n",
    "\n",
    "This notebook fetches and prepares data for named entity analysis. \n",
    "\n",
    "### Rationale\n",
    "\n",
    "This recipe takes some source content and preprocesses it for use across the rest of the cookbook.\n",
    "\n",
    "You can either:\n",
    "\n",
    "* upload your own sample text (e.g. transcribed text from a digitised item), or\n",
    "* run queries to get sample content from the Cleveland Museum of Art API from a keyword (e.g. \"Manet\") or record ID search\n",
    "\n",
    "The notebook shows the necessary steps to process the results and format them into a JSON file that can then be fed into a NER process.\n",
    "\n",
    "### Overview of the process\n",
    "\n",
    "Most parts of the recipe simply need to be run in a Notebook environment like Colab, Binder, Jupyter Notebooks.\n",
    "\n",
    "1. Initialise the notebook: run the first 'cells' to import the required libraries\n",
    "2. Fetch input: this is the part where you can provide some specific input, in csv format\n",
    "3. Preprocess text: clean text, detect language and split the text in tokens and sentences\n",
    "4. Save the results: combine results into a structured json record: text (original and cleaned), language, sentences, tokens and meta information (counts and ids)\n",
    "\n",
    "The fifth chapter provides alternatives for handling texts from other sources than websites:\n",
    "\n",
    "5. Alternatives for reading text data: text defined in code, text from file, text from pdf file and text from a website\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook depends on four files:\n",
    "\n",
    "* utils.py: helper functions\n",
    "* input_em.csv: default example input file (table)\n",
    "* input_wanderer.pdf: example pdf input file\n",
    "* input_wikipedia.txt: example text input file\n",
    "\n",
    "Please make sure they are available in this folder so that the notebook can run smoothly. You can download them from [Github](https://github.com/pelagios/llm-lod-enriching-heritage/tree/main/notebooks/tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g_sWNZ7HLN2"
   },
   "source": [
    "## 1.1. Install required software libraries\n",
    "\n",
    "Preprocessing data requires importing some standard software libraries. This step may take some time when run for the first time but in successive runs it will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with checking if the notebook is running on Google Colab. If that is the case, we need to connect to the notebook's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_notebook_environment_on_colab():\n",
    "    \"\"\"Test if run on Colab, if so test in environment is available, if not install it\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        try:\n",
    "            os.chdir(\"/content/llm-lod-enriching-heritage/notebooks/tasks\")\n",
    "            print(\"Found notebook environment\")\n",
    "        except:\n",
    "            print(\"notebook environment not found, installing...\")\n",
    "            !git clone https://github.com/pelagios/llm-lod-enriching-heritage.git\n",
    "            os.chdir(\"/content/llm-lod-enriching-heritage/notebooks/tasks\")\n",
    "    except:\n",
    "        print(\"Not running on Google Colab\")\n",
    "\n",
    "check_notebook_environment_on_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import standard libraries which should always be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_4ibh6-tsb-n"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import importlib\n",
    "from IPython.display import clear_output, HTML\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import unicodedata\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import packages which may require installation on this device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = utils.safe_import(\"spacy\")\n",
    "langid = utils.safe_import(\"langid\")\n",
    "regex = utils.safe_import(\"regex\")\n",
    "pl = utils.safe_import(\"polars\")\n",
    "pypdf = utils.safe_import(\"pypdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we set setting required for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_colab = utils.check_google_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Reading texts from a csv file\n",
    "\n",
    "We use the artefact descriptions from the Egyptian Museum in Turin as example texts. We use two columns of the file, one with the identifier of the artefact and one with the description text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function for reading the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"input_em.csv\"\n",
    "data_source = \"EMT\"\n",
    "id_column_name = \"Inventory Number\"\n",
    "text_column_name = \"Description\"\n",
    "\n",
    "\n",
    "def read_emt_data(file_name):\n",
    "    \"\"\"Read texts from the Egyptian Museum in Turin from a csv file\"\"\"\n",
    "    try:\n",
    "        table_pl = pl.read_csv(file_name)[id_column_name, text_column_name]\n",
    "        table_pl.write_csv(\"tmp.csv\")\n",
    "        return [{\"id\": row[0], \"data_source\": data_source, \"text_original\": row[1]}\n",
    "                for row in table_pl.iter_rows()]\n",
    "    except:\n",
    "        print(f\"{utils.CHAR_FAILURE} Cannot read data from file {file_name}!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the function and store the variable in the variable `texts`. We show the first text to check if the process was successful. Note that the text includes (`id`) the text identifier as metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 'C. 0115', 'data_source': 'EMT', 'text_original': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts = read_emt_data(file_name)\n",
    "if len(texts) <= 0:\n",
    "    print(f\"{utils.CHAR_FAILURE} No texts found in file {file_name}!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkcitK7zHcuK"
   },
   "source": [
    "## 1.3. Preprocess text for named entity analysis\n",
    "\n",
    "Three steps are performed while preprocessing the texts:\n",
    "\n",
    "1. Text cleanup: remove non-text characters, urls and email addresses\n",
    "2. Detect the language of the text\n",
    "3. Split the text in sentences and tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining five functions for performing the preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_Yzx1zaEv9Dp"
   },
   "outputs": [],
   "source": [
    "def cleanup_text(text: str) -> str:\n",
    "    \"\"\"Cleanup text: remove non-text characters, urls and email addresses\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = regex.sub(r\"[\\u0000-\\u0008\\u000B\\u000C\\u000E-\\u001F\\u007F]\", \"\", text)\n",
    "    text = regex.sub(r\"[ \\t\\u00A0]+\", \" \", text)\n",
    "    text = regex.sub(r\"\\s+\", \" \", text)\n",
    "    text = regex.sub(r\"https?://\\S+\", \"<URL>\", text)\n",
    "    text = regex.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"<EMAIL>\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language_id):\n",
    "    \"\"\"Preprocess a single text: divide it in sentences and tokens\"\"\"\n",
    "    try:\n",
    "        spacy_model = spacy.blank(language_id)\n",
    "    except:\n",
    "        print(f\"{utils.CHAR_FAILURE} Cannot load model for language {language_id}\")\n",
    "        spacy_model = spacy.blank(\"xx\")\n",
    "    spacy_model.add_pipe(\"sentencizer\")\n",
    "    preprocessed_text = spacy_model(text)\n",
    "    sentences = [{\"id\": sentence_id, \n",
    "                  \"start\": sentence.start_char, \n",
    "                  \"end\": sentence.end_char, \n",
    "                  \"text\": sentence.text} for sentence_id, sentence in enumerate(preprocessed_text.sents)]\n",
    "    tok2sent = {token.i: sentence_id for sentence_id, sentence in enumerate(preprocessed_text.sents) \n",
    "                                     for token in sentence}\n",
    "    tokens = [{\"id\": token.i,\n",
    "               \"text\": token.text,\n",
    "               \"start\": token.idx,\n",
    "               \"end\": token.idx + len(token.text),\n",
    "               \"ws\": token.whitespace_ != \"\",\n",
    "               \"is_punct\": token.is_punct,\n",
    "               \"sent_id\": tok2sent.get(token.i)} for token in preprocessed_text]\n",
    "    return sentences, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e6xknyGKwQJz"
   },
   "outputs": [],
   "source": [
    "def preprocess_texts(texts) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Preprocess a list of texts and return the results as a list of dictionaries\"\"\"\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for index, text in enumerate(texts):\n",
    "        utils.squeal(f\"Processing text {index + 1}\")\n",
    "        sentences, tokens = preprocess_text(text[\"text_cleaned\"], text[\"language_id\"])\n",
    "        results.append({\"meta\": {**{key: text[key] for key in text if not regex.search(\"text\", key)},\n",
    "                                 \"char_count\": len(text),\n",
    "                                 \"token_count\": len(tokens),\n",
    "                                 \"sentence_count\": len(sentences)},\n",
    "                        \"text_original\": text[\"text_original\"],\n",
    "                        \"text_cleaned\": text[\"text_cleaned\"],\n",
    "                        \"sentences\": sentences,\n",
    "                        \"tokens\": tokens})\n",
    "    print(\"Finished processing\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_text(text, skipped_fields=[]):\n",
    "    \"\"\"Show example text\"\"\"\n",
    "    text_shown = {key: text[key] for key in text if key not in skipped_fields}\n",
    "    if \"tokens\" in skipped_fields:\n",
    "        text_shown = text_shown | {\"tokens\": text[\"tokens\"][:3] + \n",
    "                                             [\"...\"] if len(text[\"tokens\"]) >= 3 else []}\n",
    "    print(text_shown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the cleanup function to the texts and store the results in the variable `text_cleaned`. We show the first text to check if the process was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sw9jFDPTxflU",
    "outputId": "68fd96cd-31e4-4e76-ce96-075cf1646a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'C. 0115', 'data_source': 'EMT', 'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts_cleaned = [text | {\"text_cleaned\": cleanup_text(text[\"text_original\"])} for text in texts]\n",
    "show_example_text(texts_cleaned[0], skipped_fields=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we apply the language derivation function to the texts and store the results in the variable `text_with_language_ids`. Again, we show the first text to check if the process was successful. A parameter `language_id` should appear at the start of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U0q26bQxzKQ",
    "outputId": "f378c7d6-b146-49f2-9015-d31bc5c52d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language_id': 'en', 'id': 'C. 0115', 'data_source': 'EMT', 'text_cleaned': 'Statuette of the god Anubis. Bronze. Late Period (722-332 BC).. Acquired before 1882. C. 115'}\n"
     ]
    }
   ],
   "source": [
    "texts_with_language_ids = [{\"language_id\": utils.detect_text_language(text[\"text_cleaned\"])} | text\n",
    "                            for text in texts_cleaned]\n",
    "show_example_text(texts_with_language_ids[0], skipped_fields=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply the preprocess function to the texts and store the results in the variable `text_preprocessed`. We show the first text to check if the process was successful. The texts have been divided in sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sklE05mCyemA",
    "outputId": "28d1072c-23b8-408c-9fd0-aa5aaa3846e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 100\n",
      "Finished processing\n",
      "{'meta': {'language_id': 'en', 'id': 'C. 0115', 'data_source': 'EMT', 'char_count': 5, 'token_count': 23, 'sentence_count': 4}, 'sentences': [{'id': 0, 'start': 0, 'end': 28, 'text': 'Statuette of the god Anubis.'}, {'id': 1, 'start': 29, 'end': 36, 'text': 'Bronze.'}, {'id': 2, 'start': 37, 'end': 85, 'text': 'Late Period (722-332 BC).. Acquired before 1882.'}, {'id': 3, 'start': 86, 'end': 92, 'text': 'C. 115'}], 'tokens': [{'id': 0, 'text': 'Statuette', 'start': 0, 'end': 9, 'ws': True, 'is_punct': False, 'sent_id': 0}, {'id': 1, 'text': 'of', 'start': 10, 'end': 12, 'ws': True, 'is_punct': False, 'sent_id': 0}, {'id': 2, 'text': 'the', 'start': 13, 'end': 16, 'ws': True, 'is_punct': False, 'sent_id': 0}, '...']}\n"
     ]
    }
   ],
   "source": [
    "texts_preprocessed = preprocess_texts(texts_with_language_ids)\n",
    "show_example_text(texts_preprocessed[0], skipped_fields=[\"text_original\", \"text_cleaned\", \"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Save results\n",
    "\n",
    "When running the notebook locally, the preprocessed texts can be saved locally. When running the notebook on Google Colab, the results need to be downloaded because saved files on Google Colab will be removed automatically. We use a function defined in the file `utils.py` for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "️✅ Saved data to file output_data_preparation_11f98441067263d80ee1a6bac27babf0f2c6734b.json\n"
     ]
    }
   ],
   "source": [
    "utils.save_data_to_json_file(texts_preprocessed, file_name=\"output_data_preparation.json\", in_colab=in_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Alternatives for reading text data\n",
    "\n",
    "Here are some methods for reading texts as alternatives for reading them from a museum website as presented in chapter 2 of this notebook. Run these code blocks instead of the code blocks of chapter 2 and then proceed with chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1. Text examples defined in the code\n",
    "\n",
    "We use descriptions from three famous artworks from Wikipedia, written in other languages than English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the texts. We include identifiers in their descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [{\"id\": 1, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"De Nachtwacht is een schuttersstuk van de \n",
    "           Hollandse schilder Rembrandt van Rijn (1606-1669) dat in 1642 gereed kwam. De huidige officiële \n",
    "           titel luidt: Officieren en andere schutters van wijk II in Amsterdam, onder leiding van kapitein \n",
    "           Frans Banninck Cocq en luitenant Willem van Ruytenburch, bekend als ‘De Nachtwacht’.\"\"\"},\n",
    "         {\"id\": 2, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"La Gioconda, nota anche come Monna Lisa, \n",
    "           è un dipinto a olio su tavola di pioppo realizzato da Leonardo da Vinci (77 × 53 cm e 13 mm di \n",
    "           spessore), databile al 1503-1506 circa e conservato nel Museo del Louvre di Parigi col numero 779 \n",
    "           di catalogo.\"\"\"},\n",
    "         {\"id\": 3, \"data_source\": \"wikipedia\", \"text_original\": \"\"\"Le Penseur (initialement intitulé Le Poète) \n",
    "           est un des chefs-d'œuvre emblématiques d'Auguste Rodin.\"\"\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check if the definition worked and display the first text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 1, 'data_source': 'wikipedia', 'text_original': 'De Nachtwacht is een schuttersstuk van de \\n           Hollandse schilder Rembrandt van Rijn (1606-1669) dat in 1642 gereed kwam. De huidige officiële \\n           titel luidt: Officieren en andere schutters van wijk II in Amsterdam, onder leiding van kapitein \\n           Frans Banninck Cocq en luitenant Willem van Ruytenburch, bekend als ‘De Nachtwacht’.'}\n"
     ]
    }
   ],
   "source": [
    "if len(texts) <= 0:\n",
    "    print(f\"{utils.CHAR_FAILURE} No texts found!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2. Read a single text from a text file\n",
    "\n",
    "We use a description of a monument from Wikipedia, written in a different language than English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining a function for reading the text from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"input_wikipedia.txt\"\n",
    "data_source = \"wikipedia\"\n",
    "\n",
    "\n",
    "def read_wikipedia_file(file_name):\n",
    "    \"\"\"Read a text from Wikipedia from a text file\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\") as infile:\n",
    "            text = infile.read().strip()\n",
    "            infile.close()\n",
    "        return [{\"id\": 1, \"data_source\": data_source, \"text_original\": text}]\n",
    "    except:\n",
    "        print(f\"{utils.CHAR_FAILURE} Cannot read data from file {file_name}!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function. This requires that a file with the specified file name in present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 1, 'data_source': 'wikipedia', 'text_original': 'Der Kölner Dom (offiziell Hohe Domkirche zu Köln) ist eine römisch-katholische Kirche in Köln unter dem Patrozinium des Apostels Petrus. Er ist die Kathedrale des Erzbistums Köln sowie Metropolitan kirche der Kirchenprovinz Köln. Hausherr ist der Dompropst. Der Kölner Dom ist eine der größten Kathedralen im gotischen Baustil. Sein Bau wurde 1248 im Auftrag von Konrad I. nach Entwurf von Meister Gerhard begonnen und 1880 im Auftrag von Friedrich Wilhelm IV. nach Entwurf von Ernst Friedrich Zwirner vollendet. Einige Kunsthistoriker haben den Dom wegen seiner einheitlichen und ausgewogenen Bauform als „vollkommene Kathedrale“ bezeichnet. Mit 157,22 Metern ist er nach dem Ulmer Münster der zweithöchste Sakralbau Deutschlands und hinter der Basilika Notre-Dame-de-la-Paix de Yamoussoukro die dritthöchste Kirche der Welt.'}\n"
     ]
    }
   ],
   "source": [
    "texts = read_wikipedia_file(file_name)\n",
    "if len(texts) <= 0:\n",
    "    print(f\"{utils.CHAR_FAILURE} No texts found in file {file_name}!\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3. Read text from a pdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function for reading text from a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"input_wanderer.pdf\"\n",
    "\n",
    "\n",
    "def get_pdf_text(file_name):\n",
    "    \"\"\"Read a single text from a pdf file\"\"\"\n",
    "    try:\n",
    "        reader = pypdf.PdfReader(file_name)\n",
    "    except:\n",
    "        print(f\"{utils.CHAR_FAILURE} Cannot read pdf file {file_name}!\")\n",
    "        return []\n",
    "    if len(reader.pages) == 0:\n",
    "        print(f\"{utils.CHAR_FAILURE} No text found in pdf file {file_name}!\")\n",
    "        return []\n",
    "    return [reader.pages[0].extract_text()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the function and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: Wanderer above the Sea of Fog is a painting by German Romanticist artist\n",
      "Caspar David Friedrich made in 1818. It depicts a man standing upon a rocky\n",
      "precipice with his back to the viewer; he is gazing out on a landscape covered\n",
      "in a thick sea of fog through which other ridges, trees, and mountains pierce,\n",
      "which stretches out into the distance indefinitely.\n"
     ]
    }
   ],
   "source": [
    "texts = get_pdf_text(file_name)\n",
    "if not texts:\n",
    "    raise(Exception(f\"{utils.CHAR_FAILURE} No texts were found. Are you connected to the internet?\"))\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKvJmCRQHQUN"
   },
   "source": [
    "### 1.5.4. Download text from museum website\n",
    "\n",
    "We use the description of a painting by Claude Monet from the artwork Cleveland Museum website as example text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function for reading the texts from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bOSpuE_xsvgX"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://openaccess-api.clevelandart.org/api/artworks\"\n",
    "data_source = \"CMA\"\n",
    "\n",
    "\n",
    "def fetch_cma(query_string: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch metadata from Cleveland Museum of Art website\"\"\"\n",
    "    try:\n",
    "        response = requests.get(base_url, params={\"q\": query_string, \"skip\": 0, \"limit\": 100}, timeout=10)\n",
    "    except:\n",
    "        print(f\"{utils.CHAR_FAILURE} Cannot download data from {base_url}\")\n",
    "        return []\n",
    "    response.raise_for_status()\n",
    "    artworks = response.json().get(\"data\", [])\n",
    "    if artworks == []:\n",
    "        return []\n",
    "    else:\n",
    "        return [{\"id\": artworks[0].get(\"id\"), \n",
    "                 \"data_source\": data_source,\n",
    "                 \"text_original\": (artworks[0].get(\"description\") or \"\").strip()}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1rjBgsgwkM4",
    "outputId": "a139382e-f994-4ce0-9fc4-627bd1f27bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text found: {'id': 135382, 'data_source': 'CMA', 'text_original': \"This painting depicts Monet's first wife, Camille, outside on a snowy day passing by the French doors of their home at Argenteuil. Her face is rendered in a radically bold Impressionist technique of mere daubs of paint quickly applied, just as the snow and trees are defined by broad, broken strokes of pure white and green.\"}\n"
     ]
    }
   ],
   "source": [
    "texts = fetch_cma(\"monet\")\n",
    "if not texts:\n",
    "    print(f\"{utils.CHAR_FAILURE} No texts were found. Are you connected to the internet?\")\n",
    "else:\n",
    "    print(\"Text found:\", texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.5 Recognise text from pdf or image with optical character recognition (ocr)\n",
    "\n",
    "We have a separate notebook for this task: `../data_preparation/Run-Docling.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YDuesoBf067g"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
